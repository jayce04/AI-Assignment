{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb26708",
   "metadata": {},
   "source": [
    "# Facial Skincare Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6c79b",
   "metadata": {},
   "source": [
    "### Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3d7077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  product_id               product_name  brand_id brand_name  loves_count  \\\n",
      "0    P473671    Fragrance Discovery Set      6342      19-69         6320   \n",
      "1    P473668    La Habana Eau de Parfum      6342      19-69         3827   \n",
      "2    P473662  Rainbow Bar Eau de Parfum      6342      19-69         3253   \n",
      "3    P473660       Kasbah Eau de Parfum      6342      19-69         3018   \n",
      "4    P473658  Purple Haze Eau de Parfum      6342      19-69         2691   \n",
      "\n",
      "   rating  reviews            size                      variation_type  \\\n",
      "0  3.6364     11.0             NaN                                 NaN   \n",
      "1  4.1538     13.0  3.4 oz/ 100 mL  Size + Concentration + Formulation   \n",
      "2  4.2500     16.0  3.4 oz/ 100 mL  Size + Concentration + Formulation   \n",
      "3  4.4762     21.0  3.4 oz/ 100 mL  Size + Concentration + Formulation   \n",
      "4  3.2308     13.0  3.4 oz/ 100 mL  Size + Concentration + Formulation   \n",
      "\n",
      "  variation_value  ... online_only out_of_stock  sephora_exclusive  \\\n",
      "0             NaN  ...           1            0                  0   \n",
      "1  3.4 oz/ 100 mL  ...           1            0                  0   \n",
      "2  3.4 oz/ 100 mL  ...           1            0                  0   \n",
      "3  3.4 oz/ 100 mL  ...           1            0                  0   \n",
      "4  3.4 oz/ 100 mL  ...           1            0                  0   \n",
      "\n",
      "                                          highlights  primary_category  \\\n",
      "0  ['Unisex/ Genderless Scent', 'Warm &Spicy Scen...         Fragrance   \n",
      "1  ['Unisex/ Genderless Scent', 'Layerable Scent'...         Fragrance   \n",
      "2  ['Unisex/ Genderless Scent', 'Layerable Scent'...         Fragrance   \n",
      "3  ['Unisex/ Genderless Scent', 'Layerable Scent'...         Fragrance   \n",
      "4  ['Unisex/ Genderless Scent', 'Layerable Scent'...         Fragrance   \n",
      "\n",
      "   secondary_category  tertiary_category  child_count  child_max_price  \\\n",
      "0   Value & Gift Sets  Perfume Gift Sets            0              NaN   \n",
      "1               Women            Perfume            2             85.0   \n",
      "2               Women            Perfume            2             75.0   \n",
      "3               Women            Perfume            2             75.0   \n",
      "4               Women            Perfume            2             75.0   \n",
      "\n",
      "   child_min_price  \n",
      "0              NaN  \n",
      "1             30.0  \n",
      "2             30.0  \n",
      "3             30.0  \n",
      "4             30.0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# Download using Kaggle API\n",
    "os.system('kaggle datasets download -d nadyinky/sephora-products-and-skincare-reviews')\n",
    "\n",
    "# Unzip the downloaded dataset\n",
    "with zipfile.ZipFile('sephora-products-and-skincare-reviews.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('sephora_dataset')\n",
    "\n",
    "# Load product information\n",
    "products_df = pd.read_csv(\"sephora_dataset/product_info.csv\")\n",
    "\n",
    "print(products_df.head()) # for checking purpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b87c5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "products_df = pd.read_csv('data/OriDataset/product_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b017c5b",
   "metadata": {},
   "source": [
    "### Filter Skincare Product only in primary_category, secondary_category, and tertiary_category columns\n",
    "(remove makeup, fragrance, bath&body, hair, teeth, supplement, etc products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ae81f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered products dataset saved as 'filtered_skincare_products.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convert product_id to string and strip whitespace\n",
    "products_df['product_id'] = products_df['product_id'].astype(str).str.strip()\n",
    "\n",
    "# Filter only 'Skincare' at primary_category \n",
    "skincare_products_df = products_df[products_df['primary_category'].str.lower() == 'skincare'].copy()\n",
    "\n",
    "# Keep only specific secondary categories\n",
    "allowed_secondary_categories = [\n",
    "    'Moisturizers', 'Treatments', 'Eye Care', 'Lip Balms & Treatments',\n",
    "    'Sunscreen', 'Cleansers', 'Masks'\n",
    "]\n",
    "\n",
    "skincare_products_df = skincare_products_df[skincare_products_df['secondary_category'].isin(allowed_secondary_categories)].copy()\n",
    "\n",
    "# Keep only selected tertiary categories\n",
    "allowed_tertiary_categories = [\n",
    "    'Moisturizers', 'Face Serums', 'Eye Creams & Treatments', 'Face Sunscreen',\n",
    "    'Face Wash & Cleansers', 'Face Oils', 'Toners', 'Face Masks', 'Facial Peels',\n",
    "    'Exfoliators', 'Eye Masks', 'Face Wipes', 'Blemish & Acne Treatments',\n",
    "    'Night Creams', 'Mists & Essences', 'Sheet Masks', 'Makeup Removers'\n",
    "]\n",
    "\n",
    "skincare_products_df = skincare_products_df[skincare_products_df['tertiary_category'].isin(allowed_tertiary_categories)].copy()\n",
    "\n",
    "# Save to new CSV file\n",
    "skincare_products_df.to_csv(\"data/CleanedDataSet/filtered_skincare_products.csv\", index=False)\n",
    "print(\"Filtered products dataset saved as 'filtered_skincare_products.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5347e9",
   "metadata": {},
   "source": [
    "### Filter Reviews to Keep only Skincare Review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b01b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joanne Wong\\AppData\\Local\\Temp\\ipykernel_16008\\3593967287.py:20: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews_df = pd.read_csv(file)\n",
      "C:\\Users\\Joanne Wong\\AppData\\Local\\Temp\\ipykernel_16008\\3593967287.py:20: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews_df = pd.read_csv(file)\n",
      "C:\\Users\\Joanne Wong\\AppData\\Local\\Temp\\ipykernel_16008\\3593967287.py:20: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews_df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final merged reviews file saved as: filtered_skincare_reviews.csv\n",
      "Total reviews: 801455\n"
     ]
    }
   ],
   "source": [
    "# Load filtered skincare products IDs\n",
    "filtered_products = pd.read_csv(\"data/CleanedDataSet/filtered_skincare_products.csv\")\n",
    "skincare_product_ids = set(filtered_products['product_id'].unique())\n",
    "\n",
    "import glob\n",
    "\n",
    "# List of file paths\n",
    "review_files = [\n",
    "    \"data/OriDataSet/reviews_0-250.csv\",\n",
    "    \"data/OriDataSet/reviews_250-500.csv\",\n",
    "    \"data/OriDataSet/reviews_500-750.csv\",\n",
    "    \"data/OriDataSet/reviews_750-1250.csv\",\n",
    "    \"data/OriDataSet/reviews_1250-end.csv\"\n",
    "]\n",
    "\n",
    "# Initialize empty list to store filtered reviews\n",
    "all_filtered_reviews = []\n",
    "\n",
    "for file in review_files:\n",
    "    reviews_df = pd.read_csv(file)\n",
    "    \n",
    "    skincare_reviews = reviews_df[reviews_df['product_id'].isin(skincare_product_ids)].copy()\n",
    "    all_filtered_reviews.append(skincare_reviews)\n",
    "\n",
    "# Merge all and save as one new file\n",
    "merged_reviews_df = pd.concat(all_filtered_reviews, ignore_index=True)\n",
    "\n",
    "# Add sequential review_id\n",
    "merged_reviews_df['review_id'] = merged_reviews_df.index + 1\n",
    "\n",
    "# Drop essential missing values\n",
    "merged_reviews_df.dropna(subset=[\"author_id\", \"product_id\", \"rating\", \"review_text\", \"skin_type\"], inplace=True)\n",
    "\n",
    "# Keep valid ratings\n",
    "merged_reviews_df = merged_reviews_df[(merged_reviews_df[\"rating\"] >= 1) & (merged_reviews_df[\"rating\"] <= 5)]\n",
    "\n",
    "# Remove duplicates\n",
    "merged_reviews_df.drop_duplicates(inplace=True)\n",
    "\n",
    "merged_reviews_df.to_csv(\"data/CleanedDataSet/filtered_skincare_reviews.csv\", index=False)\n",
    "\n",
    "print(f\"\\nFinal merged reviews file saved as: filtered_skincare_reviews.csv\")\n",
    "print(f\"Total reviews: {len(merged_reviews_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d20ed1",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414a4b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>total_feedback_count</th>\n",
       "      <th>total_neg_feedback_count</th>\n",
       "      <th>total_pos_feedback_count</th>\n",
       "      <th>submission_time</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_title</th>\n",
       "      <th>skin_tone</th>\n",
       "      <th>eye_color</th>\n",
       "      <th>skin_type</th>\n",
       "      <th>hair_color</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>review_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16140</td>\n",
       "      <td>31124221503</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-03-19</td>\n",
       "      <td>Makeup remover, gentle cleanser, and all aroun...</td>\n",
       "      <td>Best. Cleanser. Ever.</td>\n",
       "      <td>fair</td>\n",
       "      <td>brown</td>\n",
       "      <td>combination</td>\n",
       "      <td>blonde</td>\n",
       "      <td>P7880</td>\n",
       "      <td>Soy Hydrating Gentle Face Cleanser</td>\n",
       "      <td>fresh</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16141</td>\n",
       "      <td>20246074916</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>I have been using this for almost 10 years. Lo...</td>\n",
       "      <td>The cleanser I have used for 10 years</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hazel</td>\n",
       "      <td>combination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P7880</td>\n",
       "      <td>Soy Hydrating Gentle Face Cleanser</td>\n",
       "      <td>fresh</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16142</td>\n",
       "      <td>5182718480</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>I wanted to love this so bad because it felt s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mediumTan</td>\n",
       "      <td>brown</td>\n",
       "      <td>combination</td>\n",
       "      <td>brown</td>\n",
       "      <td>P7880</td>\n",
       "      <td>Soy Hydrating Gentle Face Cleanser</td>\n",
       "      <td>fresh</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16144</td>\n",
       "      <td>27905619860</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-03-09</td>\n",
       "      <td>Best facial cleanser I have used! I heard grea...</td>\n",
       "      <td>Hands down best cleanser</td>\n",
       "      <td>light</td>\n",
       "      <td>hazel</td>\n",
       "      <td>combination</td>\n",
       "      <td>brown</td>\n",
       "      <td>P7880</td>\n",
       "      <td>Soy Hydrating Gentle Face Cleanser</td>\n",
       "      <td>fresh</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16145</td>\n",
       "      <td>8523204146</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>I found this to be just ok - definitely gentle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>hazel</td>\n",
       "      <td>combination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P7880</td>\n",
       "      <td>Soy Hydrating Gentle Face Cleanser</td>\n",
       "      <td>fresh</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    author_id  rating  is_recommended  helpfulness  \\\n",
       "1       16140  31124221503       5             1.0          1.0   \n",
       "2       16141  20246074916       5             1.0          1.0   \n",
       "3       16142   5182718480       1             0.0          0.0   \n",
       "5       16144  27905619860       5             1.0          1.0   \n",
       "6       16145   8523204146       3             0.0          0.0   \n",
       "\n",
       "   total_feedback_count  total_neg_feedback_count  total_pos_feedback_count  \\\n",
       "1                     1                         0                         1   \n",
       "2                     2                         0                         2   \n",
       "3                     2                         2                         0   \n",
       "5                     2                         0                         2   \n",
       "6                     1                         1                         0   \n",
       "\n",
       "  submission_time                                        review_text  \\\n",
       "1      2023-03-19  Makeup remover, gentle cleanser, and all aroun...   \n",
       "2      2023-03-15  I have been using this for almost 10 years. Lo...   \n",
       "3      2023-03-10  I wanted to love this so bad because it felt s...   \n",
       "5      2023-03-09  Best facial cleanser I have used! I heard grea...   \n",
       "6      2023-03-06  I found this to be just ok - definitely gentle...   \n",
       "\n",
       "                            review_title  skin_tone eye_color    skin_type  \\\n",
       "1                  Best. Cleanser. Ever.       fair     brown  combination   \n",
       "2  The cleanser I have used for 10 years        NaN     hazel  combination   \n",
       "3                                    NaN  mediumTan     brown  combination   \n",
       "5               Hands down best cleanser      light     hazel  combination   \n",
       "6                                    NaN      light     hazel  combination   \n",
       "\n",
       "  hair_color product_id                        product_name brand_name  \\\n",
       "1     blonde      P7880  Soy Hydrating Gentle Face Cleanser      fresh   \n",
       "2        NaN      P7880  Soy Hydrating Gentle Face Cleanser      fresh   \n",
       "3      brown      P7880  Soy Hydrating Gentle Face Cleanser      fresh   \n",
       "5      brown      P7880  Soy Hydrating Gentle Face Cleanser      fresh   \n",
       "6        NaN      P7880  Soy Hydrating Gentle Face Cleanser      fresh   \n",
       "\n",
       "   price_usd  review_id  \n",
       "1       39.0          2  \n",
       "2       39.0          3  \n",
       "3       39.0          4  \n",
       "5       39.0          6  \n",
       "6       39.0          7  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54aa67e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(801455, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_reviews_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17146ff",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6fc2898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final products preprocessed file saved as: data/CleanedDataSet/products_preprocessed.csv\n",
      "Sample product_content:\n",
      "genius sleeping collagen moisturizer algenist s...\n",
      "genius liquid collagen serum algenist skincare ...\n",
      "triple algae eye renewal balm cream algenist sk...\n",
      "sublime defense ultra lightweight uv fluid spf_...\n",
      "genius ultimate anti-aging cream algenist skinc...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output file paths\n",
    "IN_PATH  = \"data/CleanedDataSet/filtered_skincare_products.csv\"   # raw filtered file\n",
    "OUT_PATH = \"data/CleanedDataSet/products_preprocessed.csv\"        # final preprocessed file\n",
    "SHOW_DEBUG = True\n",
    "\n",
    "# ------------------------------\n",
    "# Basic helpers\n",
    "# ------------------------------\n",
    "def norm_text(s):\n",
    "    # Normalize text: lowercase, strip, collapse whitespace\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    return re.sub(r\"\\s+\", \" \", s.lower().strip())\n",
    "\n",
    "def map_with_table(text, table):\n",
    "    # Match text against regex patterns in a table, return first label found\n",
    "    if not text:\n",
    "        return None\n",
    "    for patt, label in table:\n",
    "        if re.search(patt, text, flags=re.IGNORECASE):\n",
    "            return label\n",
    "    return None\n",
    "\n",
    "# ------------------------------\n",
    "# Product-type rules \n",
    "# ------------------------------\n",
    "# Map regex patterns from categories/names to standardized product types\n",
    "TERTIARY_MAP = [\n",
    "    (r\"\\beye (creams?(\\s*&\\s*treatments?)?|masks?|care)\\b\", \"eye treatment\"),\n",
    "    (r\"\\bface serums?\\b\", \"serum\"),\n",
    "    (r\"\\bserums?\\b\", \"serum\"),\n",
    "    (r\"\\bface oils?\\b|\\bfacial oils?\\b\", \"face oil\"),\n",
    "    (r\"\\bface sunscreen\\b|\\bsunscreens?\\b|\\bsun care\\b|\\bspf\\b\", \"sunscreen\"),\n",
    "    (r\"\\bmoisturizers?\\b|\\bnight creams?\\b|\\bgel[- ]?creams?\\b|\\bface creams?\\b|\\blotions?\\b|\\bemulsions?\\b\", \"moisturizer\"),\n",
    "    (r\"\\bface masks?\\b|\\bsheet masks?\\b|\\bsleeping masks?\\b|\\bovernight masks?\\b|\\bclay masks?\\b|\\bmud masks?\\b\", \"face mask\"),\n",
    "    (r\"\\bface wipes\\b|\\bmakeup removers?\\b\", \"makeup remover\"),\n",
    "    (r\"\\bcleansers?\\b|\\bface wash\\b|\\bmicellar\\b\", \"cleanser\"),\n",
    "    (r\"\\btoners?\\b|\\bmists? & essences?\\b|\\bessences?\\b\", \"toner\"),\n",
    "    (r\"\\bblemish\\s*&\\s*acne treatments?\\b|\\bspot (treatments?|correctors?)\\b|\\bacne spot\\b\", \"spot treatment\"),\n",
    "    (r\"\\bexfoliators?\\b|\\bexfoliants?\\b|\\bpeels?\\b|\\bscrubs?\\b|\\b(aha|bha|pha)\\b\", \"exfoliator\"),\n",
    "]\n",
    "SECONDARY_MAP = TERTIARY_MAP  # Reuse same mapping for secondary\n",
    "\n",
    "# Fallback rules if product type not found in categories\n",
    "NAME_RULES = [\n",
    "    (r\"\\beye (cream|serum|gel|balm|mask|treatment)\\b\", \"eye treatment\"),\n",
    "    (r\"\\bface oil|facial oil|night oil|firming oil\\b|\\boil\\b(?!\\s*cleanser)\", \"face oil\"),\n",
    "    (r\"\\bface wipes\\b|\\bmakeup remover\\b\", \"makeup remover\"),\n",
    "    (r\"\\b(cleanser|cleansing|face wash|micellar|balm cleanser|oil cleanser)\\b\", \"cleanser\"),\n",
    "    (r\"\\b(toner|tonique)\\b|\\bmists?\\b|\\bessence\\b(?!\\s*serum)\", \"toner\"),\n",
    "    (r\"\\b(serum|ampoule|booster)\\b(?!\\s*mask)|\\bessence serum\\b\", \"serum\"),\n",
    "    (r\"\\b(sunscreen|sun screen|spf)\\b\", \"sunscreen\"),\n",
    "    (r\"\\b(mask|sheet mask|sleeping mask|wash[- ]?off mask|overnight mask|mud mask|clay mask)\\b\", \"face mask\"),\n",
    "    (r\"\\b(spot (treatment|corrector)|acne spot|blemish treatment)\\b\", \"spot treatment\"),\n",
    "    (r\"\\b(exfoliator|exfoliant|peel|aha|bha|pha|scrub)\\b\", \"exfoliator\"),\n",
    "    (r\"\\b(moisturizer|moisturiser|cream|lotion|emulsion|gel[- ]?cream|face cream)\\b\", \"moisturizer\"),\n",
    "]\n",
    "\n",
    "def detect_product_type(row):\n",
    "    # Infer product type from tertiary > secondary > name (priority order)\n",
    "    name = norm_text(row.get(\"product_name\", \"\"))\n",
    "    sec  = norm_text(row.get(\"secondary_category\", \"\"))\n",
    "    ter  = norm_text(row.get(\"tertiary_category\", \"\"))\n",
    "\n",
    "    # Tertiary category first\n",
    "    label = map_with_table(ter, TERTIARY_MAP)\n",
    "    if label: return label\n",
    "\n",
    "    # Secondary (skip if too generic like 'treatment')\n",
    "    is_generic = bool(re.fullmatch(r\"(treatment|treatments)\", sec.strip()))\n",
    "    if not is_generic:\n",
    "        label = map_with_table(sec, SECONDARY_MAP)\n",
    "        if label: return label\n",
    "\n",
    "    # Fallback: check product name\n",
    "    label = map_with_table(name, NAME_RULES)\n",
    "    if label: return label\n",
    "\n",
    "    return \"other\"\n",
    "\n",
    "# ------------------------------\n",
    "# Skin type + concern rules \n",
    "# ------------------------------\n",
    "# Regex patterns to detect skin types from text\n",
    "SKIN_TYPE_PATTERNS = [\n",
    "    (r\"\\b(?:good|best)\\s*for:\\s*oily\\b\", \"oily\"),\n",
    "    (r\"\\b(?:good|best)\\s*for:\\s*dry\\b\", \"dry\"),\n",
    "    (r\"\\b(?:good|best)\\s*for:\\s*combination\\b\", \"combination\"),\n",
    "    (r\"\\b(?:good|best)\\s*for:\\s*sensitive\\b\", \"sensitive\"),\n",
    "    (r\"\\b(?:good|best)\\s*for:\\s*normal\\b\", \"normal\"),\n",
    "    (r\"\\b(oily skin|oily)\\b\", \"oily\"),\n",
    "    (r\"\\b(dry skin|dry)\\b\", \"dry\"),\n",
    "    (r\"\\b(combination skin|combination|combo)\\b\", \"combination\"),\n",
    "    (r\"\\b(sensitive skin|sensitive)\\b\", \"sensitive\"),\n",
    "    (r\"\\b(normal skin|normal)\\b\", \"normal\"),\n",
    "    (r\"\\bfor\\s+sensitive\\s+skin\\b\", \"sensitive\"),\n",
    "    (r\"\\bsuitable\\s+for\\s+sensitive\\b\", \"sensitive\"),\n",
    "    (r\"\\bfor\\s+sensitive\\b\", \"sensitive\"),\n",
    "    (r\"\\bhypoallergenic\\b\", \"sensitive\"),\n",
    "    (r\"\\bgentle\\b\", \"sensitive\"),\n",
    "]\n",
    "\n",
    "# Regex patterns to detect skin concerns from claims or product name\n",
    "SKIN_CONCERN_PATTERNS = [\n",
    "    (r\"\\b(acne|blemish|breakout|pimple)\\b\", \"acne\"),\n",
    "    (r\"\\bpores?\\b\", \"pores\"),\n",
    "    (r\"\\b(dark spot|hyperpigment|discoloration|melasma)\\b\", \"hyperpigmentation\"),\n",
    "    (r\"\\b(wrinkle|fine line|anti[- ]?aging|firming|loss of firmness|elasticity)\\b\", \"aging\"),\n",
    "    (r\"\\b(redness|rosacea|irritation|calming|soothing)\\b\", \"redness\"),\n",
    "    (r\"\\b(dryness|dehydration|hydrating|moisturizing|moisturising|barrier)\\b\", \"dehydration\"),\n",
    "    (r\"\\b(dull(ness)?|brighten(ing)?|glow|radiance)\\b\", \"dullness\"),\n",
    "    (r\"\\boil(y| control|iness)\\b\", \"oil-control\"),\n",
    "    (r\"\\b(blackhead|whitehead|congestion)\\b\", \"blackheads\"),\n",
    "    (r\"\\b(uneven (tone|texture)|texture|resurfacing)\\b\", \"texture\"),\n",
    "    (r\"\\b(dark circle|dark circles)\\b\", \"dark-circles\"),\n",
    "]\n",
    "\n",
    "# Ingredient-based concern mapping\n",
    "INGREDIENT_CONCERN_PATTERNS = [\n",
    "    (r\"\\b(salicylic acid|beta hydroxy|bha|willow bark|benzoyl peroxide|sulfur|zinc pca|zinc)\\b\", {\"acne\",\"pores\",\"oil-control\"}),\n",
    "    (r\"\\b(kaolin|bentonite|clay|charcoal)\\b\", {\"pores\",\"oil-control\"}),\n",
    "    (r\"\\b(tea tree|melaleuca)\\b\", {\"acne\"}),\n",
    "    (r\"\\b(hyaluronic acid|sodium hyaluronate|glycerin|panthenol|urea|betaine|trehalose|aloe)\\b\", {\"dehydration\"}),\n",
    "    (r\"\\b(ceramide|ceramides|cholesterol|squalane|squalene|shea|shea butter)\\b\", {\"dehydration\"}),\n",
    "    (r\"\\b(retinol|retinal|retinoate|bakuchiol|peptide|matrixyl|collagen|coenzyme ?q10|ubiquinone)\\b\", {\"aging\"}),\n",
    "    (r\"\\b(vitamin ?c|ascorbic|ascorbyl|ethyl ascorbic|magnesium ascorbyl|sodium ascorbyl|alpha arbutin|tranexamic|azelaic|kojic|licorice|glycyrrhiza)\\b\", {\"hyperpigmentation\",\"dullness\"}),\n",
    "    (r\"\\b(centella|cica|madecassoside|asiaticoside|allantoin|bisabolol|beta glucan|green tea|oat|colloidal oatmeal)\\b\", {\"redness\"}),\n",
    "    (r\"\\b(aha|glycolic|lactic|mandelic|tartaric|citric|pha|gluconolactone|lactobionic)\\b\", {\"texture\",\"dullness\"}),\n",
    "]\n",
    "\n",
    "def to_list_from_highlights(val):\n",
    "    # Parse highlights column into list (safe eval or regex split)\n",
    "    if pd.isna(val):\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    try:\n",
    "        parsed = ast.literal_eval(s)\n",
    "        if isinstance(parsed, list):\n",
    "            return [str(x) for x in parsed]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [t.strip() for t in re.split(r\"[,\\|;/]+\", s) if t.strip()]\n",
    "\n",
    "def extract_skin_types_from_highlights_and_name(highlights_list, product_name):\n",
    "    # Detect skin types from highlights + product name\n",
    "    stypes = set()\n",
    "    for raw in highlights_list:\n",
    "        t = norm_text(raw)\n",
    "        for patt, lab in SKIN_TYPE_PATTERNS:\n",
    "            if re.search(patt, t):\n",
    "                stypes.add(lab)\n",
    "    pname = norm_text(product_name)\n",
    "    if re.search(r\"\\bsensitive\\b\", pname):\n",
    "        stypes.add(\"sensitive\")\n",
    "    return \",\".join(sorted(stypes))\n",
    "\n",
    "def extract_concerns_from_highlights_and_name(highlights_list, product_name):\n",
    "    # Detect skin concerns from highlights + product name\n",
    "    concerns = set()\n",
    "    for raw in highlights_list:\n",
    "        t = norm_text(raw)\n",
    "        for patt, lab in SKIN_CONCERN_PATTERNS:\n",
    "            if re.search(patt, t):\n",
    "                concerns.add(lab)\n",
    "    pname = norm_text(product_name)\n",
    "    for patt, lab in SKIN_CONCERN_PATTERNS:\n",
    "        if re.search(patt, pname):\n",
    "            concerns.add(lab)\n",
    "    return concerns\n",
    "\n",
    "def extract_concerns_from_ingredients(ingredients_text):\n",
    "    # Detect skin concerns directly from ingredient list\n",
    "    concerns = set()\n",
    "    ing = norm_text(ingredients_text)\n",
    "    for patt, labs in INGREDIENT_CONCERN_PATTERNS:\n",
    "        if re.search(patt, ing):\n",
    "            concerns |= labs\n",
    "    return concerns\n",
    "\n",
    "# ------------------------------\n",
    "# Text cleaning for vectorization\n",
    "# ------------------------------\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"a\",\"an\",\"the\",\"and\",\"or\",\"but\",\"to\",\"of\",\"for\",\"on\",\"in\",\"into\",\"with\",\"by\",\"from\",\"as\",\n",
    "    \"it\",\"its\",\"this\",\"that\",\"these\",\"those\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
    "    \"at\",\"up\",\"down\",\"over\",\"under\",\"about\",\"than\",\"then\",\"so\",\"such\",\"very\",\"more\",\"most\",\n",
    "    \"less\",\"least\",\"you\",\"your\",\"yours\",\"we\",\"our\",\"ours\",\"they\",\"their\",\"theirs\",\"he\",\"she\",\"his\",\"her\",\"them\",\"i\",\"me\",\"my\",\"mine\"\n",
    "}\n",
    "\n",
    "PHRASES = [\n",
    "    (r\"\\bhyaluronic acid\\b\", \"hyaluronic_acid\"),\n",
    "    (r\"\\bsodium hyaluronate\\b\", \"sodium_hyaluronate\"),\n",
    "    (r\"\\bsalicylic acid\\b\", \"salicylic_acid\"),\n",
    "    (r\"\\bbenzoyl peroxide\\b\", \"benzoyl_peroxide\"),\n",
    "    (r\"\\balpha hydroxy\\b\", \"alpha_hydroxy\"),\n",
    "    (r\"\\bbeta hydroxy\\b\", \"beta_hydroxy\"),\n",
    "    (r\"\\bpolyhydroxy\\b\", \"polyhydroxy\"),\n",
    "    (r\"\\bvitamin c\\b\", \"vitamin_c\"),\n",
    "    (r\"\\bvitamin e\\b\", \"vitamin_e\"),\n",
    "    (r\"\\bcoenzyme q10\\b|\\bubiquinone\\b\", \"coenzyme_q10\"),\n",
    "    (r\"\\btea tree\\b\", \"tea_tree\"),\n",
    "    (r\"\\bcentella asiatica\\b|\\bcica\\b\", \"centella_asiatica\"),\n",
    "    (r\"\\bgreen tea\\b\", \"green_tea\"),\n",
    "    (r\"\\bniacinamide\\b\", \"niacinamide\"),\n",
    "    (r\"\\bretino(l|id|ate|nal)\\b\", \"retinoid\"),\n",
    "    (r\"\\baha\\b\", \"aha\"),\n",
    "    (r\"\\bbha\\b\", \"bha\"),\n",
    "    (r\"\\bpha\\b\", \"pha\"),\n",
    "    (r\"\\bspf\\s*\\d+\\b\", lambda m: \"spf_\" + re.sub(r\"\\D\", \"\", m.group(0))),\n",
    "]\n",
    "\n",
    "def deaccent(s):\n",
    "    # Remove accents/diacritics from text \n",
    "    return unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "def strip_html(s):\n",
    "    # Remove HTML tags\n",
    "    return re.sub(r\"<[^>]+>\", \" \", s)\n",
    "\n",
    "def basic_normalize(s):\n",
    "    # General normalization: lowercase, strip HTML, remove symbols/numbers\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = s.lower()\n",
    "    s = strip_html(s)\n",
    "    s = deaccent(s)\n",
    "    s = re.sub(r\"[\\/\\+]\", \" \", s)                # split slashes/plus\n",
    "    s = re.sub(r\"[^a-z0-9\\-\\_\\s]\", \" \", s)       # keep hyphen/underscore\n",
    "    s = re.sub(r\"\\b\\d+\\b\", \" \", s)               # drop standalone numbers\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def apply_phrases(s):\n",
    "    # Replace key phrases with normalized tokens\n",
    "    for patt, repl in PHRASES:\n",
    "        s = re.sub(patt, repl if isinstance(repl, str) else repl, s, flags=re.IGNORECASE)\n",
    "    return s\n",
    "\n",
    "def clean_text(s):\n",
    "    # Full text cleaning pipeline for vectorization\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = apply_phrases(s)\n",
    "    s = basic_normalize(s)\n",
    "    toks = s.split()\n",
    "    toks = [t for t in toks if (t not in STOPWORDS and len(t) > 1)]\n",
    "    # Remove duplicates while keeping word order\n",
    "    seen, out = set(), []\n",
    "    for t in toks:\n",
    "        if t not in seen:\n",
    "            out.append(t); seen.add(t)\n",
    "    return \" \".join(out)\n",
    "\n",
    "# ------------------------------\n",
    "# Build combined content\n",
    "# ------------------------------\n",
    "def combine_categories(row):\n",
    "    # Combine primary + secondary + tertiary categories into one string\n",
    "    parts = [\n",
    "        str(row.get(\"primary_category\", \"\") or \"\"),\n",
    "        str(row.get(\"secondary_category\", \"\") or \"\"),\n",
    "        str(row.get(\"tertiary_category\", \"\") or \"\")\n",
    "    ]\n",
    "    return \" \".join([p for p in parts if p]).strip()\n",
    "\n",
    "def parse_highlights_text(val):\n",
    "    # Convert highlights list into plain text string\n",
    "    lst = to_list_from_highlights(val)\n",
    "    return \" \".join(lst)\n",
    "\n",
    "def build_product_content(row):\n",
    "    # Build final product_content field with weighted chunks\n",
    "    chunks = [\n",
    "        row.get(\"clean_product_name\", \"\"),\n",
    "        row.get(\"clean_brand_name\", \"\"),\n",
    "        row.get(\"clean_categories\", \"\"),\n",
    "        row.get(\"clean_highlights\", \"\"),\n",
    "        row.get(\"clean_ingredients\", \"\"),\n",
    "        row.get(\"clean_product_type\", \"\"),\n",
    "        row.get(\"clean_skin_type\", \"\"),\n",
    "        row.get(\"clean_skin_concern\", \"\"),\n",
    "    ]\n",
    "\n",
    "    # Weight ingredients higher (add twice more)\n",
    "    ingredients = row.get(\"clean_ingredients\", \"\")\n",
    "    chunks += [ingredients] * 2\n",
    "\n",
    "    return \" \".join([c for c in chunks if c]).strip()\n",
    "\n",
    "# ------------------------------\n",
    "# Main pipeline\n",
    "# ------------------------------\n",
    "def main():\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(IN_PATH, encoding=\"utf-8\")\n",
    "\n",
    "    # Defensive defaults: ensure required columns exist\n",
    "    for c in [\"product_id\",\"product_name\",\"brand_name\",\"ingredients\",\"highlights\",\n",
    "              \"primary_category\",\"secondary_category\",\"tertiary_category\",\"price_usd\", \"rating\", \"reviews\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\"\n",
    "\n",
    "    # (A) RULE-BASED ENRICHMENT \n",
    "    df[\"product_type\"] = df.apply(detect_product_type, axis=1)  # infer product type\n",
    "    df[\"_highlights_list\"] = df[\"highlights\"].apply(to_list_from_highlights)  # parse highlights\n",
    "    df[\"skin_type\"] = df.apply(  # infer skin type\n",
    "        lambda r: extract_skin_types_from_highlights_and_name(r[\"_highlights_list\"], r[\"product_name\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    df[\"ingredients\"] = df[\"ingredients\"].fillna(\"\").astype(str)  # ensure string ingredients\n",
    "\n",
    "    # Skin concern = union of claims/name + ingredients\n",
    "    concerns_from_claims = df.apply(\n",
    "        lambda r: extract_concerns_from_highlights_and_name(r[\"_highlights_list\"], r[\"product_name\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    concerns_from_ings = df[\"ingredients\"].apply(extract_concerns_from_ingredients)\n",
    "    df[\"skin_concern\"] = [\n",
    "        \",\".join(sorted(set(a) | set(b)))\n",
    "        for a, b in zip(concerns_from_claims, concerns_from_ings)\n",
    "    ]\n",
    "\n",
    "    # Convert price to numeric\n",
    "    df[\"price_usd\"] = pd.to_numeric(df[\"price_usd\"], errors=\"coerce\")\n",
    "\n",
    "    df[\"rating\"] = pd.to_numeric(df.get(\"rating\"), errors=\"coerce\")\n",
    "    df[\"reviews\"] = pd.to_numeric(df.get(\"reviews\"), errors=\"coerce\")\n",
    "\n",
    "    # (B) TEXT CLEANING + CONTENT \n",
    "    df[\"_categories_raw\"] = df.apply(combine_categories, axis=1)\n",
    "    df[\"_highlights_text\"] = df[\"highlights\"].apply(parse_highlights_text)\n",
    "\n",
    "    # Clean text fields\n",
    "    df[\"clean_product_name\"] = df[\"product_name\"].apply(clean_text)\n",
    "    df[\"clean_brand_name\"]   = df[\"brand_name\"].apply(clean_text)\n",
    "    df[\"clean_categories\"]   = df[\"_categories_raw\"].apply(clean_text)\n",
    "    df[\"clean_highlights\"]   = df[\"_highlights_text\"].apply(clean_text)\n",
    "    df[\"clean_ingredients\"]  = df[\"ingredients\"].apply(clean_text)\n",
    "    df[\"clean_product_type\"] = df[\"product_type\"].apply(clean_text)\n",
    "    df[\"clean_skin_type\"]    = df[\"skin_type\"].apply(clean_text)\n",
    "    df[\"clean_skin_concern\"] = df[\"skin_concern\"].apply(clean_text)\n",
    "\n",
    "    # Build final product_content field (for TF-IDF)\n",
    "    df[\"product_content\"] = df.apply(build_product_content, axis=1)\n",
    "\n",
    "    # Save final\n",
    "    keep_cols = [\n",
    "        \"product_id\",\"product_name\",\"brand_name\",\"ingredients\",\"highlights\",\n",
    "        \"primary_category\",\"secondary_category\",\"tertiary_category\",\"price_usd\", \"rating\", \"reviews\", \n",
    "        \"product_type\",\"skin_type\",\"skin_concern\",\"product_content\"\n",
    "    ]\n",
    "    for c in keep_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\"\n",
    "    df[keep_cols].to_csv(OUT_PATH, index=False)\n",
    "\n",
    "    # Debug sample output\n",
    "    if SHOW_DEBUG:\n",
    "        print(f\"Final products preprocessed file saved as: {OUT_PATH}\")\n",
    "        print(\"Sample product_content:\")\n",
    "        print(df[\"product_content\"].head(5).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e313f",
   "metadata": {},
   "source": [
    "### Merging Reciew and Product DataFrames Together based on product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e821a0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 801455 entries, 0 to 801454\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   author_id           801455 non-null  object \n",
      " 1   product_id          801455 non-null  object \n",
      " 2   product_name        801455 non-null  object \n",
      " 3   brand_name          801455 non-null  object \n",
      " 4   rating              801455 non-null  int64  \n",
      " 5   skin_type           801455 non-null  object \n",
      " 6   price_usd           801455 non-null  float64\n",
      " 7   secondary_category  801455 non-null  object \n",
      " 8   tertiary_category   801455 non-null  object \n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 55.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "reviews_df = merged_reviews_df[['author_id', 'product_id', 'product_name', 'brand_name','rating', 'skin_type']]\n",
    "\n",
    "df_review = reviews_df.merge(skincare_products_df[['product_id', 'price_usd', 'secondary_category', 'tertiary_category']], \n",
    "                            on='product_id', how='left')\n",
    "print(df_review.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "403ea16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users: 455845, Unique products: 1765\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Unique users: {df_review['author_id'].nunique()}, Unique products: {df_review['product_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7068b",
   "metadata": {},
   "source": [
    "Duplicated Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce58068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_reviews_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763769db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1838)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec580875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(798983, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review = df_review.drop_duplicates(subset=['author_id', 'product_id'], keep='last')\n",
    "df_review.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d12f61",
   "metadata": {},
   "source": [
    "Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfed5dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_id             0\n",
       "product_id            0\n",
       "product_name          0\n",
       "brand_name            0\n",
       "rating                0\n",
       "skin_type             0\n",
       "price_usd             0\n",
       "secondary_category    0\n",
       "tertiary_category     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311eac3",
   "metadata": {},
   "source": [
    "Convert to lovercase and replace unknown to the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1475594",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['skin_type'] = df_review['skin_type'].str.lower().fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2953e",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd4679",
   "metadata": {},
   "source": [
    "## Content-Based Filtering - Yap Zi Wen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdebca4",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1297b0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        product_name  \\\n",
      "0               GENIUS Sleeping Collagen Moisturizer   \n",
      "1                       GENIUS Liquid Collagen Serum   \n",
      "2            Triple Algae Eye Renewal Balm Eye Cream   \n",
      "3  SUBLIME DEFENSE Ultra Lightweight UV Defense F...   \n",
      "4                   GENIUS Ultimate Anti-Aging Cream   \n",
      "\n",
      "                                     product_content  \n",
      "0  genius sleeping collagen moisturizer algenist ...  \n",
      "1  genius liquid collagen serum algenist skincare...  \n",
      "2  triple algae eye renewal balm cream algenist s...  \n",
      "3  sublime defense ultra lightweight uv fluid spf...  \n",
      "4  genius ultimate anti-aging cream algenist skin...  \n",
      "TF-IDF shape: (1803, 4872)\n",
      "Feature names: ['100h' '10eicosanedioate' '14m' '15ml' '1st' '20k' '24k' '25th' '360o'\n",
      " '3d' '40b' '5x' '72h' '7m' 'aa' 'abeille' 'abeitate' 'abelmo'\n",
      " 'abelmoschus' 'abies' 'abietata' 'abrial' 'absinthium' 'absolue'\n",
      " 'absolute' 'absorbing' 'absorption' 'abyssinica' 'ac' 'acacia' 'acacial'\n",
      " 'acai' 'acanthium' 'acanthopanax' 'acephala' 'acer' 'acerola' 'acerosa'\n",
      " 'acetal' 'acetamidoethoxyethanol' 'acetate' 'acetic' 'acetoacetate'\n",
      " 'acetone' 'acetophenone' 'acetyl' 'acetylarginyltryptophyl' 'acetylated'\n",
      " 'acetylcysteine' 'acetylheptapeptide']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv(\"data/CleanedDataSet/products_preprocessed.csv\")\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['product_content'])\n",
    "\n",
    "# for checking purpose\n",
    "print(df[['product_name', 'product_content']].head())\n",
    "print(\"TF-IDF shape: \" + str(tfidf_matrix.shape))\n",
    "print(\"Feature names: \" + str(vectorizer.get_feature_names_out()[:50]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc0993",
   "metadata": {},
   "source": [
    "### Recommending Skincare Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34c3f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# quick lookup for query product index\n",
    "name_to_idx = {str(s).strip().lower(): i for i, s in enumerate(df[\"product_name\"].fillna(\"\"))}\n",
    "\n",
    "def _to_set(x):\n",
    "    \"\"\"Normalize None / list / csv-string to a lowercase set.\"\"\"\n",
    "    if x is None: return set()\n",
    "    if isinstance(x, (list, tuple, set, pd.Series, np.ndarray)):\n",
    "        return {str(t).strip().lower() for t in x if pd.notna(t) and str(t).strip()}\n",
    "    s = str(x).strip()\n",
    "    if not s or s.lower() == \"nan\": return set()\n",
    "    return {t.strip().lower() for t in s.split(\",\") if t.strip()}\n",
    "\n",
    "def content_recommend(\n",
    "    query_product: str | None = None,\n",
    "    product_type: str | None = None,\n",
    "    skin_type: str | None = None,                 # single-select\n",
    "    skin_concern: list[str] | set[str] | None = None,   # multi-select\n",
    "    max_price: float | None = None,\n",
    "    n: int = 10,\n",
    "    blend_alpha: float = 0.75,   # how much to trust similarity vs. rating/popularity\n",
    "    min_reviews_for_boost: int = 10\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # ------------------ compute similarity vector (if query given) ------------------\n",
    "    sim = None\n",
    "    if query_product:\n",
    "        key = str(query_product).strip().lower()\n",
    "        if key in name_to_idx:\n",
    "            qidx = name_to_idx[key]\n",
    "            sim = cosine_similarity(tfidf_matrix[qidx:qidx+1], tfidf_matrix).ravel()\n",
    "        else:\n",
    "            # if query not found, we proceed without similarity\n",
    "            sim = None\n",
    "\n",
    "    # ------------------ build candidate mask from filters ------------------\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "\n",
    "    if product_type:\n",
    "        req_type = str(product_type).strip().lower()\n",
    "        mask &= (df[\"product_type\"].fillna(\"\").str.strip().str.lower() == req_type)\n",
    "\n",
    "    if skin_type:\n",
    "        req_skin = str(skin_type).strip().lower()\n",
    "        mask &= df[\"skin_type\"].fillna(\"\").apply(\n",
    "            lambda s: req_skin in {t.strip().lower() for t in str(s).split(\",\") if t.strip()}\n",
    "        )\n",
    "\n",
    "    req_concerns = _to_set(skin_concern)\n",
    "    if req_concerns:\n",
    "        mask &= df[\"skin_concern\"].fillna(\"\").apply(\n",
    "            lambda s: req_concerns.issubset({t.strip().lower() for t in str(s).split(\",\") if t.strip()})\n",
    "        )\n",
    "\n",
    "    if max_price is not None and \"price_usd\" in df.columns:\n",
    "        mask &= pd.to_numeric(df[\"price_usd\"], errors=\"coerce\").fillna(np.inf) <= float(max_price)\n",
    "\n",
    "    candidates = df[mask].copy()\n",
    "    if candidates.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # ------------------ build final score ------------------\n",
    "    # similarity part (if available)\n",
    "    if sim is not None:\n",
    "        candidates = candidates.assign(similarity=sim[candidates.index].astype(float))\n",
    "    else:\n",
    "        candidates = candidates.assign(similarity=0.0)\n",
    "\n",
    "    # popularity part (rating + review_count)\n",
    "    rating = pd.to_numeric(candidates.get(\"rating\", np.nan), errors=\"coerce\")\n",
    "    reviews = pd.to_numeric(candidates.get(\"reviews\", 0), errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # simple popularity score: mean-centered rating * log(1+reviews)\n",
    "    pop = (rating.fillna(rating.mean())) * np.log1p(np.maximum(reviews, 0))\n",
    "    # normalize both components to [0,1] to blend sensibly\n",
    "    def _minmax(x):\n",
    "        x = x.astype(float)\n",
    "        mn, mx = np.nanmin(x), np.nanmax(x)\n",
    "        if not np.isfinite(mn) or not np.isfinite(mx) or mx <= mn:\n",
    "            return np.zeros_like(x, dtype=float)\n",
    "        return (x - mn) / (mx - mn)\n",
    "\n",
    "    sim_norm = _minmax(candidates[\"similarity\"].values)\n",
    "    pop_norm = _minmax(pop.values)\n",
    "\n",
    "    # downweight popularity if there are too few reviews\n",
    "    review_boost = np.clip(reviews / max(min_reviews_for_boost, 1), 0, 1.0)\n",
    "    pop_norm = pop_norm * review_boost\n",
    "\n",
    "    final_score = blend_alpha * sim_norm + (1.0 - blend_alpha) * pop_norm\n",
    "    candidates = candidates.assign(score=final_score)\n",
    "\n",
    "    # ------------------ sort & format ------------------\n",
    "    # if we used a query, skip the query item itself\n",
    "    if query_product and sim is not None:\n",
    "        key = str(query_product).strip().lower()\n",
    "        if key in name_to_idx:\n",
    "            candidates = candidates[candidates.index != name_to_idx[key]]\n",
    "\n",
    "    out = candidates.sort_values([\"score\", \"similarity\", \"rating\", \"reviews\"], ascending=False).head(n)\n",
    "\n",
    "    # nice output columns\n",
    "    cols = [\"product_id\",\"product_name\",\"brand_name\",\"product_type\",\"skin_type\",\"skin_concern\",\n",
    "            \"price_usd\",\"rating\",\"review_count\",\"similarity\",\"score\"]\n",
    "    show = [c for c in cols if c in out.columns]\n",
    "    return out[show].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0669e111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>product_type</th>\n",
       "      <th>skin_type</th>\n",
       "      <th>skin_concern</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>rating</th>\n",
       "      <th>similarity</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P453818</td>\n",
       "      <td>GENIUS Collagen Calming Relief</td>\n",
       "      <td>Algenist</td>\n",
       "      <td>moisturizer</td>\n",
       "      <td>combination,dry,normal,sensitive</td>\n",
       "      <td>aging,dehydration,dullness,hyperpigmentation,r...</td>\n",
       "      <td>58.00</td>\n",
       "      <td>4.4640</td>\n",
       "      <td>0.372106</td>\n",
       "      <td>0.400237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P411403</td>\n",
       "      <td>Confidence in a Cream Anti-Aging Hydrating Moi...</td>\n",
       "      <td>IT Cosmetics</td>\n",
       "      <td>moisturizer</td>\n",
       "      <td>combination,dry,normal</td>\n",
       "      <td>aging,dehydration,dullness,redness,texture</td>\n",
       "      <td>20.00</td>\n",
       "      <td>4.6200</td>\n",
       "      <td>0.189315</td>\n",
       "      <td>0.349495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P248407</td>\n",
       "      <td>Ultra Repair Cream Intense Hydration</td>\n",
       "      <td>First Aid Beauty</td>\n",
       "      <td>moisturizer</td>\n",
       "      <td>dry</td>\n",
       "      <td>dehydration,dullness,hyperpigmentation,redness</td>\n",
       "      <td>38.00</td>\n",
       "      <td>4.5200</td>\n",
       "      <td>0.134077</td>\n",
       "      <td>0.333196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P421996</td>\n",
       "      <td>Ultra Facial Moisturizing Cream with Squalane</td>\n",
       "      <td>Kiehl's Since 1851</td>\n",
       "      <td>moisturizer</td>\n",
       "      <td>combination,dry,normal</td>\n",
       "      <td>acne,dehydration,dullness,oil-control,pores,te...</td>\n",
       "      <td>78.00</td>\n",
       "      <td>4.4469</td>\n",
       "      <td>0.166320</td>\n",
       "      <td>0.323746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P475185</td>\n",
       "      <td>24-7 Moisture Hydrating Day &amp; Night Cream</td>\n",
       "      <td>TULA Skincare</td>\n",
       "      <td>moisturizer</td>\n",
       "      <td>combination,dry,normal</td>\n",
       "      <td>aging,dehydration,dullness,hyperpigmentation,t...</td>\n",
       "      <td>54.00</td>\n",
       "      <td>4.1884</td>\n",
       "      <td>0.236256</td>\n",
       "      <td>0.317278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P475203</td>\n",
       "      <td>Mini 24-7 Moisture Hydrating Day &amp; Night Cream</td>\n",
       "      <td>TULA Skincare</td>\n",
       "      <td>moisturizer</td>\n",
       "      <td>combination,dry,normal</td>\n",
       "      <td>aging,dehydration,dullness,hyperpigmentation,t...</td>\n",
       "      <td>26.00</td>\n",
       "      <td>4.1884</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>0.317166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P440312</td>\n",
       "      <td>Lotus Anti-Aging Night Moisturizer</td>\n",
       "      <td>fresh</td>\n",
       "      <td>moisturizer</td>\n",
       "      <td>combination,dry,normal</td>\n",
       "      <td>aging,dehydration</td>\n",
       "      <td>56.00</td>\n",
       "      <td>4.4843</td>\n",
       "      <td>0.164671</td>\n",
       "      <td>0.313202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P446930</td>\n",
       "      <td>Cream Skin Toner &amp; Moisturizer</td>\n",
       "      <td>LANEIGE</td>\n",
       "      <td>moisturizer</td>\n",
       "      <td>dry,sensitive</td>\n",
       "      <td>dehydration</td>\n",
       "      <td>33.00</td>\n",
       "      <td>4.4481</td>\n",
       "      <td>0.151063</td>\n",
       "      <td>0.303730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P436094</td>\n",
       "      <td>Evercalm Overnight Recovery Balm</td>\n",
       "      <td>REN Clean Skincare</td>\n",
       "      <td>moisturizer</td>\n",
       "      <td>combination,dry,normal</td>\n",
       "      <td>dehydration,dullness,redness,texture</td>\n",
       "      <td>55.00</td>\n",
       "      <td>4.4864</td>\n",
       "      <td>0.187106</td>\n",
       "      <td>0.302916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P455369</td>\n",
       "      <td>Peptide Moisturizer</td>\n",
       "      <td>The INKEY List</td>\n",
       "      <td>moisturizer</td>\n",
       "      <td>combination,dry,normal</td>\n",
       "      <td>aging,dehydration</td>\n",
       "      <td>15.99</td>\n",
       "      <td>3.7913</td>\n",
       "      <td>0.213639</td>\n",
       "      <td>0.300933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id                                       product_name  \\\n",
       "0    P453818                     GENIUS Collagen Calming Relief   \n",
       "1    P411403  Confidence in a Cream Anti-Aging Hydrating Moi...   \n",
       "2    P248407               Ultra Repair Cream Intense Hydration   \n",
       "3    P421996      Ultra Facial Moisturizing Cream with Squalane   \n",
       "4    P475185          24-7 Moisture Hydrating Day & Night Cream   \n",
       "5    P475203     Mini 24-7 Moisture Hydrating Day & Night Cream   \n",
       "6    P440312                 Lotus Anti-Aging Night Moisturizer   \n",
       "7    P446930                     Cream Skin Toner & Moisturizer   \n",
       "8    P436094                   Evercalm Overnight Recovery Balm   \n",
       "9    P455369                                Peptide Moisturizer   \n",
       "\n",
       "           brand_name product_type                         skin_type  \\\n",
       "0            Algenist  moisturizer  combination,dry,normal,sensitive   \n",
       "1        IT Cosmetics  moisturizer            combination,dry,normal   \n",
       "2    First Aid Beauty  moisturizer                               dry   \n",
       "3  Kiehl's Since 1851  moisturizer            combination,dry,normal   \n",
       "4       TULA Skincare  moisturizer            combination,dry,normal   \n",
       "5       TULA Skincare  moisturizer            combination,dry,normal   \n",
       "6               fresh  moisturizer            combination,dry,normal   \n",
       "7             LANEIGE  moisturizer                     dry,sensitive   \n",
       "8  REN Clean Skincare  moisturizer            combination,dry,normal   \n",
       "9      The INKEY List  moisturizer            combination,dry,normal   \n",
       "\n",
       "                                        skin_concern  price_usd  rating  \\\n",
       "0  aging,dehydration,dullness,hyperpigmentation,r...      58.00  4.4640   \n",
       "1         aging,dehydration,dullness,redness,texture      20.00  4.6200   \n",
       "2     dehydration,dullness,hyperpigmentation,redness      38.00  4.5200   \n",
       "3  acne,dehydration,dullness,oil-control,pores,te...      78.00  4.4469   \n",
       "4  aging,dehydration,dullness,hyperpigmentation,t...      54.00  4.1884   \n",
       "5  aging,dehydration,dullness,hyperpigmentation,t...      26.00  4.1884   \n",
       "6                                  aging,dehydration      56.00  4.4843   \n",
       "7                                        dehydration      33.00  4.4481   \n",
       "8               dehydration,dullness,redness,texture      55.00  4.4864   \n",
       "9                                  aging,dehydration      15.99  3.7913   \n",
       "\n",
       "   similarity     score  \n",
       "0    0.372106  0.400237  \n",
       "1    0.189315  0.349495  \n",
       "2    0.134077  0.333196  \n",
       "3    0.166320  0.323746  \n",
       "4    0.236256  0.317278  \n",
       "5    0.236111  0.317166  \n",
       "6    0.164671  0.313202  \n",
       "7    0.151063  0.303730  \n",
       "8    0.187106  0.302916  \n",
       "9    0.213639  0.300933  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_recommend(\n",
    "    query_product=\"GENIUS Sleeping Collagen Moisturizer\",\n",
    "    product_type=\"moisturizer\",\n",
    "    skin_type=\"dry\",\n",
    "    skin_concern=[\"dehydration\"],\n",
    "    max_price=100,\n",
    "    n=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1dad22",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9910d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# df = pd.read_csv(\"data/CleanedDataSet/products_preprocessed.csv\")\n",
    "# reviews = pd.read_csv(\"data/CleanedDataSet/filtered_skincare_reviews.csv\")\n",
    "\n",
    "def evaluate_all_metrics(\n",
    "    reviews: pd.DataFrame,\n",
    "    *,\n",
    "    # RMSE/MAE params\n",
    "    top_n: int = 5,\n",
    "    sample_size: int = 500,\n",
    "    liked_threshold: float = 3.5,\n",
    "    # Precision/Recall/F1 params\n",
    "    K: int = 10,\n",
    "    min_rating: float = 4.0,\n",
    "    min_reviews: int = 20,\n",
    "    max_items: int | None = None,\n",
    "    # relevance options\n",
    "    require_concern_overlap: bool = True,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "\n",
    "    # -----------------------------\n",
    "    # MSE / RMSE / MAE\n",
    "    # -----------------------------\n",
    "    # Validate reviews columns\n",
    "    for col in [\"author_id\", \"product_id\", \"rating\"]:\n",
    "        if col not in reviews.columns:\n",
    "            raise ValueError(f\"Missing column in reviews: {col}\")\n",
    "\n",
    "    # Ensure df has product_id (or synthesize one)\n",
    "    if \"product_id\" not in df.columns:\n",
    "        df[\"product_id\"] = np.arange(len(df))\n",
    "\n",
    "    rated = reviews.copy()\n",
    "    rated[\"rating\"] = pd.to_numeric(rated[\"rating\"], errors=\"coerce\")\n",
    "    rated = rated.dropna(subset=[\"author_id\", \"product_id\", \"rating\"])\n",
    "\n",
    "    valid_ids = set(df[\"product_id\"])\n",
    "    rated = rated[rated[\"product_id\"].isin(valid_ids)]\n",
    "\n",
    "    # users with >=2 liked items\n",
    "    likes = (rated[rated[\"rating\"] >= liked_threshold]\n",
    "             .groupby(\"author_id\")[\"product_id\"].apply(list).to_dict())\n",
    "    eligible = [(u, lst) for u, lst in likes.items() if len(set(lst)) >= 2]\n",
    "\n",
    "    # user sampling\n",
    "    if sample_size and sample_size < len(eligible):\n",
    "        idx = np.random.choice(len(eligible), size=sample_size, replace=False)\n",
    "        eligible = [eligible[i] for i in idx]\n",
    "\n",
    "    y_true, y_pred, abs_errs, skipped = [], [], [], 0\n",
    "\n",
    "    for user, liked in eligible:\n",
    "        liked = [pid for pid in liked if pid in valid_ids]\n",
    "        if len(liked) < 2:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        q = np.random.choice(liked)\n",
    "        qrow = df.loc[df[\"product_id\"] == q]\n",
    "        if qrow.empty:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        qrow = qrow.iloc[0]\n",
    "\n",
    "        recs = contentbased_recommender(\n",
    "            product_type=qrow.get(\"product_type\"),\n",
    "            skin_type=qrow.get(\"skin_type\"),\n",
    "            skin_concern=qrow.get(\"skin_concern\").split(\",\") if isinstance(qrow.get(\"skin_concern\"), str) else None,\n",
    "            concern_match=\"all\",\n",
    "            max_price=qrow.get(\"price_usd\"),\n",
    "            n=top_n\n",
    "        )\n",
    "        if recs is None or len(recs) == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        r_ratings = pd.to_numeric(recs.get(\"rating\"), errors=\"coerce\").fillna(0).to_numpy()\n",
    "        r_sims    = pd.to_numeric(recs.get(\"similarity\"), errors=\"coerce\").fillna(0).to_numpy()\n",
    "        if r_ratings.size == 0 or r_sims.sum() == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        pred_rating = float(np.average(r_ratings, weights=np.clip(r_sims, 1e-6, None)))\n",
    "\n",
    "        # ground truth (this user's rating for held-out product)\n",
    "        held = rated.loc[(rated[\"author_id\"] == user) & (rated[\"product_id\"] == q), \"rating\"]\n",
    "        if held.empty:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        true_rating = float(held.iloc[0])\n",
    "        y_true.append(true_rating)\n",
    "        y_pred.append(pred_rating)\n",
    "        abs_errs.append(abs(true_rating - pred_rating))\n",
    "\n",
    "    mse_val = mean_squared_error(y_true, y_pred) if y_true else None\n",
    "    rmse_val = sqrt(mse_val) if mse_val is not None else None\n",
    "    mae_val = float(np.mean(abs_errs)) if abs_errs else None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Precision / Recall / F1 (proxy)\n",
    "    # -----------------------------\n",
    "    df_local = df.copy()\n",
    "    # handle rating + review count column names\n",
    "    rating_col = \"rating\" if \"rating\" in df_local.columns else None\n",
    "    reviews_col = \"review_count\" if \"review_count\" in df_local.columns else (\"reviews\" if \"reviews\" in df_local.columns else None)\n",
    "    if rating_col is None:\n",
    "        raise ValueError(\"`df` needs a product-level 'rating' column for proxy relevance.\")\n",
    "    if reviews_col is None:\n",
    "        # create one if missing\n",
    "        df_local[\"review_count\"] = 0\n",
    "        reviews_col = \"review_count\"\n",
    "\n",
    "    df_local[rating_col]  = pd.to_numeric(df_local[rating_col], errors=\"coerce\")\n",
    "    df_local[reviews_col] = pd.to_numeric(df_local[reviews_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    items = df_local.index.tolist()\n",
    "    if max_items:\n",
    "        items = items[:max_items]\n",
    "\n",
    "    precisions, recalls, f1s, n_eval_rank = [], [], [], 0\n",
    "\n",
    "    for i in items:\n",
    "        q = df_local.iloc[i]\n",
    "        q_name = q.get(\"product_name\", None)\n",
    "        if not isinstance(q_name, str) or not q_name.strip():\n",
    "            continue\n",
    "\n",
    "        q_type = str(q.get(\"product_type\", \"\")).strip().lower()\n",
    "        q_skin = q.get(\"skin_type\", None)\n",
    "        q_concerns = {t.strip().lower() for t in str(q.get(\"skin_concern\", \"\")).split(\",\") if t.strip()}\n",
    "\n",
    "        rel = df_local[\n",
    "            (df_local.index != i) &\n",
    "            (df_local[\"product_type\"].fillna(\"\").str.strip().str.lower() == q_type) &\n",
    "            (df_local[rating_col] >= min_rating) &\n",
    "            (df_local[reviews_col] >= min_reviews)\n",
    "        ].copy()\n",
    "        if require_concern_overlap and q_concerns:\n",
    "            rel = rel[rel[\"skin_concern\"].fillna(\"\").apply(lambda s: len(q_concerns & {t.strip().lower() for t in str(s).split(\",\") if t.strip()}) > 0)]\n",
    "\n",
    "        if rel.empty:\n",
    "            continue\n",
    "        relevant_names = set(rel[\"product_name\"].tolist())\n",
    "\n",
    "        # Use query+filters recommender for ranking\n",
    "        recs = content_recommend(\n",
    "            query_product=q_name,\n",
    "            product_type=q_type or None,\n",
    "            skin_type=q_skin or None,\n",
    "            skin_concern=list(q_concerns) if q_concerns else None,\n",
    "            n=K\n",
    "        )\n",
    "        if recs is None or recs.empty:\n",
    "            continue\n",
    "\n",
    "        rec_names = set(recs[\"product_name\"].tolist())\n",
    "        hits = len(rec_names & relevant_names)\n",
    "\n",
    "        precision = hits / float(K)\n",
    "        recall = hits / float(len(relevant_names))\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        n_eval_rank += 1\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n=== Content-based Evaluation ===\")\n",
    "        if y_true:\n",
    "            print(f\"Users/items evaluated: {len(y_true)}  |  skipped: {skipped}\")\n",
    "            print(f\"MSE:  {mse_val:.4f}\")\n",
    "            print(f\"RMSE: {rmse_val:.4f}\")\n",
    "            print(f\"MAE:  {mae_val:.4f}\")\n",
    "        else:\n",
    "            print(\"No valid predictions made (check reviews coverage).\")\n",
    "\n",
    "        if n_eval_rank > 0:\n",
    "            print(f\"Queries evaluated: {n_eval_rank}\")\n",
    "            print(f\"Precision@{K}: {np.mean(precisions):.4f}\")\n",
    "            print(f\"Recall@{K}:    {np.mean(recalls):.4f}\")\n",
    "            print(f\"F1@{K}:        {np.mean(f1s):.4f}\")\n",
    "        else:\n",
    "            print(\"No valid queries for proxy evaluation (check thresholds).\")\n",
    "\n",
    "    return {\n",
    "        \"rating_prediction\": {\n",
    "            \"n_evaluated\": len(y_true),\n",
    "            \"skipped\": skipped,\n",
    "            \"mse\": mse_val,\n",
    "            \"rmse\": rmse_val,\n",
    "            \"mae\": mae_val,\n",
    "        },\n",
    "        \"ranking_proxy\": {\n",
    "            \"n_evaluated\": n_eval_rank,\n",
    "            f\"P@{K}\": float(np.mean(precisions)) if n_eval_rank>0 else None,\n",
    "            f\"R@{K}\": float(np.mean(recalls))    if n_eval_rank>0 else None,\n",
    "            f\"F1@{K}\": float(np.mean(f1s))      if n_eval_rank>0 else None,\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96b3a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joanne Wong\\AppData\\Local\\Temp\\ipykernel_16008\\3155181684.py:92: RuntimeWarning: All-NaN slice encountered\n",
      "  mn, mx = np.nanmin(x), np.nanmax(x)\n",
      "C:\\Users\\Joanne Wong\\AppData\\Local\\Temp\\ipykernel_16008\\3155181684.py:92: RuntimeWarning: All-NaN slice encountered\n",
      "  mn, mx = np.nanmin(x), np.nanmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Content-based Evaluation ===\n",
      "Users/items evaluated: 200  |  skipped: 0\n",
      "MSE:  0.5643\n",
      "RMSE: 0.7512\n",
      "MAE:  0.6725\n",
      "Queries evaluated: 134\n",
      "Precision@10: 0.6485\n",
      "Recall@10:    0.0393\n",
      "F1@10:        0.0711\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_metrics(\n",
    "    reviews,\n",
    "    top_n=5, sample_size=200, liked_threshold=3.5,   \n",
    "    K=10, min_rating=4.0, min_reviews=20,             \n",
    "    require_concern_overlap=True, max_items=None      \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39c56c",
   "metadata": {},
   "source": [
    "## Collaborative Filtering - Chang Kar Yan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49caf7",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f12ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import SVD, Dataset, Reader, accuracy\n",
    "from surprise.model_selection import GridSearchCV, train_test_split\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Prepare data\n",
    "df = df_review[['author_id', 'product_id', 'rating', 'product_name', 'brand_name', 'skin_type', 'price_usd', 'secondary_category', 'tertiary_category']].copy()\n",
    "\n",
    "# Validate ratings (15)\n",
    "df = df[df['rating'].between(1, 5)]\n",
    "print(f\"Shape after grouping and rating validation: {df.shape}\")\n",
    "print(f\"Unique users: {df['author_id'].nunique()}, Unique products: {df['product_id'].nunique()}\")\n",
    "\n",
    "# Filter sparse entries\n",
    "min_user_reviews = 5\n",
    "min_product_reviews = 10\n",
    "user_counts = df['author_id'].value_counts()\n",
    "product_counts = df['product_id'].value_counts()\n",
    "valid_users = user_counts[user_counts >= min_user_reviews].index\n",
    "valid_products = product_counts[product_counts >= min_product_reviews].index\n",
    "df = df[df['author_id'].isin(valid_users) & df['product_id'].isin(valid_products)]\n",
    "print(f\"Filtered shape (users{min_user_reviews}, products{min_product_reviews}): {df.shape}\")\n",
    "print(f\"Sparsity: {1 - len(df) / (df['author_id'].nunique() * df['product_id'].nunique()):.4f}\")\n",
    "\n",
    "# Check for sufficient data\n",
    "if df.empty:\n",
    "    raise ValueError(\"Filtered dataset is empty. Adjust min_user_reviews or min_product_reviews.\")\n",
    "\n",
    "# Check for NaN values\n",
    "if df[['author_id', 'product_id', 'rating']].isna().any().any():\n",
    "    print(\"Warning: NaN values found in critical columns. Dropping NaN rows.\")\n",
    "    df = df.dropna(subset=['author_id', 'product_id', 'rating'])\n",
    "\n",
    "# Save the training dataframe as CSV\n",
    "training_csv_filename = \"data/CleanedDataSet/collaborative_training_data.csv\"\n",
    "df.to_csv(training_csv_filename, index=False)\n",
    "print(f\"Training data saved as: {training_csv_filename}\")\n",
    "print(f\"Training CSV contains:\")\n",
    "print(f\"- {len(df)} records\")\n",
    "print(f\"- {df['author_id'].nunique()} unique users\")\n",
    "print(f\"- {df['product_id'].nunique()} unique products\")\n",
    "print(f\"- Columns: {list(df.columns)}\")\n",
    "\n",
    "# Verify the saved file\n",
    "verify_df = pd.read_csv(training_csv_filename)\n",
    "print(f\"\\nVerification - loaded CSV has {len(verify_df)} rows and {len(verify_df.columns)} columns\")\n",
    "\n",
    "# Load into Surprise\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['author_id', 'product_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# GridSearchCV for SVD with progress bar\n",
    "param_grid = {\n",
    "    'n_factors': [10, 20, 50],\n",
    "    'n_epochs': [20, 30],\n",
    "    'lr_all': [0.005, 0.01],\n",
    "    'reg_all': [0.1, 0.2]\n",
    "}\n",
    "gs_svd = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3, n_jobs=1)\n",
    "print(\"Running GridSearchCV for SVD...\")\n",
    "gs_svd.fit(data)\n",
    "print(\"\\nBest SVD Parameters:\")\n",
    "print(f\"RMSE: {gs_svd.best_score['rmse']:.3f}, Params: {gs_svd.best_params['rmse']}\")\n",
    "print(f\"MAE: {gs_svd.best_score['mae']:.3f}, Params: {gs_svd.best_params['mae']}\")\n",
    "\n",
    "# Train SVD with best parameters\n",
    "best_svd = SVD(\n",
    "    n_factors=gs_svd.best_params['rmse']['n_factors'],\n",
    "    n_epochs=gs_svd.best_params['rmse']['n_epochs'],\n",
    "    lr_all=gs_svd.best_params['rmse']['lr_all'],\n",
    "    reg_all=gs_svd.best_params['rmse']['reg_all'],\n",
    "    random_state=42\n",
    ")\n",
    "best_svd.fit(trainset)\n",
    "\n",
    "# Save the trained SVD model and trainset to .pkl files\n",
    "with open('models/svd_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_svd, f)\n",
    "with open('models/trainset.pkl', 'wb') as f:\n",
    "    pickle.dump(trainset, f)\n",
    "print(\"Model and trainset saved to 'models/svd_model.pkl' and 'models/trainset.pkl'.\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "svd_preds = best_svd.test(testset)\n",
    "svd_rmse = accuracy.rmse(svd_preds, verbose=True)\n",
    "svd_mae = accuracy.mae(svd_preds, verbose=True)\n",
    "svd_mse = np.mean([(pred.est - pred.r_ui) ** 2 for pred in svd_preds])\n",
    "svd_accuracy = np.mean([1 if round(pred.est) == round(pred.r_ui) else 0 for pred in svd_preds])\n",
    "\n",
    "# Precision@K, Recall@K, and F1@K with progress bar\n",
    "def precision_recall_f1_at_k(predictions, k=5, threshold=4.0):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for pred in tqdm(predictions, desc=\"Building user predictions\"):\n",
    "        user_est_true[pred.uid].append((pred.est, pred.r_ui))\n",
    "    \n",
    "    precisions, recalls = [], []\n",
    "    for uid, ratings in tqdm(user_est_true.items(), desc=\"Calculating metrics\"):\n",
    "        ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel = sum((true_r >= threshold) for _, true_r in ratings)\n",
    "        n_rec_k = sum((est >= threshold) for est, _ in ratings[:k])\n",
    "        n_rel_and_rec_k = sum((true_r >= threshold) for _, true_r in ratings[:k])\n",
    "        precision = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "        recall = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        if n_rec_k == 0 or n_rel == 0:\n",
    "            print(f\"User {uid}: Zero precision/recall (n_rec_k={n_rec_k}, n_rel={n_rel})\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) != 0 else 0\n",
    "    return avg_precision, avg_recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c14b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate Precision@K, Recall@K, and F1@K\n",
    "svd_prec, svd_rec, svd_f1 = precision_recall_f1_at_k(svd_preds, k=5, threshold=4.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nSVD Evaluation Metrics:\")\n",
    "print(f\"RMSE: {svd_rmse:.3f}\")\n",
    "print(f\"MAE: {svd_mae:.3f}\")\n",
    "print(f\"MSE: {svd_mse:.3f}\")\n",
    "print(f\"Accuracy: {svd_accuracy:.3f}\")\n",
    "print(f\"Precision@5: {svd_prec:.3f}\")\n",
    "print(f\"Recall@5: {svd_rec:.3f}\")\n",
    "print(f\"F1@5: {svd_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61787e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# User profile and top-N recommendations\n",
    "def get_user_profile_and_recommendations(model, trainset, user_id, n, df, filter_by_favorite_brands=False):\n",
    "    try:\n",
    "        # Check if user is new (no ratings)\n",
    "        user_data = df[df['author_id'] == user_id]\n",
    "        is_new_user = len(user_data) == 0\n",
    "        \n",
    "        # User profile\n",
    "        if is_new_user:\n",
    "            total_reviews = 0\n",
    "            avg_rating = 0.00\n",
    "            skin_type = \"Unknown\"\n",
    "            favorite_brands = []  # Empty for new users\n",
    "        else:\n",
    "            total_reviews = len(user_data)\n",
    "            avg_rating = user_data['rating'].mean()\n",
    "            skin_type = user_data['skin_type'].mode().iloc[0] if not user_data['skin_type'].isna().all() else \"unknown\"\n",
    "            favorite_brands = user_data['brand_name'].value_counts().head(3).index.tolist()\n",
    "        \n",
    "        # Recommendations\n",
    "        if is_new_user:\n",
    "            # For new user, recommend top-N items by average rating\n",
    "            item_ratings = df.groupby('product_id')['rating'].agg(['mean', 'count']).reset_index()\n",
    "            item_ratings = item_ratings[item_ratings['count'] >= min_product_reviews]\n",
    "            top_items = item_ratings.sort_values(by='mean', ascending=False).head(n)\n",
    "            rec_df = top_items[['product_id', 'mean']].rename(columns={'mean': 'predicted_rating'})\n",
    "            rec_df = rec_df.merge(\n",
    "                df[['product_id', 'brand_name', 'product_name', 'price_usd', 'secondary_category', 'tertiary_category']].drop_duplicates(),\n",
    "                on='product_id',\n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            # For existing user, use SVD predictions\n",
    "            inner_uid = trainset.to_inner_uid(user_id)\n",
    "            all_items = set(trainset.all_items())\n",
    "            rated_items = set(iid for (iid, _) in trainset.ur[inner_uid])\n",
    "            unrated_items = [trainset.to_raw_iid(iid) for iid in all_items - rated_items]\n",
    "            \n",
    "            if not unrated_items:\n",
    "                raise ValueError(f\"No unrated items available for user {user_id}.\")\n",
    "            \n",
    "            preds = [model.predict(user_id, iid) for iid in unrated_items]\n",
    "            top_n = sorted(preds, key=lambda x: x.est, reverse=True)[:max(n * 2, 10)]\n",
    "            recommendations = [(pred.iid, pred.est) for pred in top_n]\n",
    "            rec_df = pd.DataFrame(recommendations, columns=['product_id', 'predicted_rating'])\n",
    "            rec_df = rec_df.merge(\n",
    "                df[['product_id', 'brand_name', 'product_name', 'price_usd', 'secondary_category', 'tertiary_category']].drop_duplicates(),\n",
    "                on='product_id',\n",
    "                how='left'\n",
    "            )\n",
    "        \n",
    "        # Filter by favorite brands if requested and user is not new\n",
    "        if filter_by_favorite_brands and favorite_brands and not is_new_user:\n",
    "            rec_df = rec_df[rec_df['brand_name'].isin(favorite_brands)]\n",
    "        rec_df = rec_df.head(n)\n",
    "        \n",
    "        rec_df['predicted_rating'] = rec_df['predicted_rating'].clip(1, 5).round(1)\n",
    "        rec_df['price_usd'] = rec_df['price_usd'].round(1)\n",
    "        \n",
    "        # Format output\n",
    "        print(f\"\\nTesting SVD:\\n\")\n",
    "        print(f\" USER PROFILE: {user_id}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\" Total Reviews: {total_reviews}\")\n",
    "        print(f\" Average Rating Given: {avg_rating:.2f}\" if not is_new_user else \" Average Rating Given: N/A\")\n",
    "        print(f\" Skin Type: {skin_type}\")\n",
    "        print(f\" Favorite Brands: {', '.join(favorite_brands) if favorite_brands else 'None'}\")\n",
    "        print(f\"\\n Finding recommendations for user: {user_id}\")\n",
    "        print(f\" User has rated {total_reviews} products\")\n",
    "        print(f\"\\nTop-{n} Recommendations for User ID: {user_id} (SVD model)\")\n",
    "        if not rec_df.empty:\n",
    "            print(rec_df[['brand_name', 'product_name', 'price_usd', 'secondary_category', 'tertiary_category', 'predicted_rating']].to_string(index=False))\n",
    "        else:\n",
    "            print(f\"No recommendations generated for user {user_id}. Check if products are available after filtering.\")\n",
    "        \n",
    "        return rec_df\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return pd.DataFrame(columns=['product_id', 'brand_name', 'product_name', 'price_usd', 'secondary_category', 'tertiary_category', 'predicted_rating'])\n",
    "\n",
    "# Test with new user ID and top-N\n",
    "# sample_user = 'new_user'\n",
    "top_n = 5\n",
    "# get_user_profile_and_recommendations(best_svd, trainset, sample_user, top_n, df, filter_by_favorite_brands=False)\n",
    "# Test with existing user for comparison\n",
    "get_user_profile_and_recommendations(best_svd, trainset, '1238130325', top_n, df, filter_by_favorite_brands=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453370d6",
   "metadata": {},
   "source": [
    "## Hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6fcd8",
   "metadata": {},
   "source": [
    "### Content-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924416f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Clean review text\n",
    "# ----------------------------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)   # remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "merged_reviews_df[\"cleaned_review_text\"] = merged_reviews_df[\"review_text\"].apply(clean_text)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Add combined features to products\n",
    "# ----------------------------\n",
    "skincare_products_df[\"combined_features\"] = (\n",
    "    skincare_products_df[\"brand_name\"].fillna(\"\") + \" \" +\n",
    "    skincare_products_df[\"tertiary_category\"].fillna(\"\") + \" \" +\n",
    "    skincare_products_df[\"product_name\"].fillna(\"\") + \" \" +\n",
    "    skincare_products_df.get(\"ingredients\", pd.Series(\"\")).fillna(\"\") + \" \" +\n",
    "    skincare_products_df.get(\"highlights\", pd.Series(\"\")).fillna(\"\") + \" \" \n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Merge reviews with product info\n",
    "# ----------------------------\n",
    "merged_reviews_df = merged_reviews_df.drop(columns=[\"brand_name\", \"product_name\"], errors=\"ignore\")\n",
    "\n",
    "combined_df = pd.merge(\n",
    "    merged_reviews_df,\n",
    "    skincare_products_df[[\"product_id\", \"brand_name\", \"product_name\", \"combined_features\"]],\n",
    "    on=\"product_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Sentiment analysis\n",
    "# ----------------------------\n",
    "def analyze_sentiment(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return 0.0\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "combined_df[\"sentiment_score\"] = combined_df[\"cleaned_review_text\"].apply(analyze_sentiment)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Train/test split\n",
    "# ----------------------------\n",
    "train_df, test_df = train_test_split(\n",
    "    combined_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Save final datasets\n",
    "# ----------------------------\n",
    "try:\n",
    "    combined_df.to_csv(\"data/CleanedDataSet/combined_skincare_with_sentiment.csv\", index=False)\n",
    "    print(\"Combined dataset saved successfully\")\n",
    "except PermissionError:\n",
    "    print(\"Could not save combined dataset CSV (file may be open)\")\n",
    "\n",
    "try:\n",
    "    train_df.to_csv(\"data/CleanedDataSet/train_skincare.csv\", index=False)\n",
    "    test_df.to_csv(\"data/CleanedDataSet/test_skincare.csv\", index=False)\n",
    "    print(\"Train/test datasets saved successfully\")\n",
    "except PermissionError:\n",
    "    print(\"Could not save train/test CSV files (files may be open)\")\n",
    "\n",
    "print(\"Full preprocessing and train/test split completed!\")\n",
    "print(f\"Combined dataframe shape: {combined_df.shape}\")\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# Paths\n",
    "TRAIN_PATH = \"data/CleanedDataSet/train_skincare.csv\"\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"word2vec_skincare.model\")\n",
    "EMBEDDINGS_PATH = os.path.join(MODEL_DIR, \"product_embeddings.pkl\")\n",
    "\n",
    "# ---------------- 1) Load dataset ----------------\n",
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df = df.dropna(subset=[\"combined_features\"]).copy()\n",
    "    df[\"tokens\"] = df[\"combined_features\"].astype(str).str.lower().apply(word_tokenize)\n",
    "    return df\n",
    "\n",
    "# ---------------- 2) Extract tertiary category ----------------\n",
    "def extract_tertiary(feature_str):\n",
    "    \"\"\"\n",
    "     combined_features extract tertiary category\n",
    "    \"\"\"\n",
    "    feature_str = str(feature_str).lower()\n",
    "    \n",
    "    mapping = {\n",
    "        'moisturizer': 'Moisturizers',\n",
    "        'serum': 'Face Serums',\n",
    "        'eye cream': 'Eye Creams & Treatments',\n",
    "        'treatment': 'Blemish & Acne Treatments',\n",
    "        'lip balm': 'Lip Balms & Treatments',\n",
    "        'sunscreen': 'Face Sunscreen',\n",
    "        'cleanser': 'Face Wash & Cleansers',\n",
    "        'face wash': 'Face Wash & Cleansers',\n",
    "        'oil': 'Face Oils',\n",
    "        'toner': 'Toners',\n",
    "        'mask': 'Face Masks',\n",
    "        'peel': 'Facial Peels',\n",
    "        'exfoliator': 'Exfoliators',\n",
    "        'eye mask': 'Eye Masks',\n",
    "        'wipe': 'Face Wipes',\n",
    "        'night cream': 'Night Creams',\n",
    "        'mist': 'Mists & Essences',\n",
    "        'essence': 'Mists & Essences',\n",
    "        'sheet mask': 'Sheet Masks',\n",
    "        'makeup remover': 'Makeup Removers'\n",
    "    }\n",
    "    \n",
    "    for k, v in mapping.items():\n",
    "        if k in feature_str:\n",
    "            return v\n",
    "    return 'Other'\n",
    "\n",
    "# ---------------- 3) Train Word2Vec ----------------\n",
    "def train_w2v(token_lists, model_path=MODEL_PATH):\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    model = Word2Vec(\n",
    "        sentences=token_lists,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=4,\n",
    "        sg=1,       # Skip-gram\n",
    "        epochs= 80\n",
    "    )\n",
    "    model.save(model_path)\n",
    "    print(f\"Word2Vec model saved at: {model_path}\")\n",
    "    return model\n",
    "\n",
    "# ---------------- 4) Build embeddings ----------------\n",
    "def sentence_vec(tokens, model):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "def build_embeddings(df, model):\n",
    "    #  df  tertiary_category\n",
    "    if 'tertiary_category' not in df.columns:\n",
    "        df['tertiary_category'] = df['combined_features'].astype(str).apply(extract_tertiary)\n",
    "\n",
    "    prod = df.groupby(\"product_id\", as_index=False).agg({\n",
    "        \"brand_name\": \"first\",\n",
    "        \"product_name\": \"first\",\n",
    "        \"price_usd\": \"first\",\n",
    "        \"tokens\": \"first\",\n",
    "        \"tertiary_category\": \"first\"\n",
    "    })\n",
    "\n",
    "    prod[\"embedding\"] = prod[\"tokens\"].apply(lambda toks: sentence_vec(toks, model))\n",
    "    emb = np.vstack(prod[\"embedding\"].values)\n",
    "    return prod, emb\n",
    "\n",
    "# ---------------- 5) Save embeddings ----------------\n",
    "def save_embeddings(prod_df, prod_embeds, path=EMBEDDINGS_PATH):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    joblib.dump((prod_df, prod_embeds), path)\n",
    "    print(f\"Embeddings saved at: {path}\")\n",
    "\n",
    "# ---------------- 6) Run pipeline ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_dataset(TRAIN_PATH)\n",
    "    df['tertiary_category'] = df['combined_features'].astype(str).apply(extract_tertiary)\n",
    "\n",
    "    w2v = train_w2v(df[\"tokens\"].tolist())\n",
    "    prod_df, prod_embeds = build_embeddings(df, w2v)\n",
    "    save_embeddings(prod_df, prod_embeds)\n",
    "\n",
    "    print(f\"{len(prod_df)} unique products embedded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Recommendation function: content-based + same tertiary category\n",
    "def recommend_same_category(prod_df, prod_embeds, product_id, top_n=10):\n",
    "    # get targeted product\n",
    "    target = prod_df[prod_df[\"product_id\"] == product_id]\n",
    "    if target.empty:\n",
    "        raise ValueError(f\"Product ID {product_id} not found.\")\n",
    "    \n",
    "    # extract tertiary category\n",
    "    tc = target[\"tertiary_category\"].values[0] if \"tertiary_category\" in target else None\n",
    "    if tc is None:\n",
    "        raise ValueError(f\"Product ID {product_id} has no tertiary_category information.\")\n",
    "\n",
    "    # Only select the product that same tertiary category\n",
    "    candidates = prod_df[prod_df[\"tertiary_category\"] == tc]\n",
    "    \n",
    "    # \n",
    "    if len(candidates) <= 1:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"product_id\", \"brand_name\", \"product_name\", \"price_usd\", \"tertiary_category\", \"similarity_score\"\n",
    "        ])\n",
    "    \n",
    "    # \n",
    "    target_vec = target[\"embedding\"].values[0].reshape(1, -1)\n",
    "    cand_embeds = np.vstack(candidates[\"embedding\"].values)\n",
    "    sims = cosine_similarity(target_vec, cand_embeds)[0]\n",
    "\n",
    "    # Build DataFrame\n",
    "    recs = candidates.copy()\n",
    "    recs[\"similarity_score\"] = sims\n",
    "    recs = recs[recs[\"product_id\"] != product_id].sort_values(\"similarity_score\", ascending=False)\n",
    "    \n",
    "    return recs.head(top_n)[[\n",
    "        \"product_id\", \"brand_name\", \"product_name\", \"price_usd\", \"tertiary_category\", \"similarity_score\"\n",
    "    ]]\n",
    "\n",
    "# -------- Example usage --------\n",
    "recs = recommend_same_category(prod_df, prod_embeds, product_id=\"P454095\", top_n=5)\n",
    "print(recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b2101d",
   "metadata": {},
   "source": [
    "### Surpris SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset, Reader, SVD, accuracy, dump\n",
    "from surprise.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "print(\"Using Surprise-based collaborative filtering...\")\n",
    "\n",
    "# ---------------- 1) Load data ----------------\n",
    "df = pd.read_csv(\"data/CleanedDataSet/combined_skincare_with_sentiment.csv\", low_memory=False)\n",
    "\n",
    "print(f\"Data loaded: {df.shape}\")\n",
    "print(\"Columns available:\", df.columns.tolist())\n",
    "\n",
    "# ---------------- 2) Prepare final rating ----------------\n",
    "if \"sentiment_score\" in df.columns:\n",
    "    # Normalize sentiment_score (-1 to 1)  (0 to 5)\n",
    "    df[\"sentiment_normalized\"] = (df[\"sentiment_score\"] + 1) * 2.5\n",
    "    df[\"final_rating\"] = 0.7 * df[\"rating\"] + 0.3 * df[\"sentiment_normalized\"]\n",
    "else:\n",
    "    df[\"final_rating\"] = df[\"rating\"]\n",
    "\n",
    "print(f\"Final rating range: {df['final_rating'].min():.2f} to {df['final_rating'].max():.2f}\")\n",
    "\n",
    "# ---------------- 3) Build Surprise dataset ----------------\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[[\"author_id\", \"product_id\", \"final_rating\"]], reader)\n",
    "\n",
    "# ---------------- 4) Train-test split ----------------\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------- 5) Train SVD model ----------------\n",
    "print(\"Training Surprise SVD model...\")\n",
    "model = SVD(\n",
    "    n_factors=50,   # latent dimensions\n",
    "    lr_all=0.005,   # learning rate\n",
    "    reg_all=0.02,   # regularization\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(trainset)\n",
    "\n",
    "# ---------------- 6) Save Model ----------------\n",
    "print(\"Saving model...\")\n",
    "model_path = \"models/surprise_svd_model.pkl\"\n",
    "# Create directory if not exists\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "dump.dump(model_path, algo=model)\n",
    "print(f\" Model saved to {model_path}\")\n",
    "\n",
    "# ---------------- 7) Evaluate with RMSE ----------------\n",
    "predictions = model.test(testset)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\" Model evaluation completed. Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ---------------- 8) Top-N helper functions ----------------\n",
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\"\"\"\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # sort and get top N\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n\n",
    "\n",
    "def precision_recall_f1_coverage(predictions, n=10, threshold=3.5):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, F1 and coverage at Top-N recommendations.\n",
    "    threshold: rating above this is considered relevant.\n",
    "    \"\"\"\n",
    "    top_n = get_top_n(predictions, n=n)\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    all_recommended_items = set()\n",
    "\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        # Relevant items for this user (ground truth)\n",
    "        true_items = {iid for (uid_, iid, true_r, est, _) in predictions if uid_ == uid and true_r >= threshold}\n",
    "        recommended_items = {iid for (iid, est) in user_ratings}\n",
    "\n",
    "        all_recommended_items.update(recommended_items)\n",
    "\n",
    "        n_rel = len(true_items)  # relevant\n",
    "        n_rec_k = len(recommended_items)  # recommended in top N\n",
    "        n_rel_and_rec_k = len(true_items & recommended_items)\n",
    "\n",
    "        if n_rec_k > 0:\n",
    "            precisions.append(n_rel_and_rec_k / n_rec_k)\n",
    "        if n_rel > 0:\n",
    "            recalls.append(n_rel_and_rec_k / n_rel)\n",
    "\n",
    "    precision = np.mean(precisions) if precisions else 0\n",
    "    recall = np.mean(recalls) if recalls else 0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Coverage = proportion of unique items ever recommended\n",
    "    all_items = {iid for (_, iid, _, _, _) in predictions}\n",
    "    coverage = len(all_recommended_items) / len(all_items)\n",
    "\n",
    "    return precision, recall, f1, coverage\n",
    "\n",
    "# ---------------- 9) Compute evaluation metrics ----------------\n",
    "precision, recall, f1, coverage = precision_recall_f1_coverage(predictions, n=5, threshold=3.5)\n",
    "\n",
    "print(\"\\n Evaluation Metrics (Top-5):\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1-score:  {f1:.4f}\")\n",
    "print(f\"   Coverage:  {coverage:.4f}\")\n",
    "\n",
    "# ---------------- 10) Cold Start Helper Functions ----------------\n",
    "def get_popular_products(df, top_n=10, min_ratings=5):\n",
    "    \"\"\"Get popular products for cold start fallback\"\"\"\n",
    "    product_stats = df.groupby('product_id').agg({\n",
    "        'final_rating': ['count', 'mean']\n",
    "    }).round(2)\n",
    "    product_stats.columns = ['rating_count', 'avg_rating']\n",
    "    product_stats = product_stats.reset_index()\n",
    "    \n",
    "    # Filter products with enough ratings\n",
    "    qualified = product_stats[product_stats['rating_count'] >= min_ratings]\n",
    "    qualified = qualified.sort_values(['rating_count', 'avg_rating'], ascending=False)\n",
    "    \n",
    "    return qualified.head(top_n)\n",
    "\n",
    "def format_popular_recommendations(popular_products, df):\n",
    "    \"\"\"Format popular products recommendations\"\"\"\n",
    "    product_info = df[['product_id', 'product_name', 'brand_name']].drop_duplicates()\n",
    "    result = pd.merge(popular_products, product_info, on='product_id')\n",
    "    \n",
    "    print(\"\\n Popular Products (Cold Start Fallback):\")\n",
    "    print(\"-\" * 60)\n",
    "    recommendations = []\n",
    "    \n",
    "    for i, row in result.iterrows():\n",
    "        print(f\"{i+1}. {row['product_name']}\")\n",
    "        print(f\"   Brand: {row['brand_name']}\")\n",
    "        print(f\"   Avg Rating: {row['avg_rating']} ({row['rating_count']} ratings)\")\n",
    "        print()\n",
    "        recommendations.append((row['product_id'], row['avg_rating']))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# ---------------- 11) Recommend Product for existing user ----------------\n",
    "def recommend_products_surprise(user_id, df, model, top_n=5):\n",
    "    \"\"\"Recommend top N products for existing users only (no cold start)\"\"\"\n",
    "    user_id = str(user_id)\n",
    "\n",
    "    # Ensure user exists in dataset\n",
    "    if user_id not in df[\"author_id\"].values:\n",
    "        raise ValueError(f\"User {user_id} not found in dataset. SVD handles only existing users.\")\n",
    "\n",
    "    # Get all unique products and those the user has already rated\n",
    "    all_products = df[\"product_id\"].unique()\n",
    "    user_products = df[df[\"author_id\"] == user_id][\"product_id\"].unique()\n",
    "\n",
    "    predictions = []\n",
    "    for product_id in all_products:\n",
    "        if product_id not in user_products:\n",
    "            try:\n",
    "                pred = model.predict(user_id, str(product_id))\n",
    "                predictions.append((product_id, pred.est))\n",
    "            except:\n",
    "                continue  # skip if prediction fails\n",
    "\n",
    "    # Sort by predicted rating\n",
    "    top_predictions = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    # Get product details\n",
    "    product_info = df.drop_duplicates(\"product_id\")[[\"product_id\", \"product_name\", \"brand_name\"]]\n",
    "\n",
    "    print(f\"\\n Top {top_n} Recommendations for Existing User {user_id}:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (product_id, predicted_rating) in enumerate(top_predictions, 1):\n",
    "        product_row = product_info[product_info[\"product_id\"] == product_id]\n",
    "        if not product_row.empty:\n",
    "            name = product_row[\"product_name\"].values[0]\n",
    "            brand = product_row[\"brand_name\"].values[0]\n",
    "            print(f\"{i}. {name}\")\n",
    "            print(f\"   Brand: {brand}\")\n",
    "            print(f\"   Predicted Rating: {predicted_rating:.2f}\")\n",
    "        else:\n",
    "            print(f\"{i}. Product {product_id} - Details not available\")\n",
    "\n",
    "    return top_predictions\n",
    "# ---------------- 12) Test recommendation with both existing and new user ----------------\n",
    "if len(df[\"author_id\"].unique()) > 0:\n",
    "    # Test with existing user\n",
    "    sample_user = df[\"author_id\"].iloc[0]\n",
    "    print(\"Testing with existing user:\")\n",
    "    recs = recommend_products_surprise(sample_user, df, model, top_n=5)\n",
    "    \n",
    "    # Test with new user (cold start)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing Cold Start with new user:\")\n",
    "    new_user = \"new_user_12345\"\n",
    "    new_user_recs = recommend_products_surprise(new_user, df, model, top_n=5)\n",
    "else:\n",
    "    print(\"No users found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb4916",
   "metadata": {},
   "source": [
    "### Final Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from typing import Dict, List, Tuple\n",
    "from surprise import dump\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedHybridRecommender:\n",
    "    def __init__(self, train_path: str, test_path: str, products_path: str,\n",
    "                 content_model_path: str, svd_model_path: str):\n",
    "        \"\"\"\n",
    "        Hybrid Recommender: SVD + Content-based filtering with improvements\n",
    "        \"\"\"\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.products_path = products_path\n",
    "        self.content_model_path = content_model_path\n",
    "        self.svd_model_path = svd_model_path\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.prod_df = None\n",
    "        self.prod_embeds = None\n",
    "        self.svd_model = None\n",
    "        self.global_avg = 3.0\n",
    "        self.test_df = None\n",
    "        self.train_df = None\n",
    "        self.user_history_cache = {}\n",
    "        self.product_popularity = {}\n",
    "        self.product_features = {}\n",
    "        self.skin_profiles = {}\n",
    "        \n",
    "        # Load models and data\n",
    "        self._load_models()\n",
    "        self._preload_data()\n",
    "        \n",
    "        # ------------------------------\n",
    "        # Skin type + concern rules \n",
    "        # ------------------------------\n",
    "        # Regex patterns to detect skin types from text\n",
    "        self.SKIN_TYPE_PATTERNS = [\n",
    "            (r\"\\b(?:good|best)\\s*for:\\s*oily\\b\", \"oily\"),\n",
    "            (r\"\\b(?:good|best)\\s*for:\\s*dry\\b\", \"dry\"),\n",
    "            (r\"\\b(?:good|best)\\s*for:\\s*combination\\b\", \"combination\"),\n",
    "            (r\"\\b(?:good|best)\\s*for:\\s*sensitive\\b\", \"sensitive\"),\n",
    "            (r\"\\b(?:good|best)\\s*for:\\s*normal\\b\", \"normal\"),\n",
    "            (r\"\\b(oily skin|oily)\\b\", \"oily\"),\n",
    "            (r\"\\b(dry skin|dry)\\b\", \"dry\"),\n",
    "            (r\"\\b(combination skin|combination|combo)\\b\", \"combination\"),\n",
    "            (r\"\\b(sensitive skin|sensitive)\\b\", \"sensitive\"),\n",
    "            (r\"\\b(normal skin|normal)\\b\", \"normal\"),\n",
    "            (r\"\\bfor\\s+sensitive\\s+skin\\b\", \"sensitive\"),\n",
    "            (r\"\\bsuitable\\s+for\\s+sensitive\\b\", \"sensitive\"),\n",
    "            (r\"\\bfor\\s+sensitive\\b\", \"sensitive\"),\n",
    "            (r\"\\bhypoallergenic\\b\", \"sensitive\"),\n",
    "            (r\"\\bgentle\\b\", \"sensitive\"),\n",
    "        ]\n",
    "        # Regex patterns to detect skin concerns from claims or product name\n",
    "        self.SKIN_CONCERN_PATTERNS = [\n",
    "            (r\"\\b(acne|blemish|breakout|pimple)\\b\", \"acne\"),\n",
    "            (r\"\\bpores?\\b\", \"pores\"),\n",
    "            (r\"\\b(dark spot|hyperpigment|discoloration|melasma)\\b\", \"hyperpigmentation\"),\n",
    "            (r\"\\b(wrinkle|fine line|anti[- ]?aging|firming|loss of firmness|elasticity)\\b\", \"aging\"),\n",
    "            (r\"\\b(redness|rosacea|irritation|calming|soothing)\\b\", \"redness\"),\n",
    "            (r\"\\b(dryness|dehydration|hydrating|moisturizing|moisturising|barrier)\\b\", \"dehydration\"),\n",
    "            (r\"\\b(dull(ness)?|brighten(ing)?|glow|radiance)\\b\", \"dullness\"),\n",
    "            (r\"\\boil(y| control|iness)\\b\", \"oil-control\"),\n",
    "            (r\"\\b(blackhead|whitehead|congestion)\\b\", \"blackheads\"),\n",
    "            (r\"\\b(uneven (tone|texture)|texture|resurfacing)\\b\", \"texture\"),\n",
    "            (r\"\\b(dark circle|dark circles)\\b\", \"dark-circles\"),\n",
    "        ]\n",
    "\n",
    "        # Ingredient-based concern mapping\n",
    "        self.INGREDIENT_CONCERN_PATTERNS = [\n",
    "            (r\"\\b(salicylic acid|beta hydroxy|bha|willow bark|benzoyl peroxide|sulfur|zinc pca|zinc)\\b\", {\"acne\",\"pores\",\"oil-control\"}),\n",
    "            (r\"\\b(kaolin|bentonite|clay|charcoal)\\b\", {\"pores\",\"oil-control\"}),\n",
    "            (r\"\\b(tea tree|melaleuca)\\b\", {\"acne\"}),\n",
    "            (r\"\\b(hyaluronic acid|sodium hyaluronate|glycerin|panthenol|urea|betaine|trehalose|aloe)\\b\", {\"dehydration\"}),\n",
    "            (r\"\\b(ceramide|ceramides|cholesterol|squalane|squalene|shea|shea butter)\\b\", {\"dehydration\"}),\n",
    "            (r\"\\b(retinol|retinal|retinoate|bakuchiol|peptide|matrixyl|collagen|coenzyme ?q10|ubiquinone)\\b\", {\"aging\"}),\n",
    "            (r\"\\b(vitamin ?c|ascorbic|ascorbyl|ethyl ascorbic|magnesium ascorbyl|sodium ascorbyl|alpha arbutin|tranexamic|azelaic|kojic|licorice|glycyrrhiza)\\b\", {\"hyperpigmentation\",\"dullness\"}),\n",
    "            (r\"\\b(centella|cica|madecassoside|asiaticoside|allantoin|bisabolol|beta glucan|green tea|oat|colloidal oatmeal)\\b\", {\"redness\"}),\n",
    "            (r\"\\b(aha|glycolic|lactic|mandelic|tartaric|citric|pha|gluconolactone|lactobionic)\\b\", {\"texture\",\"dullness\"}),\n",
    "        ]\n",
    "        print(\" Enhanced Hybrid Recommender initialized successfully!\")\n",
    "\n",
    "    def _load_models(self) -> None:\n",
    "        \"\"\"Load pre-trained models\"\"\"\n",
    "        print(\" Loading pre-trained models...\")\n",
    "        \n",
    "        # Load content-based model\n",
    "        if os.path.exists(self.content_model_path):\n",
    "            self.prod_df, self.prod_embeds = joblib.load(self.content_model_path)\n",
    "            print(f\" Content model loaded: {len(self.prod_df)} products\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Content model not found at {self.content_model_path}\")\n",
    "        \n",
    "        # Load SVD model\n",
    "        if os.path.exists(self.svd_model_path):\n",
    "            _, self.svd_model = dump.load(self.svd_model_path)\n",
    "            print(\" SVD model loaded\")\n",
    "            \n",
    "            # Get global average from SVD\n",
    "            if hasattr(self.svd_model, 'trainset') and self.svd_model.trainset:\n",
    "                self.global_avg = self.svd_model.trainset.global_mean\n",
    "                print(f\" Global average rating from SVD: {self.global_avg:.3f}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"SVD model not found at {self.svd_model_path}\")\n",
    "        \n",
    "        # Create index mappings\n",
    "        self.product_id_to_idx = {str(pid): idx for idx, pid in enumerate(self.prod_df[\"product_id\"])}\n",
    "        print(\" Index mappings created\")\n",
    "        \n",
    "        # Precompute product features for faster similarity calculation\n",
    "        self.precompute_product_features()\n",
    "\n",
    "    def precompute_product_features(self):\n",
    "        \"\"\"Precompute product features for faster similarity calculation\"\"\"\n",
    "        print(\" Precomputing product features for faster similarity...\")\n",
    "        \n",
    "        self.product_features = {}\n",
    "        for _, row in self.prod_df.iterrows():\n",
    "            product_id = str(row[\"product_id\"])\n",
    "            self.product_features[product_id] = {\n",
    "                'brand': row[\"brand_name\"],\n",
    "                'category': row[\"tertiary_category\"],\n",
    "                'price': row[\"price_usd\"] if pd.notna(row[\"price_usd\"]) else 0,\n",
    "                'embedding': self.prod_embeds[self.product_id_to_idx[product_id]]\n",
    "            }\n",
    "        \n",
    "        print(f\" Precomputed features for {len(self.product_features)} products\")\n",
    "\n",
    "    def _preload_data(self):\n",
    "        print(\" Preloading data...\")\n",
    "\n",
    "        # Load a small sample just to detect available columns\n",
    "        test_sample = pd.read_csv(self.test_path, nrows=5)\n",
    "\n",
    "        # Base columns we always need\n",
    "        usecols = [\"author_id\", \"product_id\", \"rating\"]\n",
    "\n",
    "        #  Try to detect a timestamp column from the sample\n",
    "        time_col = None\n",
    "        for col in [\"timestamp\", \"submission_time\", \"review_date\"]:\n",
    "            if col in test_sample.columns:\n",
    "                time_col = col\n",
    "                usecols.append(col)\n",
    "                break\n",
    "\n",
    "        # Load full test/train datasets with only needed columns\n",
    "        self.test_df = pd.read_csv(self.test_path, usecols=usecols)\n",
    "        self.train_df = pd.read_csv(self.train_path, usecols=[\"author_id\", \"product_id\", \"rating\"])\n",
    "\n",
    "        # Build user history cache (from train)\n",
    "        self.user_histories = (\n",
    "            self.train_df.groupby(\"author_id\")[\"product_id\"].apply(list).to_dict()\n",
    "        )\n",
    "\n",
    "        print(f\" Test data loaded: {len(self.test_df)} records\")\n",
    "        print(f\" Train data loaded: {len(self.train_df)} records\")\n",
    "\n",
    "        \n",
    "        # Build user history cache - OPTIMIZED\n",
    "        print(\" Building user history cache...\")\n",
    "        all_ratings = pd.concat([self.train_df, self.test_df])\n",
    "        \n",
    "        # Use groupby for faster processing\n",
    "        user_groups = all_ratings.groupby(\"author_id\")\n",
    "        for user_id, group in tqdm(user_groups, desc=\"Caching user histories\"):\n",
    "            self.user_history_cache[str(user_id)] = {\n",
    "                'rated_products': group[\"product_id\"].astype(str).tolist(),\n",
    "                'ratings': group[\"rating\"].tolist(),\n",
    "                'avg_rating': group[\"rating\"].mean()\n",
    "            }\n",
    "        \n",
    "        # Calculate product popularity\n",
    "        print(\" Calculating product popularity...\")\n",
    "        # FIXED: Changed ast(str) to astype(str)\n",
    "        self.product_popularity = all_ratings['product_id'].astype(str).value_counts().to_dict()\n",
    "        \n",
    "        print(\"=== Dataset Overview ===\")\n",
    "        print(f\"Train set: {len(self.train_df):,} rows | {self.train_df['author_id'].nunique():,} users | {self.train_df['product_id'].nunique():,} products\")\n",
    "        print(f\"Test set: {len(self.test_df):,} rows | {self.test_df['author_id'].nunique():,} users | {self.test_df['product_id'].nunique():,} products\")\n",
    "        print(f\"Products catalog: {len(self.prod_df):,} items\")\n",
    "        print(\"========================\")\n",
    "\n",
    "    def add_skin_profile(self, user_id: str, skin_data: Dict):\n",
    "        \"\"\"\"\"\"\n",
    "        user_id = str(user_id)\n",
    "        self.skin_profiles[user_id] = skin_data\n",
    "        print(f\" Added skin profile for user {user_id}\")\n",
    "        print(f\"   Skin type: {skin_data.get('skin_type', 'N/A')}\")\n",
    "        print(f\"   Concerns: {skin_data.get('concerns', 'N/A')}\")\n",
    "        print(f\"   Budget: {skin_data.get('budget', 'N/A')}\")\n",
    "\n",
    "\n",
    "    def _extract_skin_tags(self, text: str):\n",
    "        \"\"\"\n",
    "        Extract possible skin types and skin concerns from text.\n",
    "        Returns two lists: (matched_types, matched_concerns).\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "\n",
    "        # Define possible tags (expand if needed)\n",
    "        skin_types = [\"oily\", \"dry\", \"combination\", \"normal\", \"sensitive\"]\n",
    "        skin_concerns = [\n",
    "            \"acne\", \"wrinkles\", \"fine lines\", \"dark spots\", \n",
    "            \"pigmentation\", \"redness\", \"pores\", \"dullness\", \n",
    "            \"eczema\", \"rosacea\", \"hydration\", \"blackheads\"\n",
    "        ]\n",
    "\n",
    "        matched_types = [t for t in skin_types if t in text]\n",
    "        matched_concerns = [c for c in skin_concerns if c in text]\n",
    "\n",
    "        return matched_types, matched_concerns\n",
    "\n",
    "\n",
    "    def _budget_range(self, budget_str: str):\n",
    "        \"\"\"Convert budget category string into (min_price, max_price).\"\"\"\n",
    "        if not budget_str:\n",
    "            return 0, float(\"inf\")\n",
    "\n",
    "        budget_str = budget_str.strip().lower()\n",
    "        if \"no budget\" in budget_str:       #  handles \"No budget limit\"\n",
    "            return 0, float(\"inf\")\n",
    "        elif \"under\" in budget_str:\n",
    "            return 0, 25\n",
    "        elif \"$25\" in budget_str and \"$50\" in budget_str:\n",
    "            return 25, 50\n",
    "        elif \"$50\" in budget_str and \"$100\" in budget_str:\n",
    "            return 50, 100\n",
    "        elif \"over\" in budget_str or \"above\" in budget_str:\n",
    "            return 100, float(\"inf\")\n",
    "        return 0, float(\"inf\")  # fallback\n",
    "\n",
    "\n",
    "\n",
    "    def filter_by_skin_profile(self, product_id: int, user_id: int):\n",
    "        \"\"\"\n",
    "        Soft filtering: adjust score multiplier based on match\n",
    "        between product tags and user's skin profile.\n",
    "        \"\"\"\n",
    "        user_id = str(user_id)\n",
    "        if user_id not in self.skin_profiles:\n",
    "            return 1.0  # no profile  neutral\n",
    "\n",
    "        user_profile = self.skin_profiles[user_id]\n",
    "        user_type = user_profile.get(\"skin_type\", \"\").lower()\n",
    "        \n",
    "        #  Handle both string and list for concerns\n",
    "        concerns = user_profile.get(\"concerns\", [])\n",
    "        if isinstance(concerns, str):\n",
    "            user_concerns = [concerns.lower()]\n",
    "        else:\n",
    "            user_concerns = [c.lower() for c in concerns]\n",
    "\n",
    "        user_budget = user_profile.get(\"budget\", \"\")\n",
    "\n",
    "        # Get product details\n",
    "        product = self.prod_df[self.prod_df[\"product_id\"].astype(str) == str(product_id)].iloc[0]\n",
    "        product_text = \" \".join([\n",
    "            str(product.get(\"product_name\", \"\")),\n",
    "            str(product.get(\"combined_features\", \"\")),\n",
    "            str(product.get(\"ingredients\", \"\")),\n",
    "            str(product.get(\"claims\", \"\")),\n",
    "        ])\n",
    "        product_price = product.get(\"price_usd\", 0)\n",
    "\n",
    "        # Extract tags from product text\n",
    "        matched_types, matched_concerns = self._extract_skin_tags(product_text)\n",
    "\n",
    "        # Start with neutral multiplier\n",
    "        multiplier = 1.0\n",
    "\n",
    "        # Skin type match\n",
    "        if user_type in matched_types:\n",
    "            multiplier *= 1.2\n",
    "        elif user_type:  # mismatch but user specified\n",
    "            multiplier *= 0.9\n",
    "\n",
    "        # Skin concern match ( supports multiple concerns)\n",
    "        if any(c in matched_concerns for c in user_concerns):\n",
    "            multiplier *= 1.3\n",
    "        elif user_concerns:  # user specified but no match\n",
    "            multiplier *= 0.85\n",
    "\n",
    "        # Budget consideration\n",
    "        min_budget, max_budget = self._budget_range(user_budget)\n",
    "        if min_budget <= product_price <= max_budget:\n",
    "            multiplier *= 1.1\n",
    "        else:\n",
    "            multiplier *= 0.7\n",
    "\n",
    "        return max(0.3, min(multiplier, 2.0))\n",
    "\n",
    "\n",
    "\n",
    "    def enhanced_content_similarity(self, target_product_id: str, user_rated_products: List[str]) -> float:\n",
    "        \"\"\"Enhanced content similarity with multiple factors - OPTIMIZED using precomputed features\"\"\"\n",
    "        if target_product_id not in self.product_features or not user_rated_products:\n",
    "            return 0.0\n",
    "        \n",
    "        target_features = self.product_features[target_product_id]\n",
    "        target_embed = target_features['embedding']\n",
    "        target_brand = target_features['brand']\n",
    "        target_category = target_features['category']\n",
    "        target_price = target_features['price']\n",
    "        \n",
    "        similarities = []\n",
    "        \n",
    "        # Pre-filter rated products that exist in our features\n",
    "        valid_rated_products = [pid for pid in user_rated_products if pid in self.product_features]\n",
    "        \n",
    "        for rated_pid in valid_rated_products:\n",
    "            rated_features = self.product_features[rated_pid]\n",
    "            rated_embed = rated_features['embedding']\n",
    "            rated_brand = rated_features['brand']\n",
    "            rated_category = rated_features['category']\n",
    "            rated_price = rated_features['price']\n",
    "            \n",
    "            # Multiple similarity measures\n",
    "            cosine_sim = cosine_similarity([target_embed], [rated_embed])[0][0]\n",
    "            \n",
    "            # Brand similarity\n",
    "            brand_sim = 0.3 if target_brand == rated_brand else 0\n",
    "            \n",
    "            # Category similarity\n",
    "            category_sim = 0.2 if target_category == rated_category else 0\n",
    "            \n",
    "            # Price similarity (within 20% price range)\n",
    "            if target_price > 0 and rated_price > 0:\n",
    "                price_ratio = min(target_price, rated_price) / max(target_price, rated_price)\n",
    "                price_sim = 0.2 if price_ratio > 0.8 else 0\n",
    "            else:\n",
    "                price_sim = 0\n",
    "            \n",
    "            total_sim = cosine_sim + brand_sim + category_sim + price_sim\n",
    "            if total_sim > 0.3:  # Higher threshold for better quality\n",
    "                similarities.append(total_sim)\n",
    "        \n",
    "        return np.mean(similarities) if similarities else 0.0\n",
    "    \n",
    "    def get_adaptive_weights(self, user_id):\n",
    "        \"\"\"Adaptively set weights based on user activity\"\"\"\n",
    "        if user_id not in self.user_item_matrix.index:\n",
    "            return 0.6, 0.4  # fallback for new user (more content-based)\n",
    "\n",
    "        n_ratings = self.train_df[self.train_df[\"author_id\"] == user_id].shape[0]\n",
    "\n",
    "        # More ratings  trust collaborative filtering more\n",
    "        if n_ratings < 5:\n",
    "            return 0.6, 0.4   # content-heavy\n",
    "        elif n_ratings < 20:\n",
    "            return 0.4, 0.6   # balanced\n",
    "        else:\n",
    "            return 0.2, 0.8   # collab-heavy\n",
    "\n",
    "\n",
    "    def hybrid_predict(self, user_id: str, product_id: str,\n",
    "                   content_weight: float = 0.4, collab_weight: float = 0.6) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Enhanced hybrid prediction with smooth adaptive weighting\n",
    "        Returns: (prediction, confidence)\n",
    "        \"\"\"\n",
    "        user_id = str(user_id)\n",
    "        product_id = str(product_id)\n",
    "        \n",
    "        # ========== SVD PREDICTION ==========\n",
    "        svd_pred = np.nan\n",
    "        svd_confidence = 0.0\n",
    "        \n",
    "        try:\n",
    "            svd_prediction = self.svd_model.predict(user_id, product_id)\n",
    "            svd_pred = max(1.0, min(5.0, svd_prediction.est))\n",
    "            svd_confidence = 0.9 if not svd_prediction.details.get('was_impossible', False) else 0.4\n",
    "        except:\n",
    "            svd_pred = self.global_avg\n",
    "            svd_confidence = 0.3\n",
    "        \n",
    "        # ========== CONTENT PREDICTION ==========\n",
    "        content_pred = np.nan\n",
    "        content_confidence = 0.0\n",
    "        \n",
    "        if user_id in self.user_history_cache:\n",
    "            user_data = self.user_history_cache[user_id]\n",
    "            rated_products = user_data['rated_products']\n",
    "            \n",
    "            if len(rated_products) >= 2 and product_id in self.product_id_to_idx:\n",
    "                similarity_score = self.enhanced_content_similarity(product_id, rated_products)\n",
    "                \n",
    "                if similarity_score > 0.1:\n",
    "                    # Map similarity to rating scale (1-5)\n",
    "                    content_pred = 1.0 + similarity_score * 4.0\n",
    "                    content_confidence = min(1.0, similarity_score * 1.8)\n",
    "                    content_pred = max(1.0, min(5.0, content_pred))\n",
    "        \n",
    "        # ========== ADVANCED HYBRID COMBINATION ==========\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        weights = []\n",
    "        \n",
    "        user_data = self.user_history_cache.get(user_id, {})\n",
    "        user_rating_count = len(user_data.get('rated_products', []))\n",
    "        \n",
    "        # Smooth scaling: more ratings  more collaborative\n",
    "        ratio = min(1.0, user_rating_count / 30)  # cap effect at 30 ratings\n",
    "        effective_collab_weight = collab_weight * (0.4 + 0.6 * ratio)  # grows with activity\n",
    "        effective_content_weight = content_weight * (1.0 - 0.6 * ratio)  # shrinks with activity\n",
    "        \n",
    "        if not np.isnan(svd_pred):\n",
    "            predictions.append(svd_pred)\n",
    "            confidences.append(svd_confidence)\n",
    "            weights.append(effective_collab_weight)\n",
    "        \n",
    "        if not np.isnan(content_pred) and content_confidence > 0.2:\n",
    "            predictions.append(content_pred)\n",
    "            confidences.append(content_confidence)\n",
    "            weights.append(effective_content_weight)\n",
    "        \n",
    "        if len(predictions) == 2:\n",
    "            total_confidence = sum(c * w for c, w in zip(confidences, weights))\n",
    "            weighted_pred = sum(p * c * w for p, c, w in zip(predictions, confidences, weights)) / total_confidence\n",
    "            final_confidence = total_confidence / sum(weights)\n",
    "        elif len(predictions) == 1:\n",
    "            weighted_pred = predictions[0]\n",
    "            final_confidence = confidences[0]\n",
    "        else:\n",
    "            # Fallback: use user's avg rating or global avg with small jitter\n",
    "            weighted_pred = user_data.get('avg_rating', self.global_avg) + np.random.uniform(-0.2, 0.2)\n",
    "            weighted_pred = max(1.0, min(5.0, weighted_pred))\n",
    "            final_confidence = 0.2\n",
    "        \n",
    "        return max(1.0, min(5.0, weighted_pred)), final_confidence\n",
    "\n",
    "    def calculate_match_percentage(self, score: float, user_id: str, product_id: str, \n",
    "                             all_recommendation_scores: List[float] = None) -> int:\n",
    "        \"\"\"Improved match percentage with relative scoring\"\"\"\n",
    "        \n",
    "        if all_recommendation_scores:\n",
    "            # Use percentile ranking within current recommendations\n",
    "            sorted_scores = sorted(all_recommendation_scores)\n",
    "            position = sorted_scores.index(score)\n",
    "            percentile = (position / len(sorted_scores)) * 100\n",
    "            return int(percentile)\n",
    "        else:\n",
    "            # Fallback to original method\n",
    "            user_data = self.user_history_cache.get(str(user_id), {})\n",
    "            user_avg = user_data.get('avg_rating', self.global_avg)\n",
    "            \n",
    "            # Adjust based on user's rating behavior\n",
    "            if user_avg >= 4.0:\n",
    "                match_percent = min(100, max(0, (score - 2.8) / 2.2 * 100))\n",
    "            elif user_avg <= 2.5:\n",
    "                match_percent = min(100, max(0, (score - 1.8) / 3.2 * 100))\n",
    "            else:\n",
    "                if score >= 3.5:\n",
    "                    match_percent = 70 + (score - 3.5) / 1.5 * 30\n",
    "                elif score >= 2.5:\n",
    "                    match_percent = 40 + (score - 2.5) / 1.0 * 30\n",
    "                else:\n",
    "                    match_percent = max(0, score / 2.5 * 40)\n",
    "            \n",
    "            return int(match_percent)\n",
    "\n",
    "    def calculate_diversity_penalty(self, target_product_id: str, current_recommendations: List[Tuple]) -> float:\n",
    "        \"\"\"Penalize products too similar to already recommended ones\"\"\"\n",
    "        if not current_recommendations or target_product_id not in self.product_id_to_idx:\n",
    "            return 0.0\n",
    "        \n",
    "        target_idx = self.product_id_to_idx[target_product_id]\n",
    "        target_embed = self.prod_embeds[target_idx]\n",
    "        \n",
    "        max_similarity = 0.0\n",
    "        for recommendation in current_recommendations:\n",
    "            # Handle different tuple formats\n",
    "            if len(recommendation) >= 2:\n",
    "                rec_product_id = recommendation[0]\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            if rec_product_id in self.product_id_to_idx:\n",
    "                rec_idx = self.product_id_to_idx[rec_product_id]\n",
    "                rec_embed = self.prod_embeds[rec_idx]\n",
    "                sim = cosine_similarity([target_embed], [rec_embed])[0][0]\n",
    "                max_similarity = max(max_similarity, sim)\n",
    "        \n",
    "        # Penalize if too similar to existing recommendations\n",
    "        return max_similarity * 0.4  # 40% penalty for high similarity\n",
    "\n",
    "    def generate_recommendations(self, user_id: str, top_n: int = 10, \n",
    "                             content_weight: float = 0.4, collab_weight: float = 0.6,\n",
    "                             min_confidence: float = 0.5) -> List[Tuple[str, float, int]]:\n",
    "        \"\"\"\n",
    "        Generate enhanced recommendations with confidence filtering\n",
    "        Returns: List of (product_id, predicted_rating, match_percentage)\n",
    "        \"\"\"\n",
    "        user_id = str(user_id)\n",
    "        user_rated = self.user_history_cache.get(user_id, {}).get('rated_products', [])\n",
    "        \n",
    "        all_products = self.prod_df[\"product_id\"].astype(str).tolist()\n",
    "        candidate_products = [pid for pid in all_products if pid not in user_rated]\n",
    "        \n",
    "        if not candidate_products:\n",
    "            return self._get_popular_fallback(top_n)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        for product_id in tqdm(candidate_products, desc=\"Predicting ratings\"):\n",
    "            try:\n",
    "                predicted_rating, confidence = self.hybrid_predict(user_id, product_id, content_weight, collab_weight)\n",
    "                match_percent = self.calculate_match_percentage(predicted_rating, user_id, product_id)\n",
    "                \n",
    "                skin_match = self.filter_by_skin_profile(product_id, user_id)\n",
    "                adjusted_rating = predicted_rating * skin_match\n",
    "                adjusted_confidence = confidence * skin_match\n",
    "                \n",
    "                match_percent = self.calculate_match_percentage(adjusted_rating, user_id, product_id)\n",
    "                \n",
    "                if adjusted_confidence >= min_confidence and match_percent >= 40:\n",
    "                    recommendations.append((product_id, adjusted_rating, match_percent))\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        #  Sort by hybrid predicted rating directly (no reranking)\n",
    "        recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return recommendations[:top_n]\n",
    "\n",
    "    def _get_popular_fallback(self, top_n: int) -> List[Tuple[str, float, int]]:\n",
    "        \"\"\"Fallback to popular products\"\"\"\n",
    "        popular_products = self.test_df.groupby('product_id')['rating'].agg(['count', 'mean']).reset_index()\n",
    "        popular_products = popular_products[popular_products['count'] >= 10]  # Only reasonably popular\n",
    "        popular_products = popular_products.sort_values(['mean', 'count'], ascending=False)\n",
    "        \n",
    "        result = []\n",
    "        for _, row in popular_products.head(top_n).iterrows():\n",
    "            product_id = str(row['product_id'])\n",
    "            score = row['mean']\n",
    "            match_percent = self.calculate_match_percentage(score, \"average_user\", product_id)\n",
    "            result.append((product_id, score, match_percent))\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def enhanced_demo_recommendations(self, user_id: str, top_n: int = 5,\n",
    "                                   content_weight: float = 0.4, collab_weight: float = 0.6):\n",
    "        \"\"\"Show enhanced recommendations with explanations\"\"\"\n",
    "        recommendations = self.generate_recommendations(user_id, top_n * 2, content_weight, collab_weight)\n",
    "        \n",
    "        print(f\"\\n ENHANCED RECOMMENDATIONS FOR USER {user_id}:\")\n",
    "        print(f\"   Weights: Content={content_weight}, SVD={collab_weight}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        user_data = self.user_history_cache.get(str(user_id), {})\n",
    "        user_avg = user_data.get('avg_rating', self.global_avg)\n",
    "        \n",
    "        displayed = 0\n",
    "        for i, (product_id, score, match_percent) in enumerate(recommendations, 1):\n",
    "            if displayed >= top_n:\n",
    "                break\n",
    "                \n",
    "            product_info = self.prod_df[self.prod_df[\"product_id\"].astype(str) == product_id]\n",
    "            if product_info.empty:\n",
    "                continue\n",
    "                \n",
    "            product_info = product_info.iloc[0]\n",
    "            name = product_info[\"product_name\"]\n",
    "            brand = product_info[\"brand_name\"]\n",
    "            category = product_info[\"tertiary_category\"]\n",
    "            price = product_info[\"price_usd\"]\n",
    "            \n",
    "            formatted_price = f\"${price:.2f}\" if isinstance(price, (int, float)) else f\"${price}\"\n",
    "            \n",
    "            print(f\"{displayed + 1}. {name} ({brand})\")\n",
    "            print(f\"    {category}   {formatted_price}\")\n",
    "            print(f\"    {score:.4f}/5   {match_percent}% match\")\n",
    "            print(f\"    {product_id}\")\n",
    "            \n",
    "            # Add intelligent explanation\n",
    "            if score >= 4.2:\n",
    "                print(\"    Excellent match! Based on your preferences and highly rated by similar users\")\n",
    "            elif score >= 3.8:\n",
    "                print(\"    Great match - combines your product preferences with crowd wisdom\")\n",
    "            elif score >= 3.2:\n",
    "                print(\"    Good suggestion - users with similar tastes enjoyed this product\")\n",
    "            elif score >= 2.8:\n",
    "                print(\"    Recommended - similar to products you've liked, worth exploring\")\n",
    "            else:\n",
    "                print(\"    New discovery - different from your usual preferences but highly rated\")\n",
    "            \n",
    "            print()\n",
    "            displayed += 1\n",
    "        \n",
    "        if displayed == 0:\n",
    "            print(\"  No confident recommendations found. Try rating more products!\")\n",
    "            print(\" Exploring new categories might help improve recommendations\")\n",
    "        \n",
    "    def evaluate(self, top_n: int = 10,\n",
    "                content_weight: float = 0.4, collab_weight: float = 0.6,\n",
    "                min_confidence: float = 0.0) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate recommender on test dataset with multiple metrics:\n",
    "        - RMSE, MAE\n",
    "        - Accuracy (binary hit if predicted >= 3.5 matches actual >= 3.5)\n",
    "        - Precision, Recall, F1\n",
    "        - Coverage (how many unique products were recommended)\n",
    "        \"\"\"\n",
    "        print(\"\\n Evaluating recommender system...\")\n",
    "\n",
    "        y_true, y_pred = [], []\n",
    "        hit_count, rec_count, relevant_count = 0, 0, 0\n",
    "        recommended_products = set()\n",
    "\n",
    "        for _, row in tqdm(self.test_df.iterrows(), total=len(self.test_df), desc=\"Evaluating\"):\n",
    "            user_id, product_id, actual_rating = str(row[\"author_id\"]), str(row[\"product_id\"]), row[\"rating\"]\n",
    "\n",
    "            try:\n",
    "                pred_rating, confidence = self.hybrid_predict(user_id, product_id,\n",
    "                                                            content_weight, collab_weight)\n",
    "                # if confidence < min_confidence:\n",
    "                #     continue\n",
    "\n",
    "                y_true.append(actual_rating)\n",
    "                y_pred.append(pred_rating)\n",
    "\n",
    "                # --- Binary relevance for classification metrics ---\n",
    "                actual_relevant = 1 if actual_rating >= 3.5 else 0\n",
    "                predicted_relevant = 1 if pred_rating >= 3.5 else 0\n",
    "\n",
    "                if predicted_relevant == 1:\n",
    "                    rec_count += 1\n",
    "                    recommended_products.add(product_id)\n",
    "\n",
    "                if actual_relevant == 1:\n",
    "                    relevant_count += 1\n",
    "\n",
    "                if predicted_relevant == 1 and actual_relevant == 1:\n",
    "                    hit_count += 1\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        # --- Compute metrics ---\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred)) if y_true else float(\"nan\")\n",
    "        mae = mean_absolute_error(y_true, y_pred) if y_true else float(\"nan\")\n",
    "        accuracy = accuracy_score([1 if r >= 3.5 else 0 for r in y_true],\n",
    "                                [1 if p >= 3.5 else 0 for p in y_pred]) if y_true else float(\"nan\")\n",
    "        precision = precision_score([1 if r >= 3.5 else 0 for r in y_true],\n",
    "                                    [1 if p >= 3.5 else 0 for p in y_pred],\n",
    "                                    zero_division=0) if y_true else float(\"nan\")\n",
    "        recall = recall_score([1 if r >= 3.5 else 0 for r in y_true],\n",
    "                            [1 if p >= 3.5 else 0 for p in y_pred],\n",
    "                            zero_division=0) if y_true else float(\"nan\")\n",
    "        f1 = f1_score([1 if r >= 3.5 else 0 for r in y_true],\n",
    "                    [1 if p >= 3.5 else 0 for p in y_pred],\n",
    "                    zero_division=0) if y_true else float(\"nan\")\n",
    "\n",
    "        coverage = len(recommended_products) / len(self.prod_df) if len(self.prod_df) > 0 else 0\n",
    "\n",
    "        results = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"Coverage\": coverage\n",
    "        }\n",
    "\n",
    "        print(\"\\n Evaluation Results:\")\n",
    "        for metric, value in results.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        return results     \n",
    "    def evaluate_by_user_group(self, top_n=10):\n",
    "        \"\"\"\n",
    "        Evaluate recommender performance across user groups:\n",
    "        - Cold-start (<5 ratings)\n",
    "        - Medium (5-20 ratings)\n",
    "        - Heavy (>20 ratings)\n",
    "        \"\"\"\n",
    "        # Count ratings per user from training data\n",
    "        user_rating_counts = self.train_df.groupby(\"author_id\")[\"rating\"].count().to_dict()\n",
    "        \n",
    "        groups = {\n",
    "            \"Cold-start (<5)\": [],\n",
    "            \"Medium (5-20)\": [],\n",
    "            \"Heavy (>20)\": []\n",
    "        }\n",
    "\n",
    "        for _, row in self.test_df.iterrows():\n",
    "            user = str(row[\"author_id\"])\n",
    "            item = str(row[\"product_id\"])\n",
    "            true_rating = row[\"rating\"]\n",
    "\n",
    "            pred_rating, _ = self.hybrid_predict(user, item)\n",
    "\n",
    "            # Assign to group\n",
    "            count = user_rating_counts.get(user, 0)\n",
    "            if count < 5:\n",
    "                group = \"Cold-start (<5)\"\n",
    "            elif count > 20:\n",
    "                group = \"Heavy (>20)\"\n",
    "            else:\n",
    "                group = \"Medium (5-20)\"\n",
    "\n",
    "            groups[group].append((true_rating, pred_rating))\n",
    "\n",
    "        results = {}\n",
    "        for group, values in groups.items():\n",
    "            if not values:\n",
    "                continue\n",
    "            y_true, y_pred = zip(*values)\n",
    "            rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "            y_true_bin = [1 if r >= 4 else 0 for r in y_true]\n",
    "            y_pred_bin = [1 if p >= 4 else 0 for p in y_pred]\n",
    "\n",
    "            precision = precision_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "            recall = recall_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "            f1 = f1_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "            acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "\n",
    "            results[group] = {\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"F1\": f1,\n",
    "                \"Count\": len(values)\n",
    "            }\n",
    "\n",
    "        results_df = pd.DataFrame(results).T\n",
    "        print(\"\\n Evaluation by User Group:\")\n",
    "        print(results_df)\n",
    "        return results_df\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\" INITIALIZING ENHANCED HYBRID RECOMMENDER\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Initialize recommender\n",
    "    recommender = EnhancedHybridRecommender(\n",
    "        train_path=\"data/CleanedDataSet/train_skincare.csv\",\n",
    "        test_path=\"data/CleanedDataSet/test_skincare.csv\",\n",
    "        products_path=\"data/CleanedDataSet/filtered_skincare_products.csv\",\n",
    "        content_model_path=\"models/product_embeddings.pkl\",\n",
    "        svd_model_path=\"models/surprise_svd_model.pkl\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n ENHANCED INITIALIZATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    #  Fixed best weights for assignment\n",
    "    best_content_weight = 0.2\n",
    "    best_collab_weight = 0.8\n",
    "\n",
    "    # ---------------- 3) Generate recommendations for a real user ----------------\n",
    "    real_user_id = 2128891661  # <-- must exist in your dataset\n",
    "    recommender.enhanced_demo_recommendations(\n",
    "        user_id=real_user_id,\n",
    "        top_n=5,\n",
    "        content_weight=best_content_weight,\n",
    "        collab_weight=best_collab_weight\n",
    "    )\n",
    "\n",
    "    # ---------------- 4) Evaluate system ----------------\n",
    "    eval_results = recommender.evaluate(\n",
    "        top_n=10,\n",
    "        content_weight=best_content_weight,\n",
    "        collab_weight=best_collab_weight\n",
    "    )\n",
    "\n",
    "    # ---------------- 5) Evaluate by user groups ----------------\n",
    "    group_eval = recommender.evaluate_by_user_group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b6c80",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abb3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "content_rmse = content_metrics['rmse'] if content_metrics else 0\n",
    "content_mae = content_metrics['mae'] if content_metrics else 0\n",
    "collab_rmse = svd_rmse\n",
    "collab_mae = svd_mae\n",
    "\n",
    "# Data for plotting\n",
    "models = ['Content-Based', 'Collaborative']\n",
    "rmse_values = [content_rmse, collab_rmse]\n",
    "mae_values = [content_mae, collab_mae]\n",
    "\n",
    "# Set up the bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(models))\n",
    "\n",
    "# Plot bars\n",
    "bars1 = ax.bar(index, rmse_values, bar_width, label='RMSE', color='skyblue')\n",
    "bars2 = ax.bar(index + bar_width, mae_values, bar_width, label='MAE', color='lightcoral')\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_xlabel('Model Type')\n",
    "ax.set_ylabel('Error Metric Value')\n",
    "ax.set_title('RMSE and MAE Comparison: Content-Based vs Collaborative Filtering')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.4f}', \n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Adjust layout to prevent clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c659664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
