{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 INITIALIZING ENHANCED HYBRID RECOMMENDER\n",
      "================================================================================\n",
      "📦 Loading pre-trained models...\n",
      "✅ Content model loaded: 1760 products\n",
      "✅ SVD model loaded\n",
      "📈 Global average rating from SVD: 3.934\n",
      "✅ Index mappings created\n",
      "⚡ Precomputing product features for faster similarity...\n",
      "✅ Precomputed features for 1760 products\n",
      "📊 Preloading data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 749\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m    748\u001b[39m \u001b[38;5;66;03m# Initialize recommender\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m recommender = \u001b[43mEnhancedHybridRecommender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/CleanedDataSet/train_skincare.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/CleanedDataSet/test_skincare.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproducts_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/CleanedDataSet/filtered_skincare_products.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_model_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/product_embeddings.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m    \u001b[49m\u001b[43msvd_model_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/surprise_svd_model.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    755\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ ENHANCED INITIALIZATION COMPLETE!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    758\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mEnhancedHybridRecommender.__init__\u001b[39m\u001b[34m(self, train_path, test_path, products_path, content_model_path, svd_model_path)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Load models and data\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m._load_models()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Skin type + concern rules \u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Regex patterns to detect skin types from text\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.SKIN_TYPE_PATTERNS = [\n\u001b[32m     50\u001b[39m     (\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb(?:good|best)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*for:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*oily\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moily\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     51\u001b[39m     (\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb(?:good|best)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*for:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*dry\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdry\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     (\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mbgentle\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msensitive\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     65\u001b[39m ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mEnhancedHybridRecommender._preload_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28mself\u001b[39m.train_df = pd.read_csv(\u001b[38;5;28mself\u001b[39m.train_path, usecols=[\u001b[33m\"\u001b[39m\u001b[33mauthor_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrating\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Build user history cache (from train)\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mself\u001b[39m.user_histories = (\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauthor_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m.to_dict()\n\u001b[32m    165\u001b[39m )\n\u001b[32m    167\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Test data loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.test_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    168\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Train data loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI-Assignment\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:254\u001b[39m, in \u001b[36mSeriesGroupBy.apply\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(\n\u001b[32m    249\u001b[39m     _apply_docs[\u001b[33m\"\u001b[39m\u001b[33mtemplate\u001b[39m\u001b[33m\"\u001b[39m].format(\n\u001b[32m    250\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mseries\u001b[39m\u001b[33m\"\u001b[39m, examples=_apply_docs[\u001b[33m\"\u001b[39m\u001b[33mseries_examples\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    251\u001b[39m     )\n\u001b[32m    252\u001b[39m )\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, *args, **kwargs) -> Series:\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI-Assignment\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1567\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m   1559\u001b[39m     new_msg = (\n\u001b[32m   1560\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe operation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_func\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed on a column. If any error is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1561\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mraised, this will raise an exception in a future version \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1562\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof pandas. Drop these columns to avoid this warning.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1563\u001b[39m     )\n\u001b[32m   1564\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m rewrite_warning(\n\u001b[32m   1565\u001b[39m         old_msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, new_msg\n\u001b[32m   1566\u001b[39m     ) \u001b[38;5;28;01mif\u001b[39;00m is_np_func \u001b[38;5;28;01melse\u001b[39;00m nullcontext():\n\u001b[32m-> \u001b[39m\u001b[32m1567\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1569\u001b[39m     \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[32m   1570\u001b[39m     \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1574\u001b[39m     \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[32m   1575\u001b[39m     \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._group_selection_context():\n\u001b[32m   1578\u001b[39m         \u001b[38;5;66;03m# GH#50538\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI-Assignment\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1629\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1592\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1593\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1599\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1600\u001b[39m ) -> NDFrameT:\n\u001b[32m   1601\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1602\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1603\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1627\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1628\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1630\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1631\u001b[39m         not_indexed_same = mutated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI-Assignment\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:842\u001b[39m, in \u001b[36mBaseGrouper.apply\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    841\u001b[39m         mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m     result_values.append(res)\n\u001b[32m    843\u001b[39m \u001b[38;5;66;03m# getattr pattern for __name__ is needed for functools.partial objects\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(group_keys) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    845\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmad\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    846\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mskew\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    851\u001b[39m     \u001b[38;5;66;03m#  so we will not have raised even if this is an invalid dtype.\u001b[39;00m\n\u001b[32m    852\u001b[39m     \u001b[38;5;66;03m#  So do one dummy call here to raise appropriate TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from typing import Dict, List, Tuple\n",
    "from surprise import dump\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedHybridRecommender:\n",
    "    def __init__(self, train_path: str, test_path: str, products_path: str,\n",
    "                 content_model_path: str, svd_model_path: str):\n",
    "        \"\"\"\n",
    "        Hybrid Recommender: SVD + Content-based filtering with improvements\n",
    "        \"\"\"\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.products_path = products_path\n",
    "        self.content_model_path = content_model_path\n",
    "        self.svd_model_path = svd_model_path\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.prod_df = None\n",
    "        self.prod_embeds = None\n",
    "        self.svd_model = None\n",
    "        self.global_avg = 3.0\n",
    "        self.test_df = None\n",
    "        self.train_df = None\n",
    "        self.user_history_cache = {}\n",
    "        self.product_popularity = {}\n",
    "        self.product_features = {}\n",
    "        self.skin_profiles = {}\n",
    "        \n",
    "        # Load models and data\n",
    "        self._load_models()\n",
    "        self._preload_data()\n",
    "        \n",
    "        # ------------------------------\n",
    "        # Skin type + concern rules \n",
    "        # ------------------------------\n",
    "        # Regex patterns to detect skin types from text\n",
    "        self.SKIN_TYPE_PATTERNS = [\n",
    "            (r\"\\b(?:good|best)\\s*for:\\s*oily\\b\", \"oily\"),\n",
    "            (r\"\\b(?:good|best)\\s*for:\\s*dry\\b\", \"dry\"),\n",
    "            (r\"\\b(?:good|best)\\s*for:\\s*combination\\b\", \"combination\"),\n",
    "            (r\"\\b(?:good|best)\\s*for:\\s*sensitive\\b\", \"sensitive\"),\n",
    "            (r\"\\b(?:good|best)\\s*for:\\s*normal\\b\", \"normal\"),\n",
    "            (r\"\\b(oily skin|oily)\\b\", \"oily\"),\n",
    "            (r\"\\b(dry skin|dry)\\b\", \"dry\"),\n",
    "            (r\"\\b(combination skin|combination|combo)\\b\", \"combination\"),\n",
    "            (r\"\\b(sensitive skin|sensitive)\\b\", \"sensitive\"),\n",
    "            (r\"\\b(normal skin|normal)\\b\", \"normal\"),\n",
    "            (r\"\\bfor\\s+sensitive\\s+skin\\b\", \"sensitive\"),\n",
    "            (r\"\\bsuitable\\s+for\\s+sensitive\\b\", \"sensitive\"),\n",
    "            (r\"\\bfor\\s+sensitive\\b\", \"sensitive\"),\n",
    "            (r\"\\bhypoallergenic\\b\", \"sensitive\"),\n",
    "            (r\"\\bgentle\\b\", \"sensitive\"),\n",
    "        ]\n",
    "        # Regex patterns to detect skin concerns from claims or product name\n",
    "        self.SKIN_CONCERN_PATTERNS = [\n",
    "            (r\"\\b(acne|blemish|breakout|pimple)\\b\", \"acne\"),\n",
    "            (r\"\\bpores?\\b\", \"pores\"),\n",
    "            (r\"\\b(dark spot|hyperpigment|discoloration|melasma)\\b\", \"hyperpigmentation\"),\n",
    "            (r\"\\b(wrinkle|fine line|anti[- ]?aging|firming|loss of firmness|elasticity)\\b\", \"aging\"),\n",
    "            (r\"\\b(redness|rosacea|irritation|calming|soothing)\\b\", \"redness\"),\n",
    "            (r\"\\b(dryness|dehydration|hydrating|moisturizing|moisturising|barrier)\\b\", \"dehydration\"),\n",
    "            (r\"\\b(dull(ness)?|brighten(ing)?|glow|radiance)\\b\", \"dullness\"),\n",
    "            (r\"\\boil(y| control|iness)\\b\", \"oil-control\"),\n",
    "            (r\"\\b(blackhead|whitehead|congestion)\\b\", \"blackheads\"),\n",
    "            (r\"\\b(uneven (tone|texture)|texture|resurfacing)\\b\", \"texture\"),\n",
    "            (r\"\\b(dark circle|dark circles)\\b\", \"dark-circles\"),\n",
    "        ]\n",
    "\n",
    "        # Ingredient-based concern mapping\n",
    "        self.INGREDIENT_CONCERN_PATTERNS = [\n",
    "            (r\"\\b(salicylic acid|beta hydroxy|bha|willow bark|benzoyl peroxide|sulfur|zinc pca|zinc)\\b\", {\"acne\",\"pores\",\"oil-control\"}),\n",
    "            (r\"\\b(kaolin|bentonite|clay|charcoal)\\b\", {\"pores\",\"oil-control\"}),\n",
    "            (r\"\\b(tea tree|melaleuca)\\b\", {\"acne\"}),\n",
    "            (r\"\\b(hyaluronic acid|sodium hyaluronate|glycerin|panthenol|urea|betaine|trehalose|aloe)\\b\", {\"dehydration\"}),\n",
    "            (r\"\\b(ceramide|ceramides|cholesterol|squalane|squalene|shea|shea butter)\\b\", {\"dehydration\"}),\n",
    "            (r\"\\b(retinol|retinal|retinoate|bakuchiol|peptide|matrixyl|collagen|coenzyme ?q10|ubiquinone)\\b\", {\"aging\"}),\n",
    "            (r\"\\b(vitamin ?c|ascorbic|ascorbyl|ethyl ascorbic|magnesium ascorbyl|sodium ascorbyl|alpha arbutin|tranexamic|azelaic|kojic|licorice|glycyrrhiza)\\b\", {\"hyperpigmentation\",\"dullness\"}),\n",
    "            (r\"\\b(centella|cica|madecassoside|asiaticoside|allantoin|bisabolol|beta glucan|green tea|oat|colloidal oatmeal)\\b\", {\"redness\"}),\n",
    "            (r\"\\b(aha|glycolic|lactic|mandelic|tartaric|citric|pha|gluconolactone|lactobionic)\\b\", {\"texture\",\"dullness\"}),\n",
    "        ]\n",
    "        print(\"✅ Enhanced Hybrid Recommender initialized successfully!\")\n",
    "\n",
    "    def _load_models(self) -> None:\n",
    "        \"\"\"Load pre-trained models\"\"\"\n",
    "        print(\"📦 Loading pre-trained models...\")\n",
    "        \n",
    "        # Load content-based model\n",
    "        if os.path.exists(self.content_model_path):\n",
    "            self.prod_df, self.prod_embeds = joblib.load(self.content_model_path)\n",
    "            print(f\"✅ Content model loaded: {len(self.prod_df)} products\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Content model not found at {self.content_model_path}\")\n",
    "        \n",
    "        # Load SVD model\n",
    "        if os.path.exists(self.svd_model_path):\n",
    "            _, self.svd_model = dump.load(self.svd_model_path)\n",
    "            print(\"✅ SVD model loaded\")\n",
    "            \n",
    "            # Get global average from SVD\n",
    "            if hasattr(self.svd_model, 'trainset') and self.svd_model.trainset:\n",
    "                self.global_avg = self.svd_model.trainset.global_mean\n",
    "                print(f\"📈 Global average rating from SVD: {self.global_avg:.3f}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"SVD model not found at {self.svd_model_path}\")\n",
    "        \n",
    "        # Create index mappings\n",
    "        self.product_id_to_idx = {str(pid): idx for idx, pid in enumerate(self.prod_df[\"product_id\"])}\n",
    "        print(\"✅ Index mappings created\")\n",
    "        \n",
    "        # Precompute product features for faster similarity calculation\n",
    "        self.precompute_product_features()\n",
    "\n",
    "    def precompute_product_features(self):\n",
    "        \"\"\"Precompute product features for faster similarity calculation\"\"\"\n",
    "        print(\"⚡ Precomputing product features for faster similarity...\")\n",
    "        \n",
    "        self.product_features = {}\n",
    "        for _, row in self.prod_df.iterrows():\n",
    "            product_id = str(row[\"product_id\"])\n",
    "            self.product_features[product_id] = {\n",
    "                'brand': row[\"brand_name\"],\n",
    "                'category': row[\"tertiary_category\"],\n",
    "                'price': row[\"price_usd\"] if pd.notna(row[\"price_usd\"]) else 0,\n",
    "                'embedding': self.prod_embeds[self.product_id_to_idx[product_id]]\n",
    "            }\n",
    "        \n",
    "        print(f\"✅ Precomputed features for {len(self.product_features)} products\")\n",
    "\n",
    "    def _preload_data(self):\n",
    "        print(\"📊 Preloading data...\")\n",
    "\n",
    "        # Load a small sample just to detect available columns\n",
    "        test_sample = pd.read_csv(self.test_path, nrows=5)\n",
    "\n",
    "        # Base columns we always need\n",
    "        usecols = [\"author_id\", \"product_id\", \"rating\"]\n",
    "\n",
    "        # ✅ Try to detect a timestamp column from the sample\n",
    "        time_col = None\n",
    "        for col in [\"timestamp\", \"submission_time\", \"review_date\"]:\n",
    "            if col in test_sample.columns:\n",
    "                time_col = col\n",
    "                usecols.append(col)\n",
    "                break\n",
    "\n",
    "        # Load full test/train datasets with only needed columns\n",
    "        self.test_df = pd.read_csv(self.test_path, usecols=usecols)\n",
    "        self.train_df = pd.read_csv(self.train_path, usecols=[\"author_id\", \"product_id\", \"rating\"])\n",
    "\n",
    "        # Build user history cache (from train)\n",
    "        self.user_histories = (\n",
    "            self.train_df.groupby(\"author_id\")[\"product_id\"].apply(list).to_dict()\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Test data loaded: {len(self.test_df)} records\")\n",
    "        print(f\"✅ Train data loaded: {len(self.train_df)} records\")\n",
    "\n",
    "        \n",
    "        # Build user history cache - OPTIMIZED\n",
    "        print(\"🔨 Building user history cache...\")\n",
    "        all_ratings = pd.concat([self.train_df, self.test_df])\n",
    "        \n",
    "        # Use groupby for faster processing\n",
    "        user_groups = all_ratings.groupby(\"author_id\")\n",
    "        for user_id, group in tqdm(user_groups, desc=\"Caching user histories\"):\n",
    "            self.user_history_cache[str(user_id)] = {\n",
    "                'rated_products': group[\"product_id\"].astype(str).tolist(),\n",
    "                'ratings': group[\"rating\"].tolist(),\n",
    "                'avg_rating': group[\"rating\"].mean()\n",
    "            }\n",
    "        \n",
    "        # Calculate product popularity\n",
    "        print(\"📊 Calculating product popularity...\")\n",
    "        # FIXED: Changed ast(str) to astype(str)\n",
    "        self.product_popularity = all_ratings['product_id'].astype(str).value_counts().to_dict()\n",
    "        \n",
    "        print(\"=== Dataset Overview ===\")\n",
    "        print(f\"Train set: {len(self.train_df):,} rows | {self.train_df['author_id'].nunique():,} users | {self.train_df['product_id'].nunique():,} products\")\n",
    "        print(f\"Test set: {len(self.test_df):,} rows | {self.test_df['author_id'].nunique():,} users | {self.test_df['product_id'].nunique():,} products\")\n",
    "        print(f\"Products catalog: {len(self.prod_df):,} items\")\n",
    "        print(\"========================\")\n",
    "\n",
    "    def add_skin_profile(self, user_id: str, skin_data: Dict):\n",
    "        \"\"\"添加用户皮肤分析数据\"\"\"\n",
    "        user_id = str(user_id)\n",
    "        self.skin_profiles[user_id] = skin_data\n",
    "        print(f\"✅ Added skin profile for user {user_id}\")\n",
    "        print(f\"   Skin type: {skin_data.get('skin_type', 'N/A')}\")\n",
    "        print(f\"   Concerns: {skin_data.get('concerns', 'N/A')}\")\n",
    "        print(f\"   Budget: {skin_data.get('budget', 'N/A')}\")\n",
    "\n",
    "\n",
    "    def _extract_skin_tags(self, text: str):\n",
    "        \"\"\"\n",
    "        Extract possible skin types and skin concerns from text.\n",
    "        Returns two lists: (matched_types, matched_concerns).\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "\n",
    "        # Define possible tags (expand if needed)\n",
    "        skin_types = [\"oily\", \"dry\", \"combination\", \"normal\", \"sensitive\"]\n",
    "        skin_concerns = [\n",
    "            \"acne\", \"wrinkles\", \"fine lines\", \"dark spots\", \n",
    "            \"pigmentation\", \"redness\", \"pores\", \"dullness\", \n",
    "            \"eczema\", \"rosacea\", \"hydration\", \"blackheads\"\n",
    "        ]\n",
    "\n",
    "        matched_types = [t for t in skin_types if t in text]\n",
    "        matched_concerns = [c for c in skin_concerns if c in text]\n",
    "\n",
    "        return matched_types, matched_concerns\n",
    "\n",
    "\n",
    "    def _budget_range(self, budget_str: str):\n",
    "        \"\"\"Convert budget category string into (min_price, max_price).\"\"\"\n",
    "        if not budget_str:\n",
    "            return 0, float(\"inf\")\n",
    "\n",
    "        budget_str = budget_str.strip().lower()\n",
    "        if \"no budget\" in budget_str:       # ✅ handles \"No budget limit\"\n",
    "            return 0, float(\"inf\")\n",
    "        elif \"under\" in budget_str:\n",
    "            return 0, 25\n",
    "        elif \"$25\" in budget_str and \"$50\" in budget_str:\n",
    "            return 25, 50\n",
    "        elif \"$50\" in budget_str and \"$100\" in budget_str:\n",
    "            return 50, 100\n",
    "        elif \"over\" in budget_str or \"above\" in budget_str:\n",
    "            return 100, float(\"inf\")\n",
    "        return 0, float(\"inf\")  # fallback\n",
    "\n",
    "\n",
    "\n",
    "    def filter_by_skin_profile(self, product_id: int, user_id: int):\n",
    "        \"\"\"\n",
    "        Soft filtering: adjust score multiplier based on match\n",
    "        between product tags and user's skin profile.\n",
    "        \"\"\"\n",
    "        user_id = str(user_id)\n",
    "        if user_id not in self.skin_profiles:\n",
    "            return 1.0  # no profile → neutral\n",
    "\n",
    "        user_profile = self.skin_profiles[user_id]\n",
    "        user_type = user_profile.get(\"skin_type\", \"\").lower()\n",
    "        \n",
    "        # ✅ Handle both string and list for concerns\n",
    "        concerns = user_profile.get(\"concerns\", [])\n",
    "        if isinstance(concerns, str):\n",
    "            user_concerns = [concerns.lower()]\n",
    "        else:\n",
    "            user_concerns = [c.lower() for c in concerns]\n",
    "\n",
    "        user_budget = user_profile.get(\"budget\", \"\")\n",
    "\n",
    "        # Get product details\n",
    "        product = self.prod_df[self.prod_df[\"product_id\"].astype(str) == str(product_id)].iloc[0]\n",
    "        product_text = \" \".join([\n",
    "            str(product.get(\"product_name\", \"\")),\n",
    "            str(product.get(\"combined_features\", \"\")),\n",
    "            str(product.get(\"ingredients\", \"\")),\n",
    "            str(product.get(\"claims\", \"\")),\n",
    "        ])\n",
    "        product_price = product.get(\"price_usd\", 0)\n",
    "\n",
    "        # Extract tags from product text\n",
    "        matched_types, matched_concerns = self._extract_skin_tags(product_text)\n",
    "\n",
    "        # Start with neutral multiplier\n",
    "        multiplier = 1.0\n",
    "\n",
    "        # Skin type match\n",
    "        if user_type in matched_types:\n",
    "            multiplier *= 1.2\n",
    "        elif user_type:  # mismatch but user specified\n",
    "            multiplier *= 0.9\n",
    "\n",
    "        # Skin concern match (✅ supports multiple concerns)\n",
    "        if any(c in matched_concerns for c in user_concerns):\n",
    "            multiplier *= 1.3\n",
    "        elif user_concerns:  # user specified but no match\n",
    "            multiplier *= 0.85\n",
    "\n",
    "        # Budget consideration\n",
    "        min_budget, max_budget = self._budget_range(user_budget)\n",
    "        if min_budget <= product_price <= max_budget:\n",
    "            multiplier *= 1.1\n",
    "        else:\n",
    "            multiplier *= 0.7\n",
    "\n",
    "        return max(0.3, min(multiplier, 2.0))\n",
    "\n",
    "\n",
    "\n",
    "    def enhanced_content_similarity(self, target_product_id: str, user_rated_products: List[str]) -> float:\n",
    "        \"\"\"Enhanced content similarity with multiple factors - OPTIMIZED using precomputed features\"\"\"\n",
    "        if target_product_id not in self.product_features or not user_rated_products:\n",
    "            return 0.0\n",
    "        \n",
    "        target_features = self.product_features[target_product_id]\n",
    "        target_embed = target_features['embedding']\n",
    "        target_brand = target_features['brand']\n",
    "        target_category = target_features['category']\n",
    "        target_price = target_features['price']\n",
    "        \n",
    "        similarities = []\n",
    "        \n",
    "        # Pre-filter rated products that exist in our features\n",
    "        valid_rated_products = [pid for pid in user_rated_products if pid in self.product_features]\n",
    "        \n",
    "        for rated_pid in valid_rated_products:\n",
    "            rated_features = self.product_features[rated_pid]\n",
    "            rated_embed = rated_features['embedding']\n",
    "            rated_brand = rated_features['brand']\n",
    "            rated_category = rated_features['category']\n",
    "            rated_price = rated_features['price']\n",
    "            \n",
    "            # Multiple similarity measures\n",
    "            cosine_sim = cosine_similarity([target_embed], [rated_embed])[0][0]\n",
    "            \n",
    "            # Brand similarity\n",
    "            brand_sim = 0.3 if target_brand == rated_brand else 0\n",
    "            \n",
    "            # Category similarity\n",
    "            category_sim = 0.2 if target_category == rated_category else 0\n",
    "            \n",
    "            # Price similarity (within 20% price range)\n",
    "            if target_price > 0 and rated_price > 0:\n",
    "                price_ratio = min(target_price, rated_price) / max(target_price, rated_price)\n",
    "                price_sim = 0.2 if price_ratio > 0.8 else 0\n",
    "            else:\n",
    "                price_sim = 0\n",
    "            \n",
    "            total_sim = cosine_sim + brand_sim + category_sim + price_sim\n",
    "            if total_sim > 0.3:  # Higher threshold for better quality\n",
    "                similarities.append(total_sim)\n",
    "        \n",
    "        return np.mean(similarities) if similarities else 0.0\n",
    "    \n",
    "    def get_adaptive_weights(self, user_id):\n",
    "        \"\"\"Adaptively set weights based on user activity\"\"\"\n",
    "        if user_id not in self.user_item_matrix.index:\n",
    "            return 0.6, 0.4  # fallback for new user (more content-based)\n",
    "\n",
    "        n_ratings = self.train_df[self.train_df[\"author_id\"] == user_id].shape[0]\n",
    "\n",
    "        # More ratings → trust collaborative filtering more\n",
    "        if n_ratings < 5:\n",
    "            return 0.6, 0.4   # content-heavy\n",
    "        elif n_ratings < 20:\n",
    "            return 0.4, 0.6   # balanced\n",
    "        else:\n",
    "            return 0.2, 0.8   # collab-heavy\n",
    "\n",
    "\n",
    "    def hybrid_predict(self, user_id: str, product_id: str,\n",
    "                   content_weight: float = 0.4, collab_weight: float = 0.6) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Enhanced hybrid prediction with smooth adaptive weighting\n",
    "        Returns: (prediction, confidence)\n",
    "        \"\"\"\n",
    "        user_id = str(user_id)\n",
    "        product_id = str(product_id)\n",
    "        \n",
    "        # ========== SVD PREDICTION ==========\n",
    "        svd_pred = np.nan\n",
    "        svd_confidence = 0.0\n",
    "        \n",
    "        try:\n",
    "            svd_prediction = self.svd_model.predict(user_id, product_id)\n",
    "            svd_pred = max(1.0, min(5.0, svd_prediction.est))\n",
    "            svd_confidence = 0.9 if not svd_prediction.details.get('was_impossible', False) else 0.4\n",
    "        except:\n",
    "            svd_pred = self.global_avg\n",
    "            svd_confidence = 0.3\n",
    "        \n",
    "        # ========== CONTENT PREDICTION ==========\n",
    "        content_pred = np.nan\n",
    "        content_confidence = 0.0\n",
    "        \n",
    "        if user_id in self.user_history_cache:\n",
    "            user_data = self.user_history_cache[user_id]\n",
    "            rated_products = user_data['rated_products']\n",
    "            \n",
    "            if len(rated_products) >= 2 and product_id in self.product_id_to_idx:\n",
    "                similarity_score = self.enhanced_content_similarity(product_id, rated_products)\n",
    "                \n",
    "                if similarity_score > 0.1:\n",
    "                    # Map similarity to rating scale (1-5)\n",
    "                    content_pred = 1.0 + similarity_score * 4.0\n",
    "                    content_confidence = min(1.0, similarity_score * 1.8)\n",
    "                    content_pred = max(1.0, min(5.0, content_pred))\n",
    "        \n",
    "        # ========== ADVANCED HYBRID COMBINATION ==========\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        weights = []\n",
    "        \n",
    "        user_data = self.user_history_cache.get(user_id, {})\n",
    "        user_rating_count = len(user_data.get('rated_products', []))\n",
    "        \n",
    "        # Smooth scaling: more ratings → more collaborative\n",
    "        ratio = min(1.0, user_rating_count / 30)  # cap effect at 30 ratings\n",
    "        effective_collab_weight = collab_weight * (0.4 + 0.6 * ratio)  # grows with activity\n",
    "        effective_content_weight = content_weight * (1.0 - 0.6 * ratio)  # shrinks with activity\n",
    "        \n",
    "        if not np.isnan(svd_pred):\n",
    "            predictions.append(svd_pred)\n",
    "            confidences.append(svd_confidence)\n",
    "            weights.append(effective_collab_weight)\n",
    "        \n",
    "        if not np.isnan(content_pred) and content_confidence > 0.2:\n",
    "            predictions.append(content_pred)\n",
    "            confidences.append(content_confidence)\n",
    "            weights.append(effective_content_weight)\n",
    "        \n",
    "        if len(predictions) == 2:\n",
    "            total_confidence = sum(c * w for c, w in zip(confidences, weights))\n",
    "            weighted_pred = sum(p * c * w for p, c, w in zip(predictions, confidences, weights)) / total_confidence\n",
    "            final_confidence = total_confidence / sum(weights)\n",
    "        elif len(predictions) == 1:\n",
    "            weighted_pred = predictions[0]\n",
    "            final_confidence = confidences[0]\n",
    "        else:\n",
    "            # Fallback: use user's avg rating or global avg with small jitter\n",
    "            weighted_pred = user_data.get('avg_rating', self.global_avg) + np.random.uniform(-0.2, 0.2)\n",
    "            weighted_pred = max(1.0, min(5.0, weighted_pred))\n",
    "            final_confidence = 0.2\n",
    "        \n",
    "        return max(1.0, min(5.0, weighted_pred)), final_confidence\n",
    "\n",
    "    def calculate_match_percentage(self, score: float, user_id: str, product_id: str, \n",
    "                             all_recommendation_scores: List[float] = None) -> int:\n",
    "        \"\"\"Improved match percentage with relative scoring\"\"\"\n",
    "        \n",
    "        if all_recommendation_scores:\n",
    "            # Use percentile ranking within current recommendations\n",
    "            sorted_scores = sorted(all_recommendation_scores)\n",
    "            position = sorted_scores.index(score)\n",
    "            percentile = (position / len(sorted_scores)) * 100\n",
    "            return int(percentile)\n",
    "        else:\n",
    "            # Fallback to original method\n",
    "            user_data = self.user_history_cache.get(str(user_id), {})\n",
    "            user_avg = user_data.get('avg_rating', self.global_avg)\n",
    "            \n",
    "            # Adjust based on user's rating behavior\n",
    "            if user_avg >= 4.0:\n",
    "                match_percent = min(100, max(0, (score - 2.8) / 2.2 * 100))\n",
    "            elif user_avg <= 2.5:\n",
    "                match_percent = min(100, max(0, (score - 1.8) / 3.2 * 100))\n",
    "            else:\n",
    "                if score >= 3.5:\n",
    "                    match_percent = 70 + (score - 3.5) / 1.5 * 30\n",
    "                elif score >= 2.5:\n",
    "                    match_percent = 40 + (score - 2.5) / 1.0 * 30\n",
    "                else:\n",
    "                    match_percent = max(0, score / 2.5 * 40)\n",
    "            \n",
    "            return int(match_percent)\n",
    "\n",
    "    def calculate_diversity_penalty(self, target_product_id: str, current_recommendations: List[Tuple]) -> float:\n",
    "        \"\"\"Penalize products too similar to already recommended ones\"\"\"\n",
    "        if not current_recommendations or target_product_id not in self.product_id_to_idx:\n",
    "            return 0.0\n",
    "        \n",
    "        target_idx = self.product_id_to_idx[target_product_id]\n",
    "        target_embed = self.prod_embeds[target_idx]\n",
    "        \n",
    "        max_similarity = 0.0\n",
    "        for recommendation in current_recommendations:\n",
    "            # Handle different tuple formats\n",
    "            if len(recommendation) >= 2:\n",
    "                rec_product_id = recommendation[0]\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            if rec_product_id in self.product_id_to_idx:\n",
    "                rec_idx = self.product_id_to_idx[rec_product_id]\n",
    "                rec_embed = self.prod_embeds[rec_idx]\n",
    "                sim = cosine_similarity([target_embed], [rec_embed])[0][0]\n",
    "                max_similarity = max(max_similarity, sim)\n",
    "        \n",
    "        # Penalize if too similar to existing recommendations\n",
    "        return max_similarity * 0.4  # 40% penalty for high similarity\n",
    "\n",
    "    def generate_recommendations(self, user_id: str, top_n: int = 10, \n",
    "                             content_weight: float = 0.4, collab_weight: float = 0.6,\n",
    "                             min_confidence: float = 0.5) -> List[Tuple[str, float, int]]:\n",
    "        \"\"\"\n",
    "        Generate enhanced recommendations with confidence filtering\n",
    "        Returns: List of (product_id, predicted_rating, match_percentage)\n",
    "        \"\"\"\n",
    "        user_id = str(user_id)\n",
    "        user_rated = self.user_history_cache.get(user_id, {}).get('rated_products', [])\n",
    "        \n",
    "        all_products = self.prod_df[\"product_id\"].astype(str).tolist()\n",
    "        candidate_products = [pid for pid in all_products if pid not in user_rated]\n",
    "        \n",
    "        if not candidate_products:\n",
    "            return self._get_popular_fallback(top_n)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        for product_id in tqdm(candidate_products, desc=\"Predicting ratings\"):\n",
    "            try:\n",
    "                predicted_rating, confidence = self.hybrid_predict(user_id, product_id, content_weight, collab_weight)\n",
    "                match_percent = self.calculate_match_percentage(predicted_rating, user_id, product_id)\n",
    "                \n",
    "                skin_match = self.filter_by_skin_profile(product_id, user_id)\n",
    "                adjusted_rating = predicted_rating * skin_match\n",
    "                adjusted_confidence = confidence * skin_match\n",
    "                \n",
    "                match_percent = self.calculate_match_percentage(adjusted_rating, user_id, product_id)\n",
    "                \n",
    "                if adjusted_confidence >= min_confidence and match_percent >= 40:\n",
    "                    recommendations.append((product_id, adjusted_rating, match_percent))\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # ✅ Sort by hybrid predicted rating directly (no reranking)\n",
    "        recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return recommendations[:top_n]\n",
    "\n",
    "    def _get_popular_fallback(self, top_n: int) -> List[Tuple[str, float, int]]:\n",
    "        \"\"\"Fallback to popular products\"\"\"\n",
    "        popular_products = self.test_df.groupby('product_id')['rating'].agg(['count', 'mean']).reset_index()\n",
    "        popular_products = popular_products[popular_products['count'] >= 10]  # Only reasonably popular\n",
    "        popular_products = popular_products.sort_values(['mean', 'count'], ascending=False)\n",
    "        \n",
    "        result = []\n",
    "        for _, row in popular_products.head(top_n).iterrows():\n",
    "            product_id = str(row['product_id'])\n",
    "            score = row['mean']\n",
    "            match_percent = self.calculate_match_percentage(score, \"average_user\", product_id)\n",
    "            result.append((product_id, score, match_percent))\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def enhanced_demo_recommendations(self, user_id: str, top_n: int = 5,\n",
    "                                   content_weight: float = 0.4, collab_weight: float = 0.6):\n",
    "        \"\"\"Show enhanced recommendations with explanations\"\"\"\n",
    "        recommendations = self.generate_recommendations(user_id, top_n * 2, content_weight, collab_weight)\n",
    "        \n",
    "        print(f\"\\n🎯 ENHANCED RECOMMENDATIONS FOR USER {user_id}:\")\n",
    "        print(f\"   Weights: Content={content_weight}, SVD={collab_weight}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        user_data = self.user_history_cache.get(str(user_id), {})\n",
    "        user_avg = user_data.get('avg_rating', self.global_avg)\n",
    "        \n",
    "        displayed = 0\n",
    "        for i, (product_id, score, match_percent) in enumerate(recommendations, 1):\n",
    "            if displayed >= top_n:\n",
    "                break\n",
    "                \n",
    "            product_info = self.prod_df[self.prod_df[\"product_id\"].astype(str) == product_id]\n",
    "            if product_info.empty:\n",
    "                continue\n",
    "                \n",
    "            product_info = product_info.iloc[0]\n",
    "            name = product_info[\"product_name\"]\n",
    "            brand = product_info[\"brand_name\"]\n",
    "            category = product_info[\"tertiary_category\"]\n",
    "            price = product_info[\"price_usd\"]\n",
    "            \n",
    "            formatted_price = f\"${price:.2f}\" if isinstance(price, (int, float)) else f\"${price}\"\n",
    "            \n",
    "            print(f\"{displayed + 1}. {name} ({brand})\")\n",
    "            print(f\"   📍 {category} • 💰 {formatted_price}\")\n",
    "            print(f\"   ⭐ {score:.4f}/5 • 🔍 {match_percent}% match\")\n",
    "            print(f\"   🆔 {product_id}\")\n",
    "            \n",
    "            # Add intelligent explanation\n",
    "            if score >= 4.2:\n",
    "                print(\"   💎 Excellent match! Based on your preferences and highly rated by similar users\")\n",
    "            elif score >= 3.8:\n",
    "                print(\"   👍 Great match - combines your product preferences with crowd wisdom\")\n",
    "            elif score >= 3.2:\n",
    "                print(\"   🔍 Good suggestion - users with similar tastes enjoyed this product\")\n",
    "            elif score >= 2.8:\n",
    "                print(\"   💡 Recommended - similar to products you've liked, worth exploring\")\n",
    "            else:\n",
    "                print(\"   🌟 New discovery - different from your usual preferences but highly rated\")\n",
    "            \n",
    "            print()\n",
    "            displayed += 1\n",
    "        \n",
    "        if displayed == 0:\n",
    "            print(\"⚠️  No confident recommendations found. Try rating more products!\")\n",
    "            print(\"💡 Exploring new categories might help improve recommendations\")\n",
    "        \n",
    "    def evaluate(self, top_n: int = 10,\n",
    "                content_weight: float = 0.4, collab_weight: float = 0.6,\n",
    "                min_confidence: float = 0.0) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate recommender on test dataset with multiple metrics:\n",
    "        - RMSE, MAE\n",
    "        - Accuracy (binary hit if predicted >= 3.5 matches actual >= 3.5)\n",
    "        - Precision, Recall, F1\n",
    "        - Coverage (how many unique products were recommended)\n",
    "        \"\"\"\n",
    "        print(\"\\n📊 Evaluating recommender system...\")\n",
    "\n",
    "        y_true, y_pred = [], []\n",
    "        hit_count, rec_count, relevant_count = 0, 0, 0\n",
    "        recommended_products = set()\n",
    "\n",
    "        for _, row in tqdm(self.test_df.iterrows(), total=len(self.test_df), desc=\"Evaluating\"):\n",
    "            user_id, product_id, actual_rating = str(row[\"author_id\"]), str(row[\"product_id\"]), row[\"rating\"]\n",
    "\n",
    "            try:\n",
    "                pred_rating, confidence = self.hybrid_predict(user_id, product_id,\n",
    "                                                            content_weight, collab_weight)\n",
    "                # if confidence < min_confidence:\n",
    "                #     continue\n",
    "\n",
    "                y_true.append(actual_rating)\n",
    "                y_pred.append(pred_rating)\n",
    "\n",
    "                # --- Binary relevance for classification metrics ---\n",
    "                actual_relevant = 1 if actual_rating >= 3.5 else 0\n",
    "                predicted_relevant = 1 if pred_rating >= 3.5 else 0\n",
    "\n",
    "                if predicted_relevant == 1:\n",
    "                    rec_count += 1\n",
    "                    recommended_products.add(product_id)\n",
    "\n",
    "                if actual_relevant == 1:\n",
    "                    relevant_count += 1\n",
    "\n",
    "                if predicted_relevant == 1 and actual_relevant == 1:\n",
    "                    hit_count += 1\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        # --- Compute metrics ---\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred)) if y_true else float(\"nan\")\n",
    "        mae = mean_absolute_error(y_true, y_pred) if y_true else float(\"nan\")\n",
    "        accuracy = accuracy_score([1 if r >= 3.5 else 0 for r in y_true],\n",
    "                                [1 if p >= 3.5 else 0 for p in y_pred]) if y_true else float(\"nan\")\n",
    "        precision = precision_score([1 if r >= 3.5 else 0 for r in y_true],\n",
    "                                    [1 if p >= 3.5 else 0 for p in y_pred],\n",
    "                                    zero_division=0) if y_true else float(\"nan\")\n",
    "        recall = recall_score([1 if r >= 3.5 else 0 for r in y_true],\n",
    "                            [1 if p >= 3.5 else 0 for p in y_pred],\n",
    "                            zero_division=0) if y_true else float(\"nan\")\n",
    "        f1 = f1_score([1 if r >= 3.5 else 0 for r in y_true],\n",
    "                    [1 if p >= 3.5 else 0 for p in y_pred],\n",
    "                    zero_division=0) if y_true else float(\"nan\")\n",
    "\n",
    "        coverage = len(recommended_products) / len(self.prod_df) if len(self.prod_df) > 0 else 0\n",
    "\n",
    "        results = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"Coverage\": coverage\n",
    "        }\n",
    "\n",
    "        print(\"\\n📈 Evaluation Results:\")\n",
    "        for metric, value in results.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        return results     \n",
    "    def evaluate_by_user_group(self, top_n=10):\n",
    "        \"\"\"\n",
    "        Evaluate recommender performance across user groups:\n",
    "        - Cold-start (<5 ratings)\n",
    "        - Medium (5-20 ratings)\n",
    "        - Heavy (>20 ratings)\n",
    "        \"\"\"\n",
    "        # Count ratings per user from training data\n",
    "        user_rating_counts = self.train_df.groupby(\"author_id\")[\"rating\"].count().to_dict()\n",
    "        \n",
    "        groups = {\n",
    "            \"Cold-start (<5)\": [],\n",
    "            \"Medium (5-20)\": [],\n",
    "            \"Heavy (>20)\": []\n",
    "        }\n",
    "\n",
    "        for _, row in self.test_df.iterrows():\n",
    "            user = str(row[\"author_id\"])\n",
    "            item = str(row[\"product_id\"])\n",
    "            true_rating = row[\"rating\"]\n",
    "\n",
    "            pred_rating, _ = self.hybrid_predict(user, item)\n",
    "\n",
    "            # Assign to group\n",
    "            count = user_rating_counts.get(user, 0)\n",
    "            if count < 5:\n",
    "                group = \"Cold-start (<5)\"\n",
    "            elif count > 20:\n",
    "                group = \"Heavy (>20)\"\n",
    "            else:\n",
    "                group = \"Medium (5-20)\"\n",
    "\n",
    "            groups[group].append((true_rating, pred_rating))\n",
    "\n",
    "        results = {}\n",
    "        for group, values in groups.items():\n",
    "            if not values:\n",
    "                continue\n",
    "            y_true, y_pred = zip(*values)\n",
    "            rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "            y_true_bin = [1 if r >= 4 else 0 for r in y_true]\n",
    "            y_pred_bin = [1 if p >= 4 else 0 for p in y_pred]\n",
    "\n",
    "            precision = precision_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "            recall = recall_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "            f1 = f1_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "            acc = accuracy_score(y_true_bin, y_pred_bin)\n",
    "\n",
    "            results[group] = {\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"F1\": f1,\n",
    "                \"Count\": len(values)\n",
    "            }\n",
    "\n",
    "        results_df = pd.DataFrame(results).T\n",
    "        print(\"\\n📊 Evaluation by User Group:\")\n",
    "        print(results_df)\n",
    "        return results_df\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 INITIALIZING ENHANCED HYBRID RECOMMENDER\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Initialize recommender\n",
    "    recommender = EnhancedHybridRecommender(\n",
    "        train_path=\"data/CleanedDataSet/train_skincare.csv\",\n",
    "        test_path=\"data/CleanedDataSet/test_skincare.csv\",\n",
    "        products_path=\"data/CleanedDataSet/filtered_skincare_products.csv\",\n",
    "        content_model_path=\"models/product_embeddings.pkl\",\n",
    "        svd_model_path=\"models/surprise_svd_model.pkl\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ ENHANCED INITIALIZATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 🔹 Fixed best weights for assignment\n",
    "    best_content_weight = 0.2\n",
    "    best_collab_weight = 0.8\n",
    "\n",
    "    # ---------------- 3) Generate recommendations for a real user ----------------\n",
    "    real_user_id = 2128891661  # <-- must exist in your dataset\n",
    "    recommender.enhanced_demo_recommendations(\n",
    "        user_id=real_user_id,\n",
    "        top_n=5,\n",
    "        content_weight=best_content_weight,\n",
    "        collab_weight=best_collab_weight\n",
    "    )\n",
    "\n",
    "    # ---------------- 4) Evaluate system ----------------\n",
    "    eval_results = recommender.evaluate(\n",
    "        top_n=10,\n",
    "        content_weight=best_content_weight,\n",
    "        collab_weight=best_collab_weight\n",
    "    )\n",
    "\n",
    "    # ---------------- 5) Evaluate by user groups ----------------\n",
    "    group_eval = recommender.evaluate_by_user_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be7170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
