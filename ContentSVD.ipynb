{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e871804",
   "metadata": {},
   "source": [
    "# CONTENT BASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef6bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved at: models\\word2vec_skincare.model\n",
      "Embeddings saved at: models\\product_embeddings.pkl\n",
      "1760 unique products embedded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import joblib\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# Paths\n",
    "TRAIN_PATH = \"data/CleanedDataSet/train_skincare.csv\"\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"word2vec_skincare.model\")\n",
    "EMBEDDINGS_PATH = os.path.join(MODEL_DIR, \"product_embeddings.pkl\")\n",
    "\n",
    "# ---------------- 1) Load dataset ----------------\n",
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df = df.dropna(subset=[\"combined_features\"]).copy()\n",
    "    df[\"tokens\"] = df[\"combined_features\"].astype(str).str.lower().apply(word_tokenize)\n",
    "    return df\n",
    "\n",
    "# ---------------- 2) Extract tertiary category ----------------\n",
    "def extract_tertiary(feature_str):\n",
    "    \"\"\"\n",
    "    从 combined_features extract tertiary category\n",
    "    \"\"\"\n",
    "    feature_str = str(feature_str).lower()\n",
    "    \n",
    "    mapping = {\n",
    "        'moisturizer': 'Moisturizers',\n",
    "        'serum': 'Face Serums',\n",
    "        'eye cream': 'Eye Creams & Treatments',\n",
    "        'treatment': 'Blemish & Acne Treatments',\n",
    "        'lip balm': 'Lip Balms & Treatments',\n",
    "        'sunscreen': 'Face Sunscreen',\n",
    "        'cleanser': 'Face Wash & Cleansers',\n",
    "        'face wash': 'Face Wash & Cleansers',\n",
    "        'oil': 'Face Oils',\n",
    "        'toner': 'Toners',\n",
    "        'mask': 'Face Masks',\n",
    "        'peel': 'Facial Peels',\n",
    "        'exfoliator': 'Exfoliators',\n",
    "        'eye mask': 'Eye Masks',\n",
    "        'wipe': 'Face Wipes',\n",
    "        'night cream': 'Night Creams',\n",
    "        'mist': 'Mists & Essences',\n",
    "        'essence': 'Mists & Essences',\n",
    "        'sheet mask': 'Sheet Masks',\n",
    "        'makeup remover': 'Makeup Removers'\n",
    "    }\n",
    "    \n",
    "    for k, v in mapping.items():\n",
    "        if k in feature_str:\n",
    "            return v\n",
    "    return 'Other'\n",
    "\n",
    "# ---------------- 3) Train Word2Vec ----------------\n",
    "def train_w2v(token_lists, model_path=MODEL_PATH):\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    model = Word2Vec(\n",
    "        sentences=token_lists,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=4,\n",
    "        sg=1,       # Skip-gram\n",
    "        epochs= 80\n",
    "    )\n",
    "    model.save(model_path)\n",
    "    print(f\"Word2Vec model saved at: {model_path}\")\n",
    "    return model\n",
    "\n",
    "# ---------------- 4) Build embeddings ----------------\n",
    "def sentence_vec(tokens, model):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "def build_embeddings(df, model):\n",
    "    # 确保 df 已经有 tertiary_category\n",
    "    if 'tertiary_category' not in df.columns:\n",
    "        df['tertiary_category'] = df['combined_features'].astype(str).apply(extract_tertiary)\n",
    "\n",
    "    prod = df.groupby(\"product_id\", as_index=False).agg({\n",
    "        \"brand_name\": \"first\",\n",
    "        \"product_name\": \"first\",\n",
    "        \"price_usd\": \"first\",\n",
    "        \"tokens\": \"first\",\n",
    "        \"tertiary_category\": \"first\"\n",
    "    })\n",
    "\n",
    "    prod[\"embedding\"] = prod[\"tokens\"].apply(lambda toks: sentence_vec(toks, model))\n",
    "    emb = np.vstack(prod[\"embedding\"].values)\n",
    "    return prod, emb\n",
    "\n",
    "# ---------------- 5) Save embeddings ----------------\n",
    "def save_embeddings(prod_df, prod_embeds, path=EMBEDDINGS_PATH):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    joblib.dump((prod_df, prod_embeds), path)\n",
    "    print(f\"Embeddings saved at: {path}\")\n",
    "\n",
    "# ---------------- 6) Run pipeline ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_dataset(TRAIN_PATH)\n",
    "    df['tertiary_category'] = df['combined_features'].astype(str).apply(extract_tertiary)\n",
    "\n",
    "    w2v = train_w2v(df[\"tokens\"].tolist())\n",
    "    prod_df, prod_embeds = build_embeddings(df, w2v)\n",
    "    save_embeddings(prod_df, prod_embeds)\n",
    "\n",
    "    print(f\"{len(prod_df)} unique products embedded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b630c849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     product_id          brand_name  \\\n",
      "518     P438618  SEPHORA COLLECTION   \n",
      "796     P454771            lilah b.   \n",
      "1217    P474822         Dermalogica   \n",
      "15      P126301            CLINIQUE   \n",
      "1726    P504919            Glossier   \n",
      "\n",
      "                                        product_name  price_usd  \\\n",
      "518          Clean Skin Gel Cleanser with Prebiotics       12.0   \n",
      "796                           Aglow Cleansing Butter       14.0   \n",
      "1217                         Daily Glycolic Cleanser       37.0   \n",
      "15    Take The Day Off Cleansing Balm Makeup Remover       38.0   \n",
      "1726  Milky Oil Dual-Phase Waterproof Makeup Remover       14.0   \n",
      "\n",
      "          tertiary_category  similarity_score  \n",
      "518   Face Wash & Cleansers          0.912015  \n",
      "796   Face Wash & Cleansers          0.882967  \n",
      "1217  Face Wash & Cleansers          0.878240  \n",
      "15    Face Wash & Cleansers          0.876484  \n",
      "1726  Face Wash & Cleansers          0.875279  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Recommendation function: content-based + same tertiary category\n",
    "def recommend_same_category(prod_df, prod_embeds, product_id, top_n=10):\n",
    "    # get targeted product\n",
    "    target = prod_df[prod_df[\"product_id\"] == product_id]\n",
    "    if target.empty:\n",
    "        raise ValueError(f\"Product ID {product_id} not found.\")\n",
    "    \n",
    "    # extract tertiary category\n",
    "    tc = target[\"tertiary_category\"].values[0] if \"tertiary_category\" in target else None\n",
    "    if tc is None:\n",
    "        raise ValueError(f\"Product ID {product_id} has no tertiary_category information.\")\n",
    "\n",
    "    # Only select the product that same tertiary category\n",
    "    candidates = prod_df[prod_df[\"tertiary_category\"] == tc]\n",
    "    \n",
    "    # 如果只有目标产品本身，直接返回空\n",
    "    if len(candidates) <= 1:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"product_id\", \"brand_name\", \"product_name\", \"price_usd\", \"tertiary_category\", \"similarity_score\"\n",
    "        ])\n",
    "    \n",
    "    # 计算相似度\n",
    "    target_vec = target[\"embedding\"].values[0].reshape(1, -1)\n",
    "    cand_embeds = np.vstack(candidates[\"embedding\"].values)\n",
    "    sims = cosine_similarity(target_vec, cand_embeds)[0]\n",
    "\n",
    "    # Build DataFrame\n",
    "    recs = candidates.copy()\n",
    "    recs[\"similarity_score\"] = sims\n",
    "    recs = recs[recs[\"product_id\"] != product_id].sort_values(\"similarity_score\", ascending=False)\n",
    "    \n",
    "    return recs.head(top_n)[[\n",
    "        \"product_id\", \"brand_name\", \"product_name\", \"price_usd\", \"tertiary_category\", \"similarity_score\"\n",
    "    ]]\n",
    "\n",
    "# -------- Example usage --------\n",
    "recs = recommend_same_category(prod_df, prod_embeds, product_id=\"P454095\", top_n=5)\n",
    "print(recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156811f",
   "metadata": {},
   "source": [
    "# Surprise SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset, Reader, SVD, accuracy, dump\n",
    "from surprise.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "print(\"Using Surprise-based collaborative filtering...\")\n",
    "\n",
    "# ---------------- 1) Load data ----------------\n",
    "df = pd.read_csv(\"data/CleanedDataSet/combined_skincare_with_sentiment.csv\", low_memory=False)\n",
    "\n",
    "print(f\"Data loaded: {df.shape}\")\n",
    "print(\"Columns available:\", df.columns.tolist())\n",
    "\n",
    "# ---------------- 2) Prepare final rating ----------------\n",
    "if \"sentiment_score\" in df.columns:\n",
    "    # Normalize sentiment_score (-1 to 1) → (0 to 5)\n",
    "    df[\"sentiment_normalized\"] = (df[\"sentiment_score\"] + 1) * 2.5\n",
    "    df[\"final_rating\"] = 0.7 * df[\"rating\"] + 0.3 * df[\"sentiment_normalized\"]\n",
    "else:\n",
    "    df[\"final_rating\"] = df[\"rating\"]\n",
    "\n",
    "print(f\"Final rating range: {df['final_rating'].min():.2f} to {df['final_rating'].max():.2f}\")\n",
    "\n",
    "# ---------------- 3) Build Surprise dataset ----------------\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[[\"author_id\", \"product_id\", \"final_rating\"]], reader)\n",
    "\n",
    "# ---------------- 4) Train-test split ----------------\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------- 5) Train SVD model ----------------\n",
    "print(\"Training Surprise SVD model...\")\n",
    "model = SVD(\n",
    "    n_factors=50,   # latent dimensions\n",
    "    lr_all=0.005,   # learning rate\n",
    "    reg_all=0.02,   # regularization\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(trainset)\n",
    "\n",
    "# ---------------- 6) Save Model ----------------\n",
    "print(\"Saving model...\")\n",
    "model_path = \"models/surprise_svd_model.pkl\"\n",
    "# Create directory if not exists\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "dump.dump(model_path, algo=model)\n",
    "print(f\"✅ Model saved to {model_path}\")\n",
    "\n",
    "# ---------------- 7) Evaluate with RMSE ----------------\n",
    "predictions = model.test(testset)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"✅ Model evaluation completed. Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ---------------- 8) Top-N helper functions ----------------\n",
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\"\"\"\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # sort and get top N\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n\n",
    "\n",
    "def precision_recall_f1_coverage(predictions, n=10, threshold=3.5):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, F1 and coverage at Top-N recommendations.\n",
    "    threshold: rating above this is considered relevant.\n",
    "    \"\"\"\n",
    "    top_n = get_top_n(predictions, n=n)\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    all_recommended_items = set()\n",
    "\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        # Relevant items for this user (ground truth)\n",
    "        true_items = {iid for (uid_, iid, true_r, est, _) in predictions if uid_ == uid and true_r >= threshold}\n",
    "        recommended_items = {iid for (iid, est) in user_ratings}\n",
    "\n",
    "        all_recommended_items.update(recommended_items)\n",
    "\n",
    "        n_rel = len(true_items)  # relevant\n",
    "        n_rec_k = len(recommended_items)  # recommended in top N\n",
    "        n_rel_and_rec_k = len(true_items & recommended_items)\n",
    "\n",
    "        if n_rec_k > 0:\n",
    "            precisions.append(n_rel_and_rec_k / n_rec_k)\n",
    "        if n_rel > 0:\n",
    "            recalls.append(n_rel_and_rec_k / n_rel)\n",
    "\n",
    "    precision = np.mean(precisions) if precisions else 0\n",
    "    recall = np.mean(recalls) if recalls else 0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Coverage = proportion of unique items ever recommended\n",
    "    all_items = {iid for (_, iid, _, _, _) in predictions}\n",
    "    coverage = len(all_recommended_items) / len(all_items)\n",
    "\n",
    "    return precision, recall, f1, coverage\n",
    "\n",
    "# ---------------- 9) Compute evaluation metrics ----------------\n",
    "precision, recall, f1, coverage = precision_recall_f1_coverage(predictions, n=5, threshold=3.5)\n",
    "\n",
    "print(\"\\n📊 Evaluation Metrics (Top-5):\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1-score:  {f1:.4f}\")\n",
    "print(f\"   Coverage:  {coverage:.4f}\")\n",
    "\n",
    "# ---------------- 10) Cold Start Helper Functions ----------------\n",
    "def get_popular_products(df, top_n=10, min_ratings=5):\n",
    "    \"\"\"Get popular products for cold start fallback\"\"\"\n",
    "    product_stats = df.groupby('product_id').agg({\n",
    "        'final_rating': ['count', 'mean']\n",
    "    }).round(2)\n",
    "    product_stats.columns = ['rating_count', 'avg_rating']\n",
    "    product_stats = product_stats.reset_index()\n",
    "    \n",
    "    # Filter products with enough ratings\n",
    "    qualified = product_stats[product_stats['rating_count'] >= min_ratings]\n",
    "    qualified = qualified.sort_values(['rating_count', 'avg_rating'], ascending=False)\n",
    "    \n",
    "    return qualified.head(top_n)\n",
    "\n",
    "def format_popular_recommendations(popular_products, df):\n",
    "    \"\"\"Format popular products recommendations\"\"\"\n",
    "    product_info = df[['product_id', 'product_name', 'brand_name']].drop_duplicates()\n",
    "    result = pd.merge(popular_products, product_info, on='product_id')\n",
    "    \n",
    "    print(\"\\n🔥 Popular Products (Cold Start Fallback):\")\n",
    "    print(\"-\" * 60)\n",
    "    recommendations = []\n",
    "    \n",
    "    for i, row in result.iterrows():\n",
    "        print(f\"{i+1}. {row['product_name']}\")\n",
    "        print(f\"   Brand: {row['brand_name']}\")\n",
    "        print(f\"   Avg Rating: {row['avg_rating']} ({row['rating_count']} ratings)\")\n",
    "        print()\n",
    "        recommendations.append((row['product_id'], row['avg_rating']))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# ---------------- 11) Enhanced Recommendation function with Cold Start ----------------\n",
    "def recommend_products_surprise(user_id, df, model, top_n=5):\n",
    "    \"\"\"Recommend top N products with cold start handling\"\"\"\n",
    "    user_id = str(user_id)\n",
    "\n",
    "    # Check if user exists in dataset (cold start detection)\n",
    "    if user_id not in df[\"author_id\"].values:\n",
    "        print(f\"🎯 User {user_id} not found - using popular products fallback\")\n",
    "        popular_products = get_popular_products(df, top_n)\n",
    "        return format_popular_recommendations(popular_products, df)\n",
    "\n",
    "    # Get all unique products\n",
    "    all_products = df[\"product_id\"].unique()\n",
    "    user_products = df[df[\"author_id\"] == user_id][\"product_id\"].unique()\n",
    "\n",
    "    predictions = []\n",
    "    for product_id in all_products:\n",
    "        if product_id not in user_products:\n",
    "            try:\n",
    "                pred = model.predict(user_id, str(product_id))\n",
    "                predictions.append((product_id, pred.est))\n",
    "            except:\n",
    "                # Skip if prediction fails for individual item\n",
    "                continue\n",
    "\n",
    "    # Sort by predicted rating\n",
    "    top_predictions = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    # Get product details\n",
    "    product_info = df.drop_duplicates(\"product_id\")[[\"product_id\", \"product_name\", \"brand_name\"]]\n",
    "\n",
    "    print(f\"\\n🎯 Top {top_n} Recommendations for User {user_id}:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (product_id, predicted_rating) in enumerate(top_predictions, 1):\n",
    "        product_row = product_info[product_info[\"product_id\"] == product_id]\n",
    "        if not product_row.empty:\n",
    "            name = product_row[\"product_name\"].values[0]\n",
    "            brand = product_row[\"brand_name\"].values[0]\n",
    "            print(f\"{i}. {name}\")\n",
    "            print(f\"   Brand: {brand}\")\n",
    "            print(f\"   Predicted Rating: {predicted_rating:.2f}\")\n",
    "        else:\n",
    "            print(f\"{i}. Product {product_id} - Details not available\")\n",
    "\n",
    "    return top_predictions\n",
    "\n",
    "# ---------------- 12) Test recommendation with both existing and new user ----------------\n",
    "if len(df[\"author_id\"].unique()) > 0:\n",
    "    # Test with existing user\n",
    "    sample_user = df[\"author_id\"].iloc[0]\n",
    "    print(\"Testing with existing user:\")\n",
    "    recs = recommend_products_surprise(sample_user, df, model, top_n=5)\n",
    "    \n",
    "    # Test with new user (cold start)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing Cold Start with new user:\")\n",
    "    new_user = \"new_user_12345\"\n",
    "    new_user_recs = recommend_products_surprise(new_user, df, model, top_n=5)\n",
    "else:\n",
    "    print(\"No users found in dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
