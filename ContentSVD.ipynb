{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e871804",
   "metadata": {},
   "source": [
    "# CONTENT BASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load & filter skincare products\n",
    "# ----------------------------\n",
    "product_info = pd.read_csv(\"data/OriDataSet/product_info.csv\", low_memory=False)\n",
    "\n",
    "# Convert product_id to string and strip whitespace\n",
    "product_info['product_id'] = product_info['product_id'].astype(str).str.strip()\n",
    "\n",
    "# Filter primary category\n",
    "skincare_products_df = product_info[product_info['primary_category'].str.lower() == 'skincare'].copy()\n",
    "\n",
    "# Filter secondary categories\n",
    "allowed_secondary_categories = [\n",
    "    'Moisturizers', 'Treatments', 'Eye Care', 'Lip Balms & Treatments',\n",
    "    'Sunscreen', 'Cleansers', 'Masks'\n",
    "]\n",
    "skincare_products_df = skincare_products_df[\n",
    "    skincare_products_df['secondary_category'].isin(allowed_secondary_categories)\n",
    "].copy()\n",
    "\n",
    "# Filter tertiary categories\n",
    "allowed_tertiary_categories = [\n",
    "    'Moisturizers', 'Face Serums', 'Eye Creams & Treatments', 'Face Sunscreen',\n",
    "    'Face Wash & Cleansers', 'Face Oils', 'Toners', 'Face Masks', 'Facial Peels',\n",
    "    'Exfoliators', 'Eye Masks', 'Face Wipes', 'Blemish & Acne Treatments',\n",
    "    'Night Creams', 'Mists & Essences', 'Sheet Masks', 'Makeup Removers'\n",
    "]\n",
    "skincare_products_df = skincare_products_df[\n",
    "    skincare_products_df['tertiary_category'].isin(allowed_tertiary_categories)\n",
    "].copy()\n",
    "\n",
    "# Optional: save filtered products\n",
    "skincare_products_df.to_csv(\"data/CleanedDataSet/filtered_skincare_products.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load & filter reviews\n",
    "# ----------------------------\n",
    "review_files = [\n",
    "    \"data/OriDataSet/reviews_0-250.csv\",\n",
    "    \"data/OriDataSet/reviews_250-500.csv\",\n",
    "    \"data/OriDataSet/reviews_500-750.csv\",\n",
    "    \"data/OriDataSet/reviews_750-1250.csv\",\n",
    "    \"data/OriDataSet/reviews_1250-end.csv\"\n",
    "]\n",
    "\n",
    "skincare_product_ids = set(skincare_products_df['product_id'].unique())\n",
    "all_reviews = []\n",
    "\n",
    "for file in review_files:\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Drop 'Unnamed: 0' if exists\n",
    "    if \"Unnamed: 0\" in df.columns:\n",
    "        df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "    \n",
    "    df['product_id'] = df['product_id'].astype(str).str.strip()\n",
    "    skincare_reviews = df[df['product_id'].isin(skincare_product_ids)].copy()\n",
    "    all_reviews.append(skincare_reviews)\n",
    "\n",
    "# Merge all reviews\n",
    "merged_reviews_df = pd.concat(all_reviews, ignore_index=True)\n",
    "\n",
    "# Add sequential review_id\n",
    "merged_reviews_df['review_id'] = merged_reviews_df.index + 1\n",
    "\n",
    "# Drop essential missing values\n",
    "merged_reviews_df.dropna(subset=[\"author_id\", \"product_id\", \"rating\", \"review_text\", \"skin_type\"], inplace=True)\n",
    "\n",
    "# Keep valid ratings\n",
    "merged_reviews_df = merged_reviews_df[(merged_reviews_df[\"rating\"] >= 1) & (merged_reviews_df[\"rating\"] <= 5)]\n",
    "\n",
    "# Remove duplicates\n",
    "merged_reviews_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Clean review text\n",
    "# ----------------------------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)   # remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "merged_reviews_df[\"cleaned_review_text\"] = merged_reviews_df[\"review_text\"].apply(clean_text)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Add combined features to products\n",
    "# ----------------------------\n",
    "skincare_products_df[\"combined_features\"] = (\n",
    "    skincare_products_df[\"brand_name\"].fillna(\"\") + \" \" +\n",
    "    skincare_products_df[\"tertiary_category\"].fillna(\"\") + \" \" +\n",
    "    skincare_products_df[\"product_name\"].fillna(\"\") + \" \" +\n",
    "    skincare_products_df.get(\"ingredients\", pd.Series(\"\")).fillna(\"\") + \" \" +\n",
    "    skincare_products_df.get(\"highlights\", pd.Series(\"\")).fillna(\"\") + \" \" \n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Merge reviews with product info\n",
    "# ----------------------------\n",
    "merged_reviews_df = merged_reviews_df.drop(columns=[\"brand_name\", \"product_name\"], errors=\"ignore\")\n",
    "\n",
    "combined_df = pd.merge(\n",
    "    merged_reviews_df,\n",
    "    skincare_products_df[[\"product_id\", \"brand_name\", \"product_name\", \"combined_features\"]],\n",
    "    on=\"product_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Sentiment analysis\n",
    "# ----------------------------\n",
    "def analyze_sentiment(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return 0.0\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "combined_df[\"sentiment_score\"] = combined_df[\"cleaned_review_text\"].apply(analyze_sentiment)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Train/test split\n",
    "# ----------------------------\n",
    "train_df, test_df = train_test_split(\n",
    "    combined_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Save final datasets\n",
    "# ----------------------------\n",
    "try:\n",
    "    combined_df.to_csv(\"data/CleanedDataSet/combined_skincare_with_sentiment.csv\", index=False)\n",
    "    print(\"Combined dataset saved successfully\")\n",
    "except PermissionError:\n",
    "    print(\"Could not save combined dataset CSV (file may be open)\")\n",
    "\n",
    "try:\n",
    "    train_df.to_csv(\"data/CleanedDataSet/train_skincare.csv\", index=False)\n",
    "    test_df.to_csv(\"data/CleanedDataSet/test_skincare.csv\", index=False)\n",
    "    print(\"Train/test datasets saved successfully\")\n",
    "except PermissionError:\n",
    "    print(\"Could not save train/test CSV files (files may be open)\")\n",
    "\n",
    "print(\"Full preprocessing and train/test split completed!\")\n",
    "print(f\"Combined dataframe shape: {combined_df.shape}\")\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved at: models\\word2vec_skincare.model\n",
      "Embeddings saved at: models\\product_embeddings.pkl\n",
      "1760 unique products embedded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# Paths\n",
    "TRAIN_PATH = \"data/CleanedDataSet/train_skincare.csv\"\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"word2vec_skincare.model\")\n",
    "EMBEDDINGS_PATH = os.path.join(MODEL_DIR, \"product_embeddings.pkl\")\n",
    "\n",
    "# ---------------- 1) Load dataset ----------------\n",
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    df = df.dropna(subset=[\"combined_features\"]).copy()\n",
    "    df[\"tokens\"] = df[\"combined_features\"].astype(str).str.lower().apply(word_tokenize)\n",
    "    return df\n",
    "\n",
    "# ---------------- 2) Extract tertiary category ----------------\n",
    "def extract_tertiary(feature_str):\n",
    "    \"\"\"\n",
    "    ‰ªé combined_features extract tertiary category\n",
    "    \"\"\"\n",
    "    feature_str = str(feature_str).lower()\n",
    "    \n",
    "    mapping = {\n",
    "        'moisturizer': 'Moisturizers',\n",
    "        'serum': 'Face Serums',\n",
    "        'eye cream': 'Eye Creams & Treatments',\n",
    "        'treatment': 'Blemish & Acne Treatments',\n",
    "        'lip balm': 'Lip Balms & Treatments',\n",
    "        'sunscreen': 'Face Sunscreen',\n",
    "        'cleanser': 'Face Wash & Cleansers',\n",
    "        'face wash': 'Face Wash & Cleansers',\n",
    "        'oil': 'Face Oils',\n",
    "        'toner': 'Toners',\n",
    "        'mask': 'Face Masks',\n",
    "        'peel': 'Facial Peels',\n",
    "        'exfoliator': 'Exfoliators',\n",
    "        'eye mask': 'Eye Masks',\n",
    "        'wipe': 'Face Wipes',\n",
    "        'night cream': 'Night Creams',\n",
    "        'mist': 'Mists & Essences',\n",
    "        'essence': 'Mists & Essences',\n",
    "        'sheet mask': 'Sheet Masks',\n",
    "        'makeup remover': 'Makeup Removers'\n",
    "    }\n",
    "    \n",
    "    for k, v in mapping.items():\n",
    "        if k in feature_str:\n",
    "            return v\n",
    "    return 'Other'\n",
    "\n",
    "# ---------------- 3) Train Word2Vec ----------------\n",
    "def train_w2v(token_lists, model_path=MODEL_PATH):\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    model = Word2Vec(\n",
    "        sentences=token_lists,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=4,\n",
    "        sg=1,       # Skip-gram\n",
    "        epochs= 80\n",
    "    )\n",
    "    model.save(model_path)\n",
    "    print(f\"Word2Vec model saved at: {model_path}\")\n",
    "    return model\n",
    "\n",
    "# ---------------- 4) Build embeddings ----------------\n",
    "def sentence_vec(tokens, model):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "def build_embeddings(df, model):\n",
    "    # Á°Æ‰øù df Â∑≤ÁªèÊúâ tertiary_category\n",
    "    if 'tertiary_category' not in df.columns:\n",
    "        df['tertiary_category'] = df['combined_features'].astype(str).apply(extract_tertiary)\n",
    "\n",
    "    prod = df.groupby(\"product_id\", as_index=False).agg({\n",
    "        \"brand_name\": \"first\",\n",
    "        \"product_name\": \"first\",\n",
    "        \"price_usd\": \"first\",\n",
    "        \"tokens\": \"first\",\n",
    "        \"tertiary_category\": \"first\"\n",
    "    })\n",
    "\n",
    "    prod[\"embedding\"] = prod[\"tokens\"].apply(lambda toks: sentence_vec(toks, model))\n",
    "    emb = np.vstack(prod[\"embedding\"].values)\n",
    "    return prod, emb\n",
    "\n",
    "# ---------------- 5) Save embeddings ----------------\n",
    "def save_embeddings(prod_df, prod_embeds, path=EMBEDDINGS_PATH):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    joblib.dump((prod_df, prod_embeds), path)\n",
    "    print(f\"Embeddings saved at: {path}\")\n",
    "\n",
    "# ---------------- 6) Run pipeline ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_dataset(TRAIN_PATH)\n",
    "    df['tertiary_category'] = df['combined_features'].astype(str).apply(extract_tertiary)\n",
    "\n",
    "    w2v = train_w2v(df[\"tokens\"].tolist())\n",
    "    prod_df, prod_embeds = build_embeddings(df, w2v)\n",
    "    save_embeddings(prod_df, prod_embeds)\n",
    "\n",
    "    print(f\"{len(prod_df)} unique products embedded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630c849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_23276\\3500042830.py:11: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/CleanedDataSet/combined_skincare_with_sentiment.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Correct Content-Based Evaluation ===\n",
      "Average recommendation similarity: 0.940\n",
      "Similarity > 0.8: 100.0%\n",
      "Category consistency: 50.0%\n",
      "Diversity (cross-category recommendations): 82.0%\n",
      "\n",
      "=== Recommendation Example Test ===\n",
      "\n",
      "1. Recommendations for product 'HydraKate Recharging Water Cream Moisturizer':\n",
      "   ‚Üí Water Drench Hyaluronic Cloud Rich Barrier Moisturizer (similarity: 0.950)\n",
      "   ‚Üí The ZenBubble Gel Cream (similarity: 0.942)\n",
      "\n",
      "2. Recommendations for product 'High-Potency Night-a-Mins Resurfacing Cream with Fruit-Derived AHAs':\n",
      "   ‚Üí High-Potency Night-A-Mins Oil-Free Resurfacing Cream with Fruit Derived AHAs (similarity: 0.988)\n",
      "   ‚Üí Plantscription Youth-Renewing Power Night Cream (similarity: 0.958)\n",
      "\n",
      "3. Recommendations for product 'All About Clean Liquid Facial Soap':\n",
      "   ‚Üí All About Clean 2-in-1 Cleansing + Exfoliating Jelly (similarity: 0.946)\n",
      "   ‚Üí All About Clean Foaming Facial Soap (similarity: 0.920)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Recommendation function: content-based + same tertiary category\n",
    "def recommend_same_category(prod_df, prod_embeds, product_id, top_n=10):\n",
    "    # get targeted product\n",
    "    target = prod_df[prod_df[\"product_id\"] == product_id]\n",
    "    if target.empty:\n",
    "        raise ValueError(f\"Product ID {product_id} not found.\")\n",
    "    \n",
    "    # extract tertiary category\n",
    "    tc = target[\"tertiary_category\"].values[0] if \"tertiary_category\" in target else None\n",
    "    if tc is None:\n",
    "        raise ValueError(f\"Product ID {product_id} has no tertiary_category information.\")\n",
    "\n",
    "    # Only select the product that same tertiary category\n",
    "    candidates = prod_df[prod_df[\"tertiary_category\"] == tc]\n",
    "    \n",
    "    # Â¶ÇÊûúÂè™ÊúâÁõÆÊ†á‰∫ßÂìÅÊú¨Ë∫´ÔºåÁõ¥Êé•ËøîÂõûÁ©∫\n",
    "    if len(candidates) <= 1:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"product_id\", \"brand_name\", \"product_name\", \"price_usd\", \"tertiary_category\", \"similarity_score\"\n",
    "        ])\n",
    "    \n",
    "    # ËÆ°ÁÆóÁõ∏‰ººÂ∫¶\n",
    "    target_vec = target[\"embedding\"].values[0].reshape(1, -1)\n",
    "    cand_embeds = np.vstack(candidates[\"embedding\"].values)\n",
    "    sims = cosine_similarity(target_vec, cand_embeds)[0]\n",
    "\n",
    "    # Build DataFrame\n",
    "    recs = candidates.copy()\n",
    "    recs[\"similarity_score\"] = sims\n",
    "    recs = recs[recs[\"product_id\"] != product_id].sort_values(\"similarity_score\", ascending=False)\n",
    "    \n",
    "    return recs.head(top_n)[[\n",
    "        \"product_id\", \"brand_name\", \"product_name\", \"price_usd\", \"tertiary_category\", \"similarity_score\"\n",
    "    ]]\n",
    "\n",
    "# -------- Example usage --------\n",
    "recs = recommend_same_category(prod_df, prod_embeds, product_id=\"P454095\", top_n=5)\n",
    "print(recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156811f",
   "metadata": {},
   "source": [
    "# Surprise SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset, Reader, SVD, accuracy, dump\n",
    "from surprise.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "print(\"Using Surprise-based collaborative filtering...\")\n",
    "\n",
    "# ---------------- 1) Load data ----------------\n",
    "df = pd.read_csv(\"data/CleanedDataSet/combined_skincare_with_sentiment.csv\", low_memory=False)\n",
    "\n",
    "print(f\"Data loaded: {df.shape}\")\n",
    "print(\"Columns available:\", df.columns.tolist())\n",
    "\n",
    "# ---------------- 2) Prepare final rating ----------------\n",
    "if \"sentiment_score\" in df.columns:\n",
    "    # Normalize sentiment_score (-1 to 1) ‚Üí (0 to 5)\n",
    "    df[\"sentiment_normalized\"] = (df[\"sentiment_score\"] + 1) * 2.5\n",
    "    df[\"final_rating\"] = 0.7 * df[\"rating\"] + 0.3 * df[\"sentiment_normalized\"]\n",
    "else:\n",
    "    df[\"final_rating\"] = df[\"rating\"]\n",
    "\n",
    "print(f\"Final rating range: {df['final_rating'].min():.2f} to {df['final_rating'].max():.2f}\")\n",
    "\n",
    "# ---------------- 3) Build Surprise dataset ----------------\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[[\"author_id\", \"product_id\", \"final_rating\"]], reader)\n",
    "\n",
    "# ---------------- 4) Train-test split ----------------\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------- 5) Train SVD model ----------------\n",
    "print(\"Training Surprise SVD model...\")\n",
    "model = SVD(\n",
    "    n_factors=50,   # latent dimensions\n",
    "    lr_all=0.005,   # learning rate\n",
    "    reg_all=0.02,   # regularization\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(trainset)\n",
    "\n",
    "# ---------------- 6) Save Model ----------------\n",
    "print(\"Saving model...\")\n",
    "model_path = \"models/surprise_svd_model.pkl\"\n",
    "# Create directory if not exists\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "dump.dump(model_path, algo=model)\n",
    "print(f\"‚úÖ Model saved to {model_path}\")\n",
    "\n",
    "# ---------------- 7) Evaluate with RMSE ----------------\n",
    "predictions = model.test(testset)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"‚úÖ Model evaluation completed. Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ---------------- 8) Top-N helper functions ----------------\n",
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\"\"\"\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # sort and get top N\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n\n",
    "\n",
    "def precision_recall_f1_coverage(predictions, n=10, threshold=3.5):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, F1 and coverage at Top-N recommendations.\n",
    "    threshold: rating above this is considered relevant.\n",
    "    \"\"\"\n",
    "    top_n = get_top_n(predictions, n=n)\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    all_recommended_items = set()\n",
    "\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        # Relevant items for this user (ground truth)\n",
    "        true_items = {iid for (uid_, iid, true_r, est, _) in predictions if uid_ == uid and true_r >= threshold}\n",
    "        recommended_items = {iid for (iid, est) in user_ratings}\n",
    "\n",
    "        all_recommended_items.update(recommended_items)\n",
    "\n",
    "        n_rel = len(true_items)  # relevant\n",
    "        n_rec_k = len(recommended_items)  # recommended in top N\n",
    "        n_rel_and_rec_k = len(true_items & recommended_items)\n",
    "\n",
    "        if n_rec_k > 0:\n",
    "            precisions.append(n_rel_and_rec_k / n_rec_k)\n",
    "        if n_rel > 0:\n",
    "            recalls.append(n_rel_and_rec_k / n_rel)\n",
    "\n",
    "    precision = np.mean(precisions) if precisions else 0\n",
    "    recall = np.mean(recalls) if recalls else 0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Coverage = proportion of unique items ever recommended\n",
    "    all_items = {iid for (_, iid, _, _, _) in predictions}\n",
    "    coverage = len(all_recommended_items) / len(all_items)\n",
    "\n",
    "    return precision, recall, f1, coverage\n",
    "\n",
    "# ---------------- 9) Compute evaluation metrics ----------------\n",
    "precision, recall, f1, coverage = precision_recall_f1_coverage(predictions, n=5, threshold=3.5)\n",
    "\n",
    "print(\"\\nüìä Evaluation Metrics (Top-5):\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1-score:  {f1:.4f}\")\n",
    "print(f\"   Coverage:  {coverage:.4f}\")\n",
    "\n",
    "# ---------------- 10) Cold Start Helper Functions ----------------\n",
    "def get_popular_products(df, top_n=10, min_ratings=5):\n",
    "    \"\"\"Get popular products for cold start fallback\"\"\"\n",
    "    product_stats = df.groupby('product_id').agg({\n",
    "        'final_rating': ['count', 'mean']\n",
    "    }).round(2)\n",
    "    product_stats.columns = ['rating_count', 'avg_rating']\n",
    "    product_stats = product_stats.reset_index()\n",
    "    \n",
    "    # Filter products with enough ratings\n",
    "    qualified = product_stats[product_stats['rating_count'] >= min_ratings]\n",
    "    qualified = qualified.sort_values(['rating_count', 'avg_rating'], ascending=False)\n",
    "    \n",
    "    return qualified.head(top_n)\n",
    "\n",
    "def format_popular_recommendations(popular_products, df):\n",
    "    \"\"\"Format popular products recommendations\"\"\"\n",
    "    product_info = df[['product_id', 'product_name', 'brand_name']].drop_duplicates()\n",
    "    result = pd.merge(popular_products, product_info, on='product_id')\n",
    "    \n",
    "    print(\"\\nüî• Popular Products (Cold Start Fallback):\")\n",
    "    print(\"-\" * 60)\n",
    "    recommendations = []\n",
    "    \n",
    "    for i, row in result.iterrows():\n",
    "        print(f\"{i+1}. {row['product_name']}\")\n",
    "        print(f\"   Brand: {row['brand_name']}\")\n",
    "        print(f\"   Avg Rating: {row['avg_rating']} ({row['rating_count']} ratings)\")\n",
    "        print()\n",
    "        recommendations.append((row['product_id'], row['avg_rating']))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# ---------------- 11) Enhanced Recommendation function with Cold Start ----------------\n",
    "def recommend_products_surprise(user_id, df, model, top_n=5):\n",
    "    \"\"\"Recommend top N products with cold start handling\"\"\"\n",
    "    user_id = str(user_id)\n",
    "\n",
    "    # Check if user exists in dataset (cold start detection)\n",
    "    if user_id not in df[\"author_id\"].values:\n",
    "        print(f\"üéØ User {user_id} not found - using popular products fallback\")\n",
    "        popular_products = get_popular_products(df, top_n)\n",
    "        return format_popular_recommendations(popular_products, df)\n",
    "\n",
    "    # Get all unique products\n",
    "    all_products = df[\"product_id\"].unique()\n",
    "    user_products = df[df[\"author_id\"] == user_id][\"product_id\"].unique()\n",
    "\n",
    "    predictions = []\n",
    "    for product_id in all_products:\n",
    "        if product_id not in user_products:\n",
    "            try:\n",
    "                pred = model.predict(user_id, str(product_id))\n",
    "                predictions.append((product_id, pred.est))\n",
    "            except:\n",
    "                # Skip if prediction fails for individual item\n",
    "                continue\n",
    "\n",
    "    # Sort by predicted rating\n",
    "    top_predictions = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    # Get product details\n",
    "    product_info = df.drop_duplicates(\"product_id\")[[\"product_id\", \"product_name\", \"brand_name\"]]\n",
    "\n",
    "    print(f\"\\nüéØ Top {top_n} Recommendations for User {user_id}:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (product_id, predicted_rating) in enumerate(top_predictions, 1):\n",
    "        product_row = product_info[product_info[\"product_id\"] == product_id]\n",
    "        if not product_row.empty:\n",
    "            name = product_row[\"product_name\"].values[0]\n",
    "            brand = product_row[\"brand_name\"].values[0]\n",
    "            print(f\"{i}. {name}\")\n",
    "            print(f\"   Brand: {brand}\")\n",
    "            print(f\"   Predicted Rating: {predicted_rating:.2f}\")\n",
    "        else:\n",
    "            print(f\"{i}. Product {product_id} - Details not available\")\n",
    "\n",
    "    return top_predictions\n",
    "\n",
    "# ---------------- 12) Test recommendation with both existing and new user ----------------\n",
    "if len(df[\"author_id\"].unique()) > 0:\n",
    "    # Test with existing user\n",
    "    sample_user = df[\"author_id\"].iloc[0]\n",
    "    print(\"Testing with existing user:\")\n",
    "    recs = recommend_products_surprise(sample_user, df, model, top_n=5)\n",
    "    \n",
    "    # Test with new user (cold start)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing Cold Start with new user:\")\n",
    "    new_user = \"new_user_12345\"\n",
    "    new_user_recs = recommend_products_surprise(new_user, df, model, top_n=5)\n",
    "else:\n",
    "    print(\"No users found in dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
